# ğŸ”„ Latest Code Changes

```diff
diff --git a/.SourceSageignore b/.SourceSageignore
index a029c83..ac8bce0 100644
--- a/.SourceSageignore
+++ b/.SourceSageignore
@@ -52,3 +52,4 @@ repository_summary.md
 venv
 .venv
 
+uv.lock
diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..72d7c7d
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,2 @@
+OPENROUTER_API_KEY=sk-or-xxxx
+HUGGINGFACE_TOKEN=hf_xxxx
diff --git a/.gitignore b/.gitignore
index ba4f0f5..04ce1f8 100644
--- a/.gitignore
+++ b/.gitignore
@@ -208,3 +208,4 @@ __marimo__/
 .SourceSageAssets/
 uv.lock
 example/output/structured/logs/
+example/output/
diff --git a/README.md b/README.md
index 27bea1e..49e8d17 100644
--- a/README.md
+++ b/README.md
@@ -1,15 +1,38 @@
-# Easy Dataset CLI
+<div align="center">
 
-ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªCLIãƒ„ãƒ¼ãƒ«ã§ã™ã€‚LLMã‚’ä½¿ç”¨ã—ã¦Genre-Audienceãƒšã‚¢ã«åŸºã¥ã„ãŸå¤šæ§˜ãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
+![](https://github.com/user-attachments/assets/865632a4-911f-4de4-867d-c65cef365d79)
 
-## ç‰¹å¾´
+# ğŸš€ Easy Dataset CLI
 
-- **ã‚·ãƒ³ãƒ—ãƒ«**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸è¦ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§GAå®šç¾©
-- **æŸ”è»Ÿ**: è¤‡æ•°ã®Genre-Audienceãƒšã‚¢ã«å¯¾å¿œ
-- **å®‰å®š**: LLMã‹ã‚‰ã®ç›´æ¥XMLå‡ºåŠ›ã§ä¿¡é ¼æ€§å‘ä¸Š
-- **åŠ¹ç‡çš„**: ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã¨ãƒãƒƒãƒå‡¦ç†ã§å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å¯¾å¿œ
+<p align="center">
+  <img src="https://img.shields.io/badge/Python-3.9+-blue.svg" alt="Python Version">
+  <img src="https://img.shields.io/badge/CLI-Typer-green.svg" alt="CLI Framework">
+  <img src="https://img.shields.io/badge/LLM-OpenAI%20%7C%20OpenRouter-orange.svg" alt="LLM Support">
+  <img src="https://img.shields.io/badge/Format-Alpaca%20%7C%20XML-purple.svg" alt="Output Format">
+  <img src="https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow.svg" alt="Hugging Face">
+  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
+</p>
 
-## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
+<p align="center">
+  ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªCLIãƒ„ãƒ¼ãƒ«<br>
+  LLMã‚’ä½¿ç”¨ã—ã¦Genre-Audienceãƒšã‚¢ã«åŸºã¥ã„ãŸå¤šæ§˜ãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€<br>
+  <strong>Alpacaå½¢å¼JSON</strong>ã‚„Genreåˆ¥XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã€<strong>Hugging Face Hub</strong>ã¸ã®ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚‚å¯¾å¿œ
+</p>
+
+</div>
+
+## âœ¨ ç‰¹å¾´
+
+- **ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸è¦ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§GAå®šç¾©
+- **ğŸ”„ æŸ”è»Ÿ**: è¤‡æ•°ã®Genre-Audienceãƒšã‚¢ã«å¯¾å¿œ
+- **ğŸ›¡ï¸ å®‰å®š**: LLMã‹ã‚‰ã®ç›´æ¥XMLå‡ºåŠ›ã§ä¿¡é ¼æ€§å‘ä¸Š
+- **âš¡ åŠ¹ç‡çš„**: ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã¨ãƒãƒƒãƒå‡¦ç†ã§å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å¯¾å¿œ
+- **ğŸ¦™ Alpacaå¯¾å¿œ**: ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONã§å‡ºåŠ›
+- **ğŸ¤— HFçµ±åˆ**: Hugging Face Hubã¸ã®ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
+- **ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰**: è‡ªå‹•çš„ãªREADME.mdç”Ÿæˆã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’æ•´ç†
+- **ğŸ”„ å¤‰æ›æ©Ÿèƒ½**: æ—¢å­˜XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Alpacaå½¢å¼ã¸ã®å¤‰æ›ã‚³ãƒãƒ³ãƒ‰
+
+## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
 
 \```bash
 # ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆæ¨å¥¨ï¼‰
@@ -22,9 +45,9 @@ venv\Scripts\activate     # Windows
 pip install -e .
 \```
 
-## ä½¿ç”¨æ–¹æ³•
+## ğŸš€ ä½¿ç”¨æ–¹æ³•
 
-### æ–°ã—ã„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆæ¨å¥¨ï¼‰
+### ğŸ“‹ åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
 
 1. **GAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ç”Ÿæˆ**
 \```bash
@@ -32,121 +55,284 @@ pip install -e .
 export OPENAI_API_KEY="your-api-key-here"
 
 # å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã‚’è‡ªå‹•ç”Ÿæˆ
-easy-dataset create-ga sample_document.txt --output ga-definitions.md
+uv run easy-dataset create-ga .\example\input\documents\sample_document.txt --output-dir .\example\output\sample_document --num-ga-pairs 10
 \```
 
-2. **ï¼ˆä»»æ„ï¼‰ç”Ÿæˆã•ã‚ŒãŸGAå®šç¾©ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ç·¨é›†**
+2. **Q&Aãƒšã‚¢ã®ç”Ÿæˆ**
 \```bash
-# ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§å†…å®¹ã‚’ç¢ºèªãƒ»ä¿®æ­£
-notepad ga-definitions.md  # Windows
-# ã¾ãŸã¯
-nano ga-definitions.md     # Linux/macOS
+# GAãƒšã‚¢å®šç¾©ã‚’ä½¿ã£ã¦Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ
+uv run easy-dataset generate .\example\input\documents\sample_document.txt --ga-file .\example\output\sample_document\ga\ga_definitions.xml --output-dir .\example\output\sample_document\ --chunk-size 500
 \```
 
-3. **Q&Aãƒšã‚¢ã®ç”Ÿæˆ**
+### ğŸ¦™ Alpacaå½¢å¼ã¨Hugging Faceé€£æºã®ä½¿ç”¨ä¾‹
+
+#### Alpacaå½¢å¼ã§ã®å‡ºåŠ›
 \```bash
-# GAãƒšã‚¢å®šç¾©ã‚’ä½¿ã£ã¦Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ
-easy-dataset generate sample_document.txt --ga-file ga-definitions.md --output-dir ./results
+# Q&Aç”Ÿæˆã¨åŒæ™‚ã«Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›
+uv run easy-dataset generate .\example\input\documents\sample_document.txt \
+  --ga-file .\example\output\sample_document\ga\ga_definitions.xml \
+  --output-dir .\example\output\sample_document\ \
+  --export-alpaca
 \```
 
-### å¾“æ¥ã®æ–¹æ³•ï¼ˆæ‰‹å‹•ã§GAå®šç¾©ã‚’ä½œæˆï¼‰
+#### Hugging Face Hubã¸ã®ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+\```bash
+# ç’°å¢ƒå¤‰æ•°ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
+set HUGGINGFACE_TOKEN=hf_your_token_here
+
+# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã¨Hugging Face Hubã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸€åº¦ã«å®Ÿè¡Œ
+uv run easy-dataset generate .\example\input\documents\sample_document.txt \
+  --ga-file .\example\output\sample_document\ga\ga_definitions.xml \
+  --output-dir .\example\output\sample_document\ \
+  --export-alpaca \
+  --upload-hf \
+  --hf-repo-name username/my-qa-dataset
+\```
 
+#### æ—¢å­˜XMLãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
 \```bash
-# æ‰‹å‹•ã§ä½œæˆã—ãŸGAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
-easy-dataset generate sample_document.txt --ga-file sample_ga_definition.md --output-dir ./results
+# æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã«å¤‰æ›ã—ã¦Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+uv run easy-dataset convert-to-alpaca .\example\output\sample_document\qa \
+  --output-file dataset.json \
+  --upload-hf \
+  --hf-repo-name username/my-qa-dataset \
+  --hf-private
 \```
 
-### ã‚³ãƒãƒ³ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
+### âš™ï¸ ã‚³ãƒãƒ³ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
 
-#### create-ga ã‚³ãƒãƒ³ãƒ‰
+#### ğŸ”§ create-ga ã‚³ãƒãƒ³ãƒ‰
 \```bash
-easy-dataset create-ga [OPTIONS] FILE_PATH
+uv run easy-dataset create-ga [OPTIONS] FILE_PATH
 
 Arguments:
-  FILE_PATH  GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
+  FILE_PATH  GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« [required]
 
 Options:
-  -o, --output PATH    ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ [required]
-  -m, --model TEXT     GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ« [default: gpt-4o]
-  -h, --help           ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
+  -o, --output-dir DIRECTORY  ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª [required]
+  -m, --model TEXT           GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«å [default: openrouter/openai/gpt-4o]
+  -g, --num-ga-pairs INTEGER ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™
+  -h, --help                 Show this message and exit
 \```
 
-#### generate ã‚³ãƒãƒ³ãƒ‰
+#### ğŸ”§ generate ã‚³ãƒãƒ³ãƒ‰
 \```bash
-easy-dataset generate [OPTIONS] FILE_PATH
+uv run easy-dataset generate [OPTIONS] FILE_PATH
 
 Arguments:
-  FILE_PATH  å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
+  FILE_PATH  å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ [required]
 
 Options:
-  -g, --ga-file PATH        Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸMarkdownãƒ•ã‚¡ã‚¤ãƒ« [required]
-  -o, --output-dir PATH     XMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆçœç•¥æ™‚ã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼‰
-  -m, --model TEXT          Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ« [default: gpt-4o]
-  --chunk-size INTEGER      ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚º [default: 2000]
-  --chunk-overlap INTEGER   ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º [default: 200]
-  -h, --help                ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
+  --ga-file PATH           Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLãƒ•ã‚¡ã‚¤ãƒ« [required]
+  -o, --output-dir PATH    XMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
+  -m, --model TEXT         Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ« [default: openrouter/openai/gpt-4o]
+  --chunk-size INTEGER     ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚º [default: 2000]
+  --chunk-overlap INTEGER  ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º [default: 200]
+  -h, --help               Show this message and exit
 \```
 
-## GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼
-
-Genre-Audienceãƒšã‚¢ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å®šç¾©ã—ã¾ã™ï¼š
-
-\```markdown
-# Genre: å­¦è¡“è«–æ–‡
-å­¦è¡“çš„ã§å³å¯†ãªè¡¨ç¾ã‚’ç”¨ã„ã€å°‚é–€ç”¨èªã‚’æ­£ç¢ºã«ä½¿ç”¨ã—ã€è«–ç†çš„ã§å®¢è¦³çš„ãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚
+## ğŸ“„ GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼
 
-# Audience: å¤§å­¦ç”Ÿ
-å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®çŸ¥è­˜ã‚’æŒã¤å­¦ç¿’è€…å‘ã‘ã«ã€åŸºç¤æ¦‚å¿µã‹ã‚‰å¿œç”¨ã¾ã§æ®µéšçš„ã«èª¬æ˜ã—ã¾ã™ã€‚
+`create-ga`ã‚³ãƒãƒ³ãƒ‰ã§è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã¯XMLå½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ï¼š
 
----
+\```xml
+<?xml version="1.0" encoding="utf-8"?>
+<GADefinitions>
+  <Pair>
+    <Genre>å­¦è¡“è«–æ–‡</Genre>
+    <GenreDescription>å­¦è¡“çš„ã§å³å¯†ãªè¡¨ç¾ã‚’ç”¨ã„ã€å°‚é–€ç”¨èªã‚’æ­£ç¢ºã«ä½¿ç”¨ã—ã€è«–ç†çš„ã§å®¢è¦³çš„ãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚</GenreDescription>
+    <Audience>ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç ”ç©¶è€…</Audience>
+    <AudienceDescription>ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åˆ†é‡ã®ç ”ç©¶è€…å‘ã‘ã«ã€æœ€æ–°ã®ç ”ç©¶å‹•å‘ã‚„ç†è«–çš„èƒŒæ™¯ã‚’å«ã‚€å°‚é–€çš„ãªå†…å®¹ã‚’æä¾›ã—ã¾ã™ã€‚</AudienceDescription>
+  </Pair>
+  <Pair>
+    <Genre>æŠ€è¡“ãƒ–ãƒ­ã‚°</Genre>
+    <GenreDescription>å®Ÿè·µçš„ã§è¦ªã—ã¿ã‚„ã™ã„è¡¨ç¾ã‚’ç”¨ã„ã€å…·ä½“ä¾‹ã‚„ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜ã—ã¾ã™ã€‚</GenreDescription>
+    <Audience>ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…</Audience>
+    <AudienceDescription>ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã³å§‹ã‚ãŸåˆå¿ƒè€…å‘ã‘ã«ã€åŸºç¤çš„ãªæ¦‚å¿µã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¾ã™ã€‚</AudienceDescription>
+  </Pair>
+</GADefinitions>
+\```
 
-# Genre: æŠ€è¡“ãƒ–ãƒ­ã‚°
-å®Ÿè·µçš„ã§è¦ªã—ã¿ã‚„ã™ã„è¡¨ç¾ã‚’ç”¨ã„ã€å…·ä½“ä¾‹ã‚„ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜ã—ã¾ã™ã€‚
+ã¾ãŸã€å„Genreåˆ¥ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆã•ã‚Œã€å¿…è¦ã«å¿œã˜ã¦æ‰‹å‹•ã§ç·¨é›†ã§ãã¾ã™ã€‚
 
-# Audience: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
-å®Ÿå‹™çµŒé¨“ã®ã‚ã‚‹é–‹ç™ºè€…å‘ã‘ã«ã€å®Ÿè£…ã®è©³ç´°ã‚„æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆã‚’é‡è¦–ã—ãŸå†…å®¹ã‚’æä¾›ã—ã¾ã™ã€‚
-\```
+## ğŸ“ å‡ºåŠ›å½¢å¼
 
-## å‡ºåŠ›å½¢å¼
+### ğŸ“„ XMLå½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
 
-å„Genreã”ã¨ã«XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼š
+`generate`ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œã«ã‚ˆã‚Šã€å„Genreã”ã¨ã«XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼š
 
 \```xml
 <?xml version="1.0" ?>
 <QAPairs genre="å­¦è¡“è«–æ–‡">
   <Pair>
-    <Audience>å¤§å­¦ç”Ÿ</Audience>
+    <Audience>ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç ”ç©¶è€…</Audience>
     <Question>Pythonã®è¨­è¨ˆå“²å­¦ã«ãŠã‘ã‚‹ä¸»è¦ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
     <Answer>Pythonã®è¨­è¨ˆå“²å­¦ã¯ã€Œèª­ã¿ã‚„ã™ã•ã€ã‚’é‡è¦–ã—ã¦ãŠã‚Šã€ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„æ§‹æ–‡ãŒç‰¹å¾´ã§ã™ã€‚</Answer>
   </Pair>
 </QAPairs>
 \```
 
-## ã‚µãƒãƒ¼ãƒˆã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«
+### ğŸ¦™ Alpacaå½¢å¼ï¼ˆ`--export-alpaca`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+
+`--export-alpaca`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€æ©Ÿæ¢°å­¦ç¿’ã§åºƒãä½¿ç”¨ã•ã‚Œã‚‹Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼š
+
+\```json
+[
+  {
+    "instruction": "Pythonã®è¨­è¨ˆå“²å­¦ã«ãŠã‘ã‚‹ä¸»è¦ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
+    "input": "",
+    "output": "Pythonã®è¨­è¨ˆå“²å­¦ã¯ã€Œèª­ã¿ã‚„ã™ã•ã€ã‚’é‡è¦–ã—ã¦ãŠã‚Šã€ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„æ§‹æ–‡ãŒç‰¹å¾´ã§ã™ã€‚",
+    "genre": "å­¦è¡“è«–æ–‡",
+    "audience": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç ”ç©¶è€…"
+  },
+  {
+    "instruction": "Pythonã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ãƒ¼å‹è¨€èªã¨ã—ã¦ã®åˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
+    "input": "",
+    "output": "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ãƒ¼å‹ã®ãŸã‚ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸è¦ã§å³åº§ã«ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã§ãã€é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚",
+    "genre": "æŠ€è¡“ãƒ–ãƒ­ã‚°",
+    "audience": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…"
+  }
+]
+\```
+
+### ğŸ“Š è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰
+
+Alpacaå½¢å¼ã§å‡ºåŠ›ã™ã‚‹éš›ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚€README.mdãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ï¼š
+
+- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦**: ã‚¨ãƒ³ãƒˆãƒªæ•°ã€å½¢å¼ã€è¨€èªã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
+- **ã‚¸ãƒ£ãƒ³ãƒ«åˆ†å¸ƒ**: å«ã¾ã‚Œã‚‹ã™ã¹ã¦ã®ã‚¸ãƒ£ãƒ³ãƒ«ã®ãƒªã‚¹ãƒˆ
+- **å¯¾è±¡èª­è€…åˆ†å¸ƒ**: å«ã¾ã‚Œã‚‹ã™ã¹ã¦ã®å¯¾è±¡èª­è€…ã®ãƒªã‚¹ãƒˆ
+- **ä½¿ç”¨æ–¹æ³•**: Hugging Face Datasetsã§ã®èª­ã¿è¾¼ã¿ä¾‹
+- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**: Hugging Face Hubç”¨ã®YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼
+
+### ğŸ“ ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 
+
+\```
+output_directory/
+â”œâ”€â”€ ga/
+â”‚   â”œâ”€â”€ ga_definitions.xml          # ãƒ¡ã‚¤ãƒ³ã®GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«
+â”‚   â”œâ”€â”€ ga_definitions_å­¦è¡“è«–æ–‡.md   # Genreåˆ¥ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
+â”‚   â”œâ”€â”€ ga_definitions_æŠ€è¡“ãƒ–ãƒ­ã‚°.md
+â”‚   â””â”€â”€ ...
+â”œâ”€â”€ qa/
+â”‚   â”œâ”€â”€ å­¦è¡“è«–æ–‡.xml                # Genreåˆ¥Q&Aãƒ•ã‚¡ã‚¤ãƒ«XMLå½¢å¼ï¼‰
+â”‚   â”œâ”€â”€ æŠ€è¡“ãƒ–ãƒ­ã‚°.xml
+â”‚   â””â”€â”€ ...
+â”œâ”€â”€ logs/
+â”‚   â””â”€â”€ raw.md                      # LLMã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹
+â”œâ”€â”€ dataset_alpaca.json             # ğŸ¦™ Alpacaå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ--export-alpacaã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ï¼‰
+â””â”€â”€ README.md                       # ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆ--export-alpacaã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ï¼‰
+\```
+
+## ğŸ¤– ã‚µãƒãƒ¼ãƒˆã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«
 
-### OpenAIï¼ˆç›´æ¥ï¼‰
+### ğŸ”‘ OpenAIï¼ˆç›´æ¥ï¼‰
 \```bash
 export OPENAI_API_KEY="sk-..."
 easy-dataset generate document.txt -g ga.md -m gpt-4o
 \```
 
-### OpenRouterçµŒç”±
+### ğŸŒ OpenRouterçµŒç”±
 \```bash
 export OPENROUTER_API_KEY="sk-or-v1-..."
 easy-dataset generate document.txt -g ga.md -m gpt-4o  # è‡ªå‹•ã§openai/gpt-4oã«å¤‰æ›
 easy-dataset generate document.txt -g ga.md -m claude-3-sonnet  # è‡ªå‹•ã§anthropic/claude-3-sonnetã«å¤‰æ›
 \```
 
-### ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
-- Anthropic: `claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku`
-- Ollama: `ollama/llama3`, `ollama/mistral`
-- ãã®ä»–litellmãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«
+## ğŸ¤— Hugging Face Hubçµ±åˆ
+
+### ğŸ”‘ ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
+
+\```bash
+# Windows (cmd)
+set HUGGINGFACE_TOKEN=hf_your_token_here
+
+# Windows (PowerShell)
+$env:HUGGINGFACE_TOKEN="hf_your_token_here"
+
+# Linux/macOS
+export HUGGINGFACE_TOKEN="hf_your_token_here"
+\```
+
+### ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+
+\```bash
+# ç”Ÿæˆã¨åŒæ™‚ã«Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+uv run easy-dataset generate document.txt \
+  --ga-file ga.xml \
+  --export-alpaca \
+  --upload-hf \
+  --hf-repo-name username/my-dataset
+
+# æ—¢å­˜XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+uv run easy-dataset convert-to-alpaca ./qa_directory \
+  --upload-hf \
+  --hf-repo-name username/my-dataset \
+  --hf-private  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒªãƒã‚¸ãƒˆãƒªã¨ã—ã¦ä½œæˆ
+\```
+
+### ğŸ“¥ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã®ä½¿ç”¨æ–¹æ³•
+
+\```python
+from datasets import load_dataset
+
+# Hugging Face Hubã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
+dataset = load_dataset("username/my-dataset")
+
+# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ã‚’ç¢ºèª
+print(dataset['train'][0])
+# {
+#   'instruction': 'Pythonã®è¨­è¨ˆå“²å­¦ã«ãŠã‘ã‚‹ä¸»è¦ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ',
+#   'input': '',
+#   'output': 'Pythonã®è¨­è¨ˆå“²å­¦ã¯ã€Œèª­ã¿ã‚„ã™ã•ã€ã‚’é‡è¦–ã—ã¦ãŠã‚Š...',
+#   'genre': 'å­¦è¡“è«–æ–‡',
+#   'audience': 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç ”ç©¶è€…'
+# }
+
+# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
+def format_instruction(example):
+    return f"### æŒ‡ç¤º:\n{example['instruction']}\n\n### å›ç­”:\n{example['output']}"
+
+formatted_dataset = dataset.map(lambda x: {"text": format_instruction(x)})
+\```
+
+### ğŸ“Š è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®ä¾‹
+
+ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹README.mdã«ã¯ä»¥ä¸‹ã®æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ï¼š
+
+\```yaml
+---
+license: mit
+task_categories:
+- question-answering
+- text-generation
+language:
+- ja
+tags:
+- alpaca
+- qa
+- japanese
+size_categories:
+- n<1K  # ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦è‡ªå‹•è¨­å®š
+---
+\```
+
+- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦**: ã‚¨ãƒ³ãƒˆãƒªæ•°ã€å½¢å¼ã€è¨€èªã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
+- **ã‚¸ãƒ£ãƒ³ãƒ«ãƒ»å¯¾è±¡èª­è€…åˆ†å¸ƒ**: å«ã¾ã‚Œã‚‹ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒª
+- **ä½¿ç”¨æ–¹æ³•**: Hugging Face Datasetsã§ã®èª­ã¿è¾¼ã¿ä¾‹
+- **ç”Ÿæˆãƒ„ãƒ¼ãƒ«æƒ…å ±**: easy-dataset-cliã¸ã®ãƒªãƒ³ã‚¯
+
+## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
+
+MIT License
+
+## ğŸ”— å‚è€ƒæƒ…å ±
+
+æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®OSSã¨è«–æ–‡ã‚’å‚è€ƒã«é–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ï¼š
 
-### æ¨å¥¨ãƒ¢ãƒ‡ãƒ«
-- **é«˜å“è³ª**: `gpt-4o`, `claude-3-opus`
-- **ãƒãƒ©ãƒ³ã‚¹**: `gpt-4`, `claude-3-sonnet`
-- **é«˜é€Ÿ**: `gpt-3.5-turbo`, `claude-3-haiku`
+### ğŸ“¦ å‚è€ƒOSS
+- **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**
 
-## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
+### ğŸ“„ å‚è€ƒè«–æ–‡
+- **[Dataset Generation for Instruction Tuning](https://arxiv.org/html/2507.04009v1)**
 
-MIT License
\ No newline at end of file
diff --git a/easy_dataset_cli/alpaca_converter.py b/easy_dataset_cli/alpaca_converter.py
new file mode 100644
index 0000000..0a148ea
--- /dev/null
+++ b/easy_dataset_cli/alpaca_converter.py
@@ -0,0 +1,238 @@
+# easy_dataset_cli/alpaca_converter.py
+"""ã‚¢ãƒ«ãƒ‘ã‚«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã¸ã®å¤‰æ›ã¨Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½"""
+
+import json
+import xml.etree.ElementTree as ET
+from pathlib import Path
+from typing import List, Dict, Optional
+from rich.console import Console
+from huggingface_hub import HfApi, create_repo
+from datasets import Dataset
+import os
+
+console = Console()
+
+def xml_to_alpaca_format(xml_file_path: Path) -> List[Dict[str, str]]:
+    """XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹"""
+    alpaca_data = []
+    
+    try:
+        tree = ET.parse(xml_file_path)
+        root = tree.getroot()
+        
+        genre = root.get('genre', 'Unknown')
+        
+        for pair in root.findall('Pair'):
+            audience_elem = pair.find('Audience')
+            question_elem = pair.find('Question')
+            answer_elem = pair.find('Answer')
+            
+            if all([audience_elem is not None, question_elem is not None, answer_elem is not None]):
+                audience = audience_elem.text or ""
+                question = question_elem.text or ""
+                answer = answer_elem.text or ""
+                
+                # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã«å¤‰æ›
+                alpaca_entry = {
+                    "instruction": question,
+                    "input": "",  # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã¯é€šå¸¸ç©ºæ–‡å­—
+                    "output": answer,
+                    "genre": genre,
+                    "audience": audience
+                }
+                alpaca_data.append(alpaca_entry)
+                
+    except ET.ParseError as e:
+        console.print(f"[bold red]XMLãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
+    except Exception as e:
+        console.print(f"[bold red]äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
+    
+    return alpaca_data
+
+def convert_all_xml_to_alpaca(qa_dir: Path, output_file: Path) -> List[Dict[str, str]]:
+    """QAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã«å¤‰æ›"""
+    all_alpaca_data = []
+    
+    xml_files = list(qa_dir.glob("*.xml"))
+    
+    if not xml_files:
+        console.print(f"[yellow]XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {qa_dir}[/yellow]")
+        return all_alpaca_data
+    
+    console.print(f"[green]{len(xml_files)}å€‹ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›ä¸­...[/green]")
+    
+    for xml_file in xml_files:
+        console.print(f"[dim]å¤‰æ›ä¸­: {xml_file.name}[/dim]")
+        alpaca_data = xml_to_alpaca_format(xml_file)
+        all_alpaca_data.extend(alpaca_data)
+        console.print(f"[green]âœ“[/green] {len(alpaca_data)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ")
+    
+    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
+    with open(output_file, 'w', encoding='utf-8') as f:
+        json.dump(all_alpaca_data, f, ensure_ascii=False, indent=2)
+    
+    console.print(f"[bold green]âœ“[/bold green] åˆè¨ˆ{len(all_alpaca_data)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ "
+                  f"[cyan]{output_file}[/cyan] ã«ä¿å­˜ã—ã¾ã—ãŸ")
+    
+    return all_alpaca_data
+
+def upload_to_huggingface(
+    dataset_data: List[Dict[str, str]],
+    repo_name: str,
+    hf_token: Optional[str] = None,
+    private: bool = False,
+    commit_message: str = "Upload alpaca dataset",
+    readme_file: Optional[Path] = None
+) -> bool:
+    """Hugging Face Hubã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
+    
+    if not hf_token:
+        hf_token = os.getenv("HUGGINGFACE_TOKEN")
+        if not hf_token:
+            console.print("[bold red]HUGGINGFACE_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+            console.print("[yellow]ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„[/yellow]")
+            return False
+    
+    try:
+        # HfApiã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
+        api = HfApi(token=hf_token)
+        
+        # ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
+        try:
+            create_repo(
+                repo_id=repo_name,
+                token=hf_token,
+                repo_type="dataset",
+                private=private,
+                exist_ok=True
+            )
+            console.print(f"[green]âœ“[/green] ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆ/ç¢ºèªã—ã¾ã—ãŸ: [cyan]{repo_name}[/cyan]")
+        except Exception as e:
+            console.print(f"[yellow]ãƒªãƒã‚¸ãƒˆãƒªä½œæˆæ™‚ã®è­¦å‘Š: {e}[/yellow]")
+        
+        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
+        dataset = Dataset.from_list(dataset_data)
+        
+        # Hugging Face Hubã«ãƒ—ãƒƒã‚·ãƒ¥
+        dataset.push_to_hub(
+            repo_id=repo_name,
+            token=hf_token,
+            commit_message=commit_message,
+            private=private
+        )
+        
+        # README.mdãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+        if readme_file and readme_file.exists():
+            try:
+                api.upload_file(
+                    path_or_fileobj=readme_file,
+                    path_in_repo="README.md",
+                    repo_id=repo_name,
+                    repo_type="dataset",
+                    commit_message=f"Update README.md",
+                    token=hf_token
+                )
+                console.print(f"[green]âœ“[/green] README.mdã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ!")
+            except Exception as readme_error:
+                console.print(f"[yellow]README.mdã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®è­¦å‘Š: {readme_error}[/yellow]")
+        
+        console.print(f"[bold green]âœ“[/bold green] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ!")
+        console.print(f"[cyan]https://huggingface.co/datasets/{repo_name}[/cyan]")
+        
+        return True
+        
+    except Exception as e:
+        console.print(f"[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
+        return False
+
+def create_dataset_card(
+    dataset_data: List[Dict[str, str]], 
+    output_file: Path,
+    dataset_name: str = "Generated QA Dataset"
+) -> None:
+    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆREADME.mdï¼‰ã‚’ç”Ÿæˆ"""
+    
+    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
+    total_entries = len(dataset_data)
+    genres = set(entry.get('genre', 'Unknown') for entry in dataset_data)
+    audiences = set(entry.get('audience', 'Unknown') for entry in dataset_data)
+    
+    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®å†…å®¹
+    card_content = f"""---
+license: mit
+task_categories:
+- question-answering
+- text-generation
+language:
+- ja
+tags:
+- alpaca
+- qa
+- japanese
+size_categories:
+- {get_size_category(total_entries)}
+---
+
+# {dataset_name}
+
+ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€easy-dataset-cliã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã®æ—¥æœ¬èªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
+
+## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦
+
+- **ç·ã‚¨ãƒ³ãƒˆãƒªæ•°**: {total_entries:,}
+- **å½¢å¼**: Alpacaå½¢å¼
+- **è¨€èª**: æ—¥æœ¬èª
+- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: MIT
+
+## ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
+
+å„ã‚¨ãƒ³ãƒˆãƒªã¯ä»¥ä¸‹ã®å½¢å¼ã§ã™ï¼š
+
+\```json
+{{
+  "instruction": "è³ªå•æ–‡",
+  "input": "",
+  "output": "å›ç­”æ–‡",
+  "genre": "ã‚¸ãƒ£ãƒ³ãƒ«",
+  "audience": "å¯¾è±¡èª­è€…"
+}}
+\```
+
+## ã‚¸ãƒ£ãƒ³ãƒ«åˆ†å¸ƒ
+
+å«ã¾ã‚Œã‚‹ã‚¸ãƒ£ãƒ³ãƒ«:
+{chr(10).join(f"- {genre}" for genre in sorted(genres))}
+
+## å¯¾è±¡èª­è€…åˆ†å¸ƒ
+
+å«ã¾ã‚Œã‚‹å¯¾è±¡èª­è€…:
+{chr(10).join(f"- {audience}" for audience in sorted(audiences))}
+
+## ä½¿ç”¨æ–¹æ³•
+
+\```python
+from datasets import load_dataset
+
+dataset = load_dataset("{dataset_name}")
+\```
+
+## ç”Ÿæˆãƒ„ãƒ¼ãƒ«
+
+ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[easy-dataset-cli](https://github.com/Sunwood-ai-labsII/easy-dataset-cli)ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚
+"""
+    
+    output_file.write_text(card_content, encoding='utf-8')
+    console.print(f"[green]âœ“[/green] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: [cyan]{output_file}[/cyan]")
+
+def get_size_category(count: int) -> str:
+    """ã‚¨ãƒ³ãƒˆãƒªæ•°ã«åŸºã¥ã„ã¦ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™"""
+    if count < 1000:
+        return "n<1K"
+    elif count < 10000:
+        return "1K<n<10K"
+    elif count < 100000:
+        return "10K<n<100K"
+    elif count < 1000000:
+        return "100K<n<1M"
+    else:
+        return "n>1M"
diff --git a/easy_dataset_cli/core.py b/easy_dataset_cli/core.py
index c9db332..056452b 100644
--- a/easy_dataset_cli/core.py
+++ b/easy_dataset_cli/core.py
@@ -1,519 +1,53 @@
 # easy_dataset_cli/core.py
-"""ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã€Q&Aç”Ÿæˆã€XMLå¤‰æ›"""
-
-import os
-import xml.etree.ElementTree as ET
-from xml.dom import minidom
-from collections import defaultdict
-from pathlib import Path
-from typing import List, Dict
-import mistune
-from langchain_text_splitters import RecursiveCharacterTextSplitter
-from litellm import completion
-from rich.console import Console
-from dotenv import load_dotenv
-
-from .prompts import get_qa_generation_prompt, get_ga_definition_generation_prompt
-
-# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
-load_dotenv()
-
-console = Console()
-
-
-def parse_ga_file(file_path: Path) -> List[Dict[str, Dict[str, str]]]:
-    """XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹"""
-    text = file_path.read_text(encoding="utf-8")
-    pairs = []
-    console.print(f"[dim]GAãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {file_path}[/dim]")
-    console.print(f"[dim]ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹é•·: {len(text)} æ–‡å­—[/dim]")
-    
-    try:
-        # XMLã‹ã‚‰<GADefinitions>éƒ¨åˆ†ã‚’æŠ½å‡º
-        xml_start = text.find("<GADefinitions>")
-        xml_end = text.rfind("</GADefinitions>")
-        console.print(f"[dim]XMLé–‹å§‹ä½ç½®: {xml_start}, çµ‚äº†ä½ç½®: {xml_end}[/dim]")
-        
-        if xml_start != -1 and xml_end != -1:
-            clean_xml = text[xml_start: xml_end + len("</GADefinitions>")]
-            console.print(f"[dim]æŠ½å‡ºã•ã‚ŒãŸXMLé•·: {len(clean_xml)} æ–‡å­—[/dim]")
-            
-            root = ET.fromstring(clean_xml)
-            pair_nodes = root.findall('Pair')
-            console.print(f"[dim]è¦‹ã¤ã‹ã£ãŸPairãƒãƒ¼ãƒ‰æ•°: {len(pair_nodes)}[/dim]")
-            
-            for i, pair_node in enumerate(pair_nodes):
-                genre_node = pair_node.find('Genre')
-                audience_node = pair_node.find('Audience')
-                
-                if genre_node is not None and audience_node is not None:
-                    genre_title_node = genre_node.find('Title')
-                    genre_desc_node = genre_node.find('Description')
-                    audience_title_node = audience_node.find('Title')
-                    audience_desc_node = audience_node.find('Description')
-                    
-                    has_all = all([
-                        genre_title_node is not None and genre_title_node.text and genre_title_node.text.strip(),
-                        genre_desc_node is not None and genre_desc_node.text and genre_desc_node.text.strip(),
-                        audience_title_node is not None and audience_title_node.text and audience_title_node.text.strip(),
-                        audience_desc_node is not None and audience_desc_node.text and audience_desc_node.text.strip()
-                    ])
-                    
-                    console.print(f"[dim]Pair {i+1}: {'âœ“' if has_all else 'âœ—'} {genre_title_node.text if genre_title_node is not None else 'None'}[/dim]")
-                    
-                    if has_all:
-                        pairs.append({
-                            "genre": {
-                                "title": genre_title_node.text.strip(),
-                                "description": genre_desc_node.text.strip()
-                            },
-                            "audience": {
-                                "title": audience_title_node.text.strip(),
-                                "description": audience_desc_node.text.strip()
-                            }
-                        })
-        
-        # XMLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å¾“æ¥ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è§£æã‚’è©¦è¡Œ
-        if not pairs:
-            console.print("[yellow]XMLã‹ã‚‰ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å†è©¦è¡Œ[/yellow]")
-            pairs = parse_ga_markdown_fallback(text)
-            
-    except ET.ParseError as e:
-        console.print(f"[yellow]XMLè§£æã‚¨ãƒ©ãƒ¼: {e}[/yellow]")
-        # XMLè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è§£æã‚’è©¦è¡Œ
-        pairs = parse_ga_markdown_fallback(text)
-    
-    console.print(f"[dim]æœ€çµ‚çš„ã«è§£æã•ã‚ŒãŸãƒšã‚¢æ•°: {len(pairs)}[/dim]")
-    return pairs
-
-
-def parse_ga_markdown_fallback(text: str) -> List[Dict[str, Dict[str, str]]]:
-    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
-    pairs = []
-    sections = text.split('---')
-    
-    for section in sections:
-        if not section.strip():
-            continue
-
-        ast = mistune.create_markdown(renderer=None)(section)
-        genre = {"title": "", "description": ""}
-        audience = {"title": "", "description": ""}
-        current_type = None
-
-        for node in ast:
-            if node['type'] == 'heading':
-                header_text = "".join(child['text'] for child in node['children'])
-                if 'genre' in header_text.lower():
-                    current_type = 'genre'
-                    genre['title'] = header_text.replace('Genre:', '').strip()
-                elif 'audience' in header_text.lower():
-                    current_type = 'audience'
-                    audience['title'] = header_text.replace('Audience:', '').strip()
-            elif node['type'] == 'paragraph':
-                description = "".join(child['text'] for child in node['children'])
-                if current_type == 'genre':
-                    genre['description'] = description
-                elif current_type == 'audience':
-                    audience['description'] = description
-
-        if genre['title'] and audience['title']:
-            pairs.append({"genre": genre, "audience": audience})
-    
-    return pairs
-
-
-def split_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
-    """LangChainã®TextSplitterã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹"""
-    text_splitter = RecursiveCharacterTextSplitter(
-        chunk_size=chunk_size,
-        chunk_overlap=chunk_overlap,
-        length_function=len,
-        is_separator_regex=False,
-    )
-    docs = text_splitter.create_documents([text])
-    return [doc.page_content for doc in docs]
-
-
-def generate_qa_for_chunk_with_ga(
-    chunk: str, model: str, ga_pair: Dict[str, Dict[str, str]], logs_dir: Path = None, num_qa_pairs: int = None
-) -> List[Dict[str, str]]:
-    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
-    prompt_template = get_qa_generation_prompt()
-    prompt = prompt_template.format(
-        context=chunk,
-        genre_title=ga_pair['genre']['title'],
-        genre_description=ga_pair['genre']['description'],
-        audience_title=ga_pair['audience']['title'],
-        audience_description=ga_pair['audience']['description'],
-        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
-    )
-
-    messages = [
-        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
-        {"role": "user", "content": prompt}
-    ]
-
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
-    
-    try:
-        response = completion(model=model, messages=messages)
-        xml_content = response.choices[0].message.content
-        
-        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
-        if logs_dir:
-            genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-            audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-            raw_filename = f"qa_raw_{genre_safe}_{audience_safe}.md"
-            raw_file_path = logs_dir / raw_filename
-            raw_file_path.write_text(xml_content, encoding="utf-8")
-        
-        qa_pairs = []
-
-        # LLMã‹ã‚‰ã®å‡ºåŠ›ã«ã¯ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€XMLéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
-        xml_start = xml_content.find("<QAPairs>")
-        xml_end = xml_content.rfind("</QAPairs>")
-
-        if xml_start != -1 and xml_end != -1:
-            clean_xml = xml_content[xml_start: xml_end + len("</QAPairs>")]
-            
-            try:
-                root = ET.fromstring(clean_xml)
-
-                for pair_node in root.findall('Pair'):
-                    question_node = pair_node.find('Question')
-                    answer_node = pair_node.find('Answer')
-
-                    if question_node is not None and answer_node is not None:
-                        qa_pairs.append({
-                            "question": question_node.text or "",
-                            "answer": answer_node.text or ""
-                        })
-            
-            except ET.ParseError:
-                # XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æ
-                console.print(f"[yellow]XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€æ‰‹å‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
-                qa_pairs = parse_qa_from_text_fallback(clean_xml)
-
-        return qa_pairs
-
-    except ET.ParseError as parse_error:
-        console.print(
-            f"[bold red]LLMãŒç”Ÿæˆã—ãŸXMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ:[/bold red] {parse_error}"
-        )
-        console.print(f"[dim]å—ä¿¡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ: {xml_content[:200]}...[/dim]")
-        return []
-    except Exception as general_error:
-        console.print(
-            f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] "
-            f"{general_error}"
-        )
-        console.print(
-            f"[dim]Genre: {ga_pair['genre']['title']}, "
-            f"Audience: {ga_pair['audience']['title']}[/dim]"
-        )
-        return []
-
-
-def generate_ga_definitions(text_content: str, model: str, num_ga_pairs: int = None) -> str:
-    """litellmã‚’ä½¿ã„ã€å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã®XMLã‚’ç”Ÿæˆã™ã‚‹"""
-    # LLMã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã™ãã‚‹ã¨ã‚³ã‚¹ãƒˆã‚„æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ãŸã‚ã€å…ˆé ­éƒ¨åˆ†ã«é™å®šã™ã‚‹
-    context = text_content[:8000]
-    console.print(f"[dim]ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(context)} æ–‡å­—[/dim]")
-
-    prompt_template = get_ga_definition_generation_prompt()
-    prompt = prompt_template.format(
-        context=context,
-        num_ga_pairs=num_ga_pairs if num_ga_pairs is not None else "3-5å€‹ã®"
-    )
-
-    messages = [
-        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
-        {"role": "user", "content": prompt}
-    ]
-
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    api_key = os.getenv("OPENROUTER_API_KEY", "")
-    if not api_key:
-        console.print("[bold red]OPENROUTER_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
-        raise ValueError("OPENROUTER_API_KEYãŒå¿…è¦ã§ã™")
-    
-    os.environ["OPENROUTER_API_KEY"] = api_key
-    
-    # OpenRouterã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
-    if "openrouter" not in model and not model.startswith("openrouter/"):
-        if model.startswith("gpt-"):
-            model = f"openrouter/openai/{model}"
-        elif model.startswith("claude-"):
-            model = f"openrouter/anthropic/{model}"
-        else:
-            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§openrouterãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
-            model = f"openrouter/{model}"
-
-    try:
-        response = completion(model=model, messages=messages)
-        xml_content = response.choices[0].message.content
-        console.print(f"[dim]LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(xml_content)} æ–‡å­—[/dim]")
-        return xml_content
-    except Exception as error:
-        console.print(f"[bold red]GAå®šç¾©ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {error}")
-        raise
-
-
-def parse_ga_definitions_from_xml(xml_content: str) -> List[Dict[str, Dict[str, str]]]:
-    """XMLå½¢å¼ã®GAå®šç¾©ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹"""
-    pairs = []
-    
-    try:
-        # XMLã‚¿ã‚°ã‹ã‚‰<GADefinitions>éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç„¡è¦–ï¼‰
-        xml_start = xml_content.find("<GADefinitions>")
-        xml_end = xml_content.rfind("</GADefinitions>")
-        
-        if xml_start != -1 and xml_end != -1:
-            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
-            
-            # XMLã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
-            import html
-            clean_xml = html.unescape(clean_xml)
-            
-            root = ET.fromstring(clean_xml)
-            pair_nodes = root.findall('Pair')
-            console.print(f"[dim]è¦‹ã¤ã‹ã£ãŸPairãƒãƒ¼ãƒ‰æ•°: {len(pair_nodes)}[/dim]")
-            
-            for i, pair_node in enumerate(pair_nodes):
-                genre_node = pair_node.find('Genre')
-                audience_node = pair_node.find('Audience')
-                
-                if genre_node is not None and audience_node is not None:
-                    genre_title_node = genre_node.find('Title')
-                    genre_desc_node = genre_node.find('Description')
-                    audience_title_node = audience_node.find('Title')
-                    audience_desc_node = audience_node.find('Description')
-                    
-                    # ã‚ˆã‚Šè©³ç´°ãªãƒã‚§ãƒƒã‚¯
-                    has_genre_title = genre_title_node is not None and genre_title_node.text and genre_title_node.text.strip()
-                    has_genre_desc = genre_desc_node is not None and genre_desc_node.text and genre_desc_node.text.strip()
-                    has_audience_title = audience_title_node is not None and audience_title_node.text and audience_title_node.text.strip()
-                    has_audience_desc = audience_desc_node is not None and audience_desc_node.text and audience_desc_node.text.strip()
-                    
-                    if all([has_genre_title, has_genre_desc, has_audience_title, has_audience_desc]):
-                        pairs.append({
-                            "genre": {
-                                "title": genre_title_node.text.strip(),
-                                "description": genre_desc_node.text.strip()
-                            },
-                            "audience": {
-                                "title": audience_title_node.text.strip(),
-                                "description": audience_desc_node.text.strip()
-                            }
-                        })
-                        console.print(f"[green]âœ“[/green] {genre_title_node.text.strip()} x {audience_title_node.text.strip()}")
-                    else:
-                        console.print(f"[yellow]âš [/yellow] Pair {i+1}: å¿…è¦ãªè¦ç´ ãŒä¸è¶³")
-                else:
-                    console.print(f"[yellow]âš [/yellow] Pair {i+1}: Genreã¾ãŸã¯Audienceãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
-        else:
-            console.print("[yellow]GADefinitionsã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
-                        
-    except ET.ParseError as parse_error:
-        console.print(f"[bold red]GAå®šç¾©XMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ:[/bold red] {parse_error}")
-        console.print(f"[dim]å•é¡Œã®ã‚ã‚‹XML: {xml_content[xml_start:xml_start+200] if xml_start != -1 else xml_content[:200]}...[/dim]")
-        
-        # XMLã‚¨ãƒ©ãƒ¼ã®å ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æã‚’è©¦è¡Œ
-        console.print("[yellow]æ‰‹å‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
-        pairs = parse_ga_from_text_fallback(xml_content)
-        
-    except Exception as e:
-        console.print(f"[bold red]äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
-    
-    return pairs
-
-
-def parse_ga_from_text_fallback(content: str) -> List[Dict[str, Dict[str, str]]]:
-    """XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è§£æ"""
-    pairs = []
-    
-    try:
-        # <Pair>ã‚¿ã‚°ã§åˆ†å‰²
-        pair_sections = content.split('<Pair>')
-        
-        for section in pair_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
-            if '</Pair>' not in section:
-                continue
-                
-            pair_content = section.split('</Pair>')[0]
-            
-            # Titleè¦ç´ ã‚’æŠ½å‡º
-            genre_title = extract_text_between_tags(pair_content, 'Genre', 'Title')
-            genre_desc = extract_text_between_tags(pair_content, 'Genre', 'Description')
-            audience_title = extract_text_between_tags(pair_content, 'Audience', 'Title')
-            audience_desc = extract_text_between_tags(pair_content, 'Audience', 'Description')
-            
-            if all([genre_title, genre_desc, audience_title, audience_desc]):
-                pairs.append({
-                    "genre": {
-                        "title": genre_title.strip(),
-                        "description": genre_desc.strip()
-                    },
-                    "audience": {
-                        "title": audience_title.strip(),
-                        "description": audience_desc.strip()
-                    }
-                })
-                console.print(f"[green]âœ“[/green] (æ‰‹å‹•è§£æ) {genre_title} x {audience_title}")
-    
-    except Exception as e:
-        console.print(f"[red]æ‰‹å‹•è§£æã‚‚å¤±æ•—:[/red] {e}")
-    
-    return pairs
-
-
-def extract_text_between_tags(content: str, parent_tag: str, child_tag: str) -> str:
-    """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚°é–“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
-    try:
-        # è¦ªã‚¿ã‚°å†…ã‚’æ¢ã™
-        parent_start = content.find(f'<{parent_tag}>')
-        parent_end = content.find(f'</{parent_tag}>')
-        
-        if parent_start == -1 or parent_end == -1:
-            return ""
-            
-        parent_content = content[parent_start:parent_end]
-        
-        # å­ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
-        child_start = parent_content.find(f'<{child_tag}>')
-        child_end = parent_content.find(f'</{child_tag}>')
-        
-        if child_start == -1 or child_end == -1:
-            return ""
-            
-        return parent_content[child_start + len(f'<{child_tag}>'):child_end]
-    
-    except Exception:
-        return ""
-
-
-def parse_qa_from_text_fallback(content: str) -> List[Dict[str, str]]:
-    """Q&A XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è§£æ"""
-    qa_pairs = []
-    
-    try:
-        # <Pair>ã‚¿ã‚°ã§åˆ†å‰²
-        pair_sections = content.split('<Pair>')
-        
-        for section in pair_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
-            if '</Pair>' not in section:
-                continue
-                
-            pair_content = section.split('</Pair>')[0]
-            
-            # Question ã¨ Answer ã‚’æŠ½å‡º
-            question = extract_simple_tag_content(pair_content, 'Question')
-            answer = extract_simple_tag_content(pair_content, 'Answer')
-            
-            if question and answer:
-                qa_pairs.append({
-                    "question": question.strip(),
-                    "answer": answer.strip()
-                })
-                console.print(f"[green]âœ“[/green] (æ‰‹å‹•è§£æ) Q&Aè¿½åŠ ")
-    
-    except Exception as e:
-        console.print(f"[red]Q&Aæ‰‹å‹•è§£æã‚‚å¤±æ•—:[/red] {e}")
-    
-    return qa_pairs
-
-
-def extract_simple_tag_content(content: str, tag: str) -> str:
-    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
-    try:
-        start_tag = f'<{tag}>'
-        end_tag = f'</{tag}>'
-        
-        start_pos = content.find(start_tag)
-        end_pos = content.find(end_tag)
-        
-        if start_pos == -1 or end_pos == -1:
-            return ""
-            
-        return content[start_pos + len(start_tag):end_pos]
-    
-    except Exception:
-        return ""
-
-
-def create_output_directories(base_dir: Path) -> Dict[str, Path]:
-    """å‡ºåŠ›ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆã™ã‚‹"""
-    directories = {
-        "base": base_dir,
-        "ga": base_dir / "ga",
-        "logs": base_dir / "logs", 
-        "qa": base_dir / "qa"
-    }
-    
-    for dir_path in directories.values():
-        dir_path.mkdir(parents=True, exist_ok=True)
-    
-    return directories
-
-
-def save_ga_definitions_by_genre(ga_pairs: List[Dict[str, Dict[str, str]]], ga_dir: Path) -> None:
-    """GAãƒšã‚¢ã‚’Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
-    genre_groups = defaultdict(list)
-    
-    # Genreã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
-    for pair in ga_pairs:
-        genre_title = pair['genre']['title']
-        genre_groups[genre_title].append(pair)
-    
-    # å„Genreã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
-    for genre_title, pairs in genre_groups.items():
-        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›
-        safe_filename = "".join(c for c in genre_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
-        safe_filename = safe_filename.replace(' ', '_').lower()
-        
-        file_path = ga_dir / f"ga_definitions_{safe_filename}.md"
-        
-        content = f"# {genre_title}\n\n"
-        
-        for pair in pairs:
-            content += f"## Genre: {pair['genre']['title']}\n"
-            content += f"{pair['genre']['description']}\n\n"
-            content += f"## Audience: {pair['audience']['title']}\n"
-            content += f"{pair['audience']['description']}\n\n"
-            content += "---\n\n"
-        
-        file_path.write_text(content, encoding="utf-8")
-        console.print(f"[green]GAå®šç¾©ã‚’ä¿å­˜ã—ã¾ã—ãŸ:[/green] {file_path}")
-
-
-def convert_to_xml_by_genre(all_qa_pairs: List[Dict[str, str]]) -> Dict[str, str]:
-    """Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’Genreã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€æ•´å½¢ã•ã‚ŒãŸXMLæ–‡å­—åˆ—ã®è¾æ›¸ã«å¤‰æ›ã™ã‚‹"""
-    grouped_by_genre = defaultdict(list)
-
-    for item in all_qa_pairs:
-        grouped_by_genre[item["genre"]].append(item)
-
-    xml_outputs = {}
-    for genre, pairs in grouped_by_genre.items():
-        root = ET.Element("QAPairs")
-        root.set("genre", genre)
-
-        for item in pairs:
-            pair_elem = ET.SubElement(root, "Pair")
-
-            audience_elem = ET.SubElement(pair_elem, "Audience")
-            audience_elem.text = item["audience"]
-
-            question_elem = ET.SubElement(pair_elem, "Question")
-            question_elem.text = item["question"]
-
-            answer_elem = ET.SubElement(pair_elem, "Answer")
-            answer_elem.text = item["answer"]
-
-        rough_string = ET.tostring(root, 'utf-8')
-        reparsed = minidom.parseString(rough_string)
-        xml_outputs[genre] = reparsed.toprettyxml(indent="  ")
-
-    return xml_outputs
+"""çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ©Ÿèƒ½ã‚’çµ±åˆ"""
+
+# å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
+from .ga_parser import (
+    parse_ga_file,
+    parse_ga_definitions_from_xml
+)
+from .qa_generator import (
+    generate_qa_for_chunk_with_ga,
+    generate_qa_for_chunk_with_ga_and_fulltext,
+    generate_ga_definitions
+)
+from .text_splitter import split_text
+from .xml_utils import convert_to_xml_by_genre
+from .file_utils import (
+    create_output_directories,
+    save_ga_definitions_by_genre,
+    sanitize_filename
+)
+from .alpaca_converter import (
+    convert_all_xml_to_alpaca,
+    upload_to_huggingface,
+    create_dataset_card
+)
+
+# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€ã™ã¹ã¦ã®é–¢æ•°ã‚’å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
+__all__ = [
+    # GAè§£æé–¢é€£
+    'parse_ga_file',
+    'parse_ga_definitions_from_xml',
+    
+    # Q&Aç”Ÿæˆé–¢é€£
+    'generate_qa_for_chunk_with_ga',
+    'generate_qa_for_chunk_with_ga_and_fulltext',
+    'generate_ga_definitions',
+    
+    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
+    'split_text',
+    
+    # XMLå‡¦ç†
+    'convert_to_xml_by_genre',
+    
+    # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
+    'create_output_directories',
+    'save_ga_definitions_by_genre',
+    'sanitize_filename',
+    
+    # ã‚¢ãƒ«ãƒ‘ã‚«å¤‰æ›ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+    'convert_all_xml_to_alpaca',
+    'upload_to_huggingface',
+    'create_dataset_card'
+]
diff --git a/easy_dataset_cli/file_utils.py b/easy_dataset_cli/file_utils.py
new file mode 100644
index 0000000..f42280a
--- /dev/null
+++ b/easy_dataset_cli/file_utils.py
@@ -0,0 +1,59 @@
+# easy_dataset_cli/file_utils.py
+"""ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œé–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
+
+from pathlib import Path
+from typing import Dict, List
+from collections import defaultdict
+from rich.console import Console
+
+console = Console()
+
+
+def create_output_directories(base_dir: Path) -> Dict[str, Path]:
+    """å‡ºåŠ›ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆã™ã‚‹"""
+    directories = {
+        "base": base_dir,
+        "ga": base_dir / "ga",
+        "logs": base_dir / "logs",
+        "qa": base_dir / "qa"
+    }
+
+    for dir_path in directories.values():
+        dir_path.mkdir(parents=True, exist_ok=True)
+
+    return directories
+
+
+def save_ga_definitions_by_genre(ga_pairs: List[Dict[str, Dict[str, str]]], ga_dir: Path) -> None:
+    """GAãƒšã‚¢ã‚’Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
+    genre_groups = defaultdict(list)
+
+    # Genreã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
+    for pair in ga_pairs:
+        genre_title = pair['genre']['title']
+        genre_groups[genre_title].append(pair)
+
+    # å„Genreã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
+    for genre_title, pairs in genre_groups.items():
+        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›
+        safe_filename = "".join(c for c in genre_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
+        safe_filename = safe_filename.replace(' ', '_').lower()
+
+        file_path = ga_dir / f"ga_definitions_{safe_filename}.md"
+
+        content = f"# {genre_title}\n\n"
+
+        for pair in pairs:
+            content += f"## Genre: {pair['genre']['title']}\n"
+            content += f"{pair['genre']['description']}\n\n"
+            content += f"## Audience: {pair['audience']['title']}\n"
+            content += f"{pair['audience']['description']}\n\n"
+            content += "---\n\n"
+
+        file_path.write_text(content, encoding="utf-8")
+        console.print(f"[green]GAå®šç¾©ã‚’ä¿å­˜ã—ã¾ã—ãŸ:[/green] {file_path}")
+
+
+def sanitize_filename(name: str) -> str:
+    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
+    return "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).rstrip()
diff --git a/easy_dataset_cli/ga_parser.py b/easy_dataset_cli/ga_parser.py
new file mode 100644
index 0000000..fb9aa16
--- /dev/null
+++ b/easy_dataset_cli/ga_parser.py
@@ -0,0 +1,182 @@
+# easy_dataset_cli/ga_parser.py
+"""GAå®šç¾©ã®è§£æé–¢é€£æ©Ÿèƒ½"""
+
+import xml.etree.ElementTree as ET
+from pathlib import Path
+from typing import List, Dict
+import mistune
+from rich.console import Console
+
+console = Console()
+
+
+def parse_ga_file(file_path: Path) -> List[Dict[str, Dict[str, str]]]:
+    """XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹"""
+    text = file_path.read_text(encoding="utf-8")
+    pairs = []
+    console.print(f"[dim]GAãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {file_path}[/dim]")
+    console.print(f"[dim]ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹é•·: {len(text)} æ–‡å­—[/dim]")
+
+    try:
+        # XMLã‹ã‚‰<GADefinitions>éƒ¨åˆ†ã‚’æŠ½å‡º
+        xml_start = text.find("<GADefinitions>")
+        xml_end = text.rfind("</GADefinitions>")
+        console.print(f"[dim]XMLé–‹å§‹ä½ç½®: {xml_start}, çµ‚äº†ä½ç½®: {xml_end}[/dim]")
+
+        if xml_start != -1 and xml_end != -1:
+            clean_xml = text[xml_start: xml_end + len("</GADefinitions>")]
+            console.print(f"[dim]æŠ½å‡ºã•ã‚ŒãŸXMLé•·: {len(clean_xml)} æ–‡å­—[/dim]")
+
+            root = ET.fromstring(clean_xml)
+            pair_nodes = root.findall('Pair')
+            console.print(f"[dim]è¦‹ã¤ã‹ã£ãŸPairãƒãƒ¼ãƒ‰æ•°: {len(pair_nodes)}[/dim]")
+
+            for i, pair_node in enumerate(pair_nodes):
+                genre_node = pair_node.find('Genre')
+                audience_node = pair_node.find('Audience')
+
+                if genre_node is not None and audience_node is not None:
+                    genre_title_node = genre_node.find('Title')
+                    genre_desc_node = genre_node.find('Description')
+                    audience_title_node = audience_node.find('Title')
+                    audience_desc_node = audience_node.find('Description')
+
+                    has_all = all([
+                        genre_title_node is not None and genre_title_node.text and genre_title_node.text.strip(),
+                        genre_desc_node is not None and genre_desc_node.text and genre_desc_node.text.strip(),
+                        audience_title_node is not None and audience_title_node.text and audience_title_node.text.strip(),
+                        audience_desc_node is not None and audience_desc_node.text and audience_desc_node.text.strip()
+                    ])
+
+                    console.print(f"[dim]Pair {i+1}: {'âœ“' if has_all else 'âœ—'} {genre_title_node.text if genre_title_node is not None else 'None'}[/dim]")
+
+                    if has_all:
+                        pairs.append({
+                            "genre": {
+                                "title": genre_title_node.text.strip(),
+                                "description": genre_desc_node.text.strip()
+                            },
+                            "audience": {
+                                "title": audience_title_node.text.strip(),
+                                "description": audience_desc_node.text.strip()
+                            }
+                        })
+
+        # XMLãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å¾“æ¥ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è§£æã‚’è©¦è¡Œ
+        if not pairs:
+            console.print("[yellow]XMLã‹ã‚‰ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å†è©¦è¡Œ[/yellow]")
+            pairs = parse_ga_markdown_fallback(text)
+
+    except ET.ParseError as e:
+        console.print(f"[yellow]XMLè§£æã‚¨ãƒ©ãƒ¼: {e}[/yellow]")
+        # XMLè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è§£æã‚’è©¦è¡Œ
+        pairs = parse_ga_markdown_fallback(text)
+
+    console.print(f"[dim]æœ€çµ‚çš„ã«è§£æã•ã‚ŒãŸãƒšã‚¢æ•°: {len(pairs)}[/dim]")
+    return pairs
+
+
+def parse_ga_markdown_fallback(text: str) -> List[Dict[str, Dict[str, str]]]:
+    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
+    pairs = []
+    sections = text.split('---')
+
+    for section in sections:
+        if not section.strip():
+            continue
+
+        ast = mistune.create_markdown(renderer=None)(section)
+        genre = {"title": "", "description": ""}
+        audience = {"title": "", "description": ""}
+        current_type = None
+
+        for node in ast:
+            if node['type'] == 'heading':
+                header_text = "".join(child['text'] for child in node['children'])
+                if 'genre' in header_text.lower():
+                    current_type = 'genre'
+                    genre['title'] = header_text.replace('Genre:', '').strip()
+                elif 'audience' in header_text.lower():
+                    current_type = 'audience'
+                    audience['title'] = header_text.replace('Audience:', '').strip()
+            elif node['type'] == 'paragraph':
+                description = "".join(child['text'] for child in node['children'])
+                if current_type == 'genre':
+                    genre['description'] = description
+                elif current_type == 'audience':
+                    audience['description'] = description
+
+        if genre['title'] and audience['title']:
+            pairs.append({"genre": genre, "audience": audience})
+
+    return pairs
+
+
+def parse_ga_definitions_from_xml(xml_content: str) -> List[Dict[str, Dict[str, str]]]:
+    """XMLå½¢å¼ã®GAå®šç¾©ã‹ã‚‰GAãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’è§£æã™ã‚‹"""
+    pairs = []
+
+    try:
+        # XMLã‚¿ã‚°ã‹ã‚‰<GADefinitions>éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç„¡è¦–ï¼‰
+        xml_start = xml_content.find("<GADefinitions>")
+        xml_end = xml_content.rfind("</GADefinitions>")
+
+        if xml_start != -1 and xml_end != -1:
+            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
+
+            # XMLã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
+            import html
+            clean_xml = html.unescape(clean_xml)
+
+            root = ET.fromstring(clean_xml)
+            pair_nodes = root.findall('Pair')
+            console.print(f"[dim]è¦‹ã¤ã‹ã£ãŸPairãƒãƒ¼ãƒ‰æ•°: {len(pair_nodes)}[/dim]")
+
+            for i, pair_node in enumerate(pair_nodes):
+                genre_node = pair_node.find('Genre')
+                audience_node = pair_node.find('Audience')
+
+                if genre_node is not None and audience_node is not None:
+                    genre_title_node = genre_node.find('Title')
+                    genre_desc_node = genre_node.find('Description')
+                    audience_title_node = audience_node.find('Title')
+                    audience_desc_node = audience_node.find('Description')
+
+                    # ã‚ˆã‚Šè©³ç´°ãªãƒã‚§ãƒƒã‚¯
+                    has_genre_title = genre_title_node is not None and genre_title_node.text and genre_title_node.text.strip()
+                    has_genre_desc = genre_desc_node is not None and genre_desc_node.text and genre_desc_node.text.strip()
+                    has_audience_title = audience_title_node is not None and audience_title_node.text and audience_title_node.text.strip()
+                    has_audience_desc = audience_desc_node is not None and audience_desc_node.text and audience_desc_node.text.strip()
+
+                    if all([has_genre_title, has_genre_desc, has_audience_title, has_audience_desc]):
+                        pairs.append({
+                            "genre": {
+                                "title": genre_title_node.text.strip(),
+                                "description": genre_desc_node.text.strip()
+                            },
+                            "audience": {
+                                "title": audience_title_node.text.strip(),
+                                "description": audience_desc_node.text.strip()
+                            }
+                        })
+                        console.print(f"[green]âœ“[/green] {genre_title_node.text.strip()} x {audience_title_node.text.strip()}")
+                    else:
+                        console.print(f"[yellow]âš [/yellow] Pair {i+1}: å¿…è¦ãªè¦ç´ ãŒä¸è¶³")
+                else:
+                    console.print(f"[yellow]âš [/yellow] Pair {i+1}: Genreã¾ãŸã¯Audienceãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
+        else:
+            console.print("[yellow]GADefinitionsã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
+
+    except ET.ParseError as parse_error:
+        console.print(f"[bold red]GAå®šç¾©XMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ:[/bold red] {parse_error}")
+        console.print(f"[dim]å•é¡Œã®ã‚ã‚‹XML: {xml_content[xml_start:xml_start+200] if xml_start != -1 else xml_content[:200]}...[/dim]")
+
+        # XMLã‚¨ãƒ©ãƒ¼ã®å ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æã‚’è©¦è¡Œ
+        console.print("[yellow]æ‰‹å‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+        from .xml_utils import parse_ga_from_text_fallback
+        pairs = parse_ga_from_text_fallback(xml_content)
+
+    except Exception as e:
+        console.print(f"[bold red]äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
+
+    return pairs
diff --git a/easy_dataset_cli/main.py b/easy_dataset_cli/main.py
index 28adbaf..6f62532 100644
--- a/easy_dataset_cli/main.py
+++ b/easy_dataset_cli/main.py
@@ -12,11 +12,16 @@ from .core import (
     split_text,
     parse_ga_file,
     generate_qa_for_chunk_with_ga,
+    generate_qa_for_chunk_with_ga_and_fulltext,
     convert_to_xml_by_genre,
     generate_ga_definitions,
     parse_ga_definitions_from_xml,
     save_ga_definitions_by_genre,
-    create_output_directories
+    create_output_directories,
+    sanitize_filename,
+    convert_all_xml_to_alpaca,
+    upload_to_huggingface,
+    create_dataset_card
 )
 
 # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
@@ -29,11 +34,6 @@ app = typer.Typer(
 console = Console()
 
 
-def sanitize_filename(name: str) -> str:
-    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
-    return "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).rstrip()
-
-
 @app.command()
 def create_ga(
     file_path: Annotated[Path, typer.Argument(
@@ -47,11 +47,11 @@ def create_ga(
     model: Annotated[str, typer.Option(
         "--model", "-m",
         help="GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
-    )] = "openrouter/openai/gpt-4o",
+    )] = "openrouter/openai/gpt-oss-120b",
     num_ga_pairs: Annotated[int, typer.Option(
         "--num-ga-pairs", "-g",
         help="ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
-    )] = None,
+    )] = 5,
 ):
     """å…ƒã®æ–‡ç« ã‚’åˆ†æã—ã€GAãƒšã‚¢å®šç¾©ã‚’XMLå½¢å¼ã§ç”Ÿæˆã—ã€Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
     console.print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: [cyan]{file_path}[/cyan]")
@@ -138,9 +138,37 @@ def generate(
     num_qa_pairs: Annotated[int, typer.Option(
         "--num-qa-pairs", "-q",
         help="å„ãƒãƒ£ãƒ³ã‚¯ãƒ»GAãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
-    )] = None,
+    )] = 10,
+    use_fulltext: Annotated[bool, typer.Option(
+        "--use-fulltext", "-f",
+        help="å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
+    )] = False,
+    export_alpaca: Annotated[bool, typer.Option(
+        "--export-alpaca", "-a",
+        help="ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"
+    )] = False,
+    upload_hf: Annotated[bool, typer.Option(
+        "--upload-hf", "-u",
+        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
+    )] = False,
+    hf_repo_name: Annotated[str, typer.Option(
+        "--hf-repo-name", "-r",
+        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
+    )] = "",
+    hf_token: Annotated[str, typer.Option(
+        "--hf-token", "-t",
+        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
+    )] = "",
+    hf_private: Annotated[bool, typer.Option(
+        "--hf-private",
+        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
+    )] = False,
 ):
-    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"""
+    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
+    
+    --use-fulltextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†æ™‚ã«å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
+    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ãŸã ã—ã€å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
+    """
     try:
         console.print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: [cyan]{file_path}[/cyan]")
         text = file_path.read_text(encoding="utf-8")
@@ -167,16 +195,31 @@ def generate(
             dirs = create_output_directories(output_dir)
             console.print(f"[dim]å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: ga/, logs/, qa/[/dim]")
 
+        # å…¨æ–‡ä½¿ç”¨ã®å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤º
+        if use_fulltext:
+            console.print("[yellow]âš  å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚[/yellow]")
+            console.print(f"[dim]å…¨æ–‡é•·: {len(text)} æ–‡å­—[/dim]")
+
         with Progress(console=console) as progress:
             task = progress.add_task("[green]Q&Aãƒšã‚¢ã‚’ç”Ÿæˆä¸­...", total=total_tasks)
 
             for chunk in chunks:
                 for ga_pair in ga_pairs:
-                    qa_pairs = generate_qa_for_chunk_with_ga(
-                        chunk, model=model, ga_pair=ga_pair, 
-                        logs_dir=dirs["logs"] if dirs else None,
-                        num_qa_pairs=num_qa_pairs
-                    )
+                    if use_fulltext:
+                        qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
+                            chunk=chunk,
+                            full_text=text,
+                            model=model,
+                            ga_pair=ga_pair,
+                            logs_dir=dirs["logs"] if dirs else None,
+                            num_qa_pairs=num_qa_pairs
+                        )
+                    else:
+                        qa_pairs = generate_qa_for_chunk_with_ga(
+                            chunk, model=model, ga_pair=ga_pair,
+                            logs_dir=dirs["logs"] if dirs else None,
+                            num_qa_pairs=num_qa_pairs
+                        )
 
                     for pair in qa_pairs:
                         all_qa_pairs_with_ga.append({
@@ -208,6 +251,34 @@ def generate(
                 console.print(f"  - [green]âœ“[/green] {output_file_path.name}")
 
             console.print("\n[bold green]ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚[/bold green]")
+            
+            # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
+            if export_alpaca:
+                console.print("\n[bold blue]Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...[/bold blue]")
+                alpaca_file = dirs["base"] / "dataset_alpaca.json"
+                alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
+                
+                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
+                readme_file = dirs["base"] / "README.md"
+                create_dataset_card(alpaca_data, readme_file, "Generated QA Dataset")
+                
+                # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+                if upload_hf:
+                    if not hf_repo_name:
+                        console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+                        console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
+                    else:
+                        console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
+                        success = upload_to_huggingface(
+                            dataset_data=alpaca_data,
+                            repo_name=hf_repo_name,
+                            hf_token=hf_token if hf_token else None,
+                            private=hf_private,
+                            commit_message=f"Upload QA dataset with {len(alpaca_data)} entries",
+                            readme_file=readme_file
+                        )
+                        if not success:
+                            console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
         else:
             console.print("\n--- ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ (Genreåˆ¥XML) ---")
             for genre, xml_content in xml_outputs_by_genre.items():
@@ -219,5 +290,80 @@ def generate(
         raise typer.Exit(code=1)
 
 
+@app.command()
+def convert_to_alpaca(
+    qa_dir: Annotated[Path, typer.Argument(
+        exists=True, dir_okay=True, readable=True,
+        help="XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹qaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
+    )],
+    output_file: Annotated[Path, typer.Option(
+        "--output-file", "-o", file_okay=True, dir_okay=False,
+        help="å‡ºåŠ›ã™ã‚‹Alpacaå½¢å¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚"
+    )] = None,
+    upload_hf: Annotated[bool, typer.Option(
+        "--upload-hf", "-u",
+        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
+    )] = False,
+    hf_repo_name: Annotated[str, typer.Option(
+        "--hf-repo-name", "-r",
+        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
+    )] = "",
+    hf_token: Annotated[str, typer.Option(
+        "--hf-token", "-t",
+        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
+    )] = "",
+    hf_private: Annotated[bool, typer.Option(
+        "--hf-private",
+        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
+    )] = False,
+):
+    """æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã®JSONã«å¤‰æ›ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
+    
+    try:
+        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
+        if output_file is None:
+            output_file = qa_dir.parent / "dataset_alpaca.json"
+        
+        console.print(f"XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›ä¸­: [cyan]{qa_dir}[/cyan]")
+        
+        # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã«å¤‰æ›
+        alpaca_data = convert_all_xml_to_alpaca(qa_dir, output_file)
+        
+        if not alpaca_data:
+            console.print("[bold red]å¤‰æ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚[/bold red]")
+            raise typer.Exit(code=1)
+        
+        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
+        readme_file = output_file.parent / "README.md"
+        create_dataset_card(alpaca_data, readme_file, "Converted QA Dataset")
+        
+        # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+        if upload_hf:
+            if not hf_repo_name:
+                console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+                console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
+                raise typer.Exit(code=1)
+            
+            console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
+            success = upload_to_huggingface(
+                dataset_data=alpaca_data,
+                repo_name=hf_repo_name,
+                hf_token=hf_token if hf_token else None,
+                private=hf_private,
+                commit_message=f"Upload converted QA dataset with {len(alpaca_data)} entries",
+                readme_file=readme_file
+            )
+            
+            if not success:
+                console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
+                raise typer.Exit(code=1)
+        
+        console.print(f"\n[bold green]âœ“[/bold green] å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
+        
+    except Exception as e:
+        console.print(f"[bold red]å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {e}")
+        raise typer.Exit(code=1)
+
+
 if __name__ == "__main__":
     app()
diff --git a/easy_dataset_cli/prompts.py b/easy_dataset_cli/prompts.py
index 5b2c825..24ba9f9 100644
--- a/easy_dataset_cli/prompts.py
+++ b/easy_dataset_cli/prompts.py
@@ -20,6 +20,11 @@ def get_qa_generation_prompt() -> str:
     return load_prompt_template("qa_generation")
 
 
+def get_qa_generation_with_fulltext_prompt() -> str:
+    """å…¨æ–‡+ãƒãƒ£ãƒ³ã‚¯å¯¾å¿œQ&Aç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
+    return load_prompt_template("qa_generation_with_fulltext")
+
+
 def get_ga_definition_generation_prompt() -> str:
     """GAå®šç¾©ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
     return load_prompt_template("ga_definition_generation")
diff --git a/easy_dataset_cli/prompts/qa_generation_with_fulltext.md b/easy_dataset_cli/prompts/qa_generation_with_fulltext.md
new file mode 100644
index 0000000..a41ae1f
--- /dev/null
+++ b/easy_dataset_cli/prompts/qa_generation_with_fulltext.md
@@ -0,0 +1,52 @@
+# å½¹å‰²: Q&Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ï¼ˆå…¨æ–‡+ãƒãƒ£ãƒ³ã‚¯å¯¾å¿œç‰ˆï¼‰
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰é«˜å“è³ªãªè³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹ã«ã€æŒ‡å®šã•ã‚ŒãŸã€Œä½“è£ã€ã¨ã€Œèª­è€…ã€ã«åˆã‚ã›ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã«é•·ã‘ã¦ã„ã¾ã™ã€‚
+
+## æŒ‡ç¤º:
+1. ä¸ãˆã‚‰ã‚ŒãŸã€Œå…¨æ–‡ã€ã¨ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ãã ã•ã„ã€‚
+2. æŒ‡å®šã•ã‚ŒãŸã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã¨ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®å½¹å‰²ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
+3. **ãƒãƒ£ãƒ³ã‚¯**ã®å†…å®¹ã‚’ä¸­å¿ƒã¨ã—ã¤ã¤ã€**å…¨æ–‡**ã®æ–‡è„ˆã‚’ç†è§£ã—ãŸä¸Šã§ã€{num_qa_pairs}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§æ´å¯Ÿã«å¯Œã‚“ã Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+4. è³ªå•ã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨è¤‡é›‘ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®è¦–ç‚¹ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+5. å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ¼ãƒ³ã€è©³ç´°ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+6. **é‡è¦**: è³ªå•ã¨å›ç­”ã¯ä¸»ã«ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã®å†…å®¹ã«åŸºã¥ã„ã¦ä½œæˆã—ã€ã€Œå…¨æ–‡ã€ã¯æ–‡è„ˆç†è§£ã®ãŸã‚ã®è£œåŠ©æƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
+
+## ç›®æ¨™ã¨ã™ã‚‹ä½“è£:
+{genre_title}
+{genre_description}
+
+## ç›®æ¨™ã¨ã™ã‚‹èª­è€…:
+{audience_title}
+{audience_description}
+
+## å…¨æ–‡ï¼ˆæ–‡è„ˆç†è§£ç”¨ï¼‰:
+---
+{full_text}
+---
+
+## ãƒãƒ£ãƒ³ã‚¯ï¼ˆQAç”Ÿæˆå¯¾è±¡ï¼‰:
+---
+{chunk}
+---
+
+## å‡ºåŠ›å½¢å¼:
+**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<QAPairs>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
+å„Q&Aãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Question>` ã¨ `<Answer>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+
+**é‡è¦**: XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, ", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š& â†’ &amp;, < â†’ &lt;ï¼‰ã€‚
+å›ç­”æ–‡ã«æ”¹è¡Œã‚’å«ã‚ãšã€ä¸€è¡Œã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## å‡ºåŠ›ä¾‹:
+\```xml
+<QAPairs>
+<Pair>
+<Question>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ã€ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é€šè²¨ã§ã‚ã‚‹ã‚¢ãƒ‡ãƒã‚·ãƒ³ä¸‰ãƒªãƒ³é…¸ï¼ˆATPï¼‰ã®å¤§éƒ¨åˆ†ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚</Answer>
+</Pair>
+<Pair>
+<Question>ATPã¯ç´°èƒå†…ã§ã©ã®ã‚ˆã†ã«åˆ©ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿ</Question>
+<Answer>ATPã¯ç´°èƒå†…ã®æ§˜ã€…ãªåŒ–å­¦åå¿œã®ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚</Answer>
+</Pair>
+</QAPairs>
+\```
+
+ãã‚Œã§ã¯ã€Q&Aãƒšã‚¢ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
diff --git a/easy_dataset_cli/qa_generator.py b/easy_dataset_cli/qa_generator.py
new file mode 100644
index 0000000..0817065
--- /dev/null
+++ b/easy_dataset_cli/qa_generator.py
@@ -0,0 +1,209 @@
+# easy_dataset_cli/qa_generator.py
+"""Q&Aç”Ÿæˆé–¢é€£æ©Ÿèƒ½"""
+
+import os
+import xml.etree.ElementTree as ET
+from pathlib import Path
+from typing import List, Dict
+from litellm import completion
+from rich.console import Console
+from dotenv import load_dotenv
+
+from .prompts import (
+    get_qa_generation_prompt,
+    get_qa_generation_with_fulltext_prompt,
+    get_ga_definition_generation_prompt
+)
+from .xml_utils import parse_qa_from_text_fallback
+
+# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
+load_dotenv()
+
+console = Console()
+
+
+def generate_qa_for_chunk_with_ga_and_fulltext(
+    chunk: str,
+    full_text: str,
+    model: str,
+    ga_pair: Dict[str, Dict[str, str]],
+    logs_dir: Path = None,
+    num_qa_pairs: int = None
+) -> List[Dict[str, str]]:
+    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨å…¨æ–‡ã€1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_with_fulltext_prompt()
+    prompt = prompt_template.format(
+        chunk=chunk,
+        full_text=full_text,
+        genre_title=ga_pair['genre']['title'],
+        genre_description=ga_pair['genre']['description'],
+        audience_title=ga_pair['audience']['title'],
+        audience_description=ga_pair['audience']['description'],
+        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
+    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
+
+    try:
+        response = completion(model=model, messages=messages)
+        xml_content = response.choices[0].message.content
+
+        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+        if logs_dir:
+            genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+            audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+            raw_filename = f"qa_fulltext_raw_{genre_safe}_{audience_safe}.md"
+            raw_file_path = logs_dir / raw_filename
+            raw_file_path.write_text(xml_content, encoding="utf-8")
+
+        return _parse_qa_response(xml_content)
+
+    except Exception as general_error:
+        console.print(
+            f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] "
+            f"{general_error}"
+        )
+        console.print(
+            f"[dim]Genre: {ga_pair['genre']['title']}, "
+            f"Audience: {ga_pair['audience']['title']}[/dim]"
+        )
+        return []
+
+
+def generate_qa_for_chunk_with_ga(
+    chunk: str,
+    model: str,
+    ga_pair: Dict[str, Dict[str, str]],
+    logs_dir: Path = None,
+    num_qa_pairs: int = None
+) -> List[Dict[str, str]]:
+    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_prompt()
+    prompt = prompt_template.format(
+        context=chunk,
+        genre_title=ga_pair['genre']['title'],
+        genre_description=ga_pair['genre']['description'],
+        audience_title=ga_pair['audience']['title'],
+        audience_description=ga_pair['audience']['description'],
+        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
+    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
+
+    try:
+        response = completion(model=model, messages=messages)
+        xml_content = response.choices[0].message.content
+
+        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+        if logs_dir:
+            genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+            audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+            raw_filename = f"qa_raw_{genre_safe}_{audience_safe}.md"
+            raw_file_path = logs_dir / raw_filename
+            raw_file_path.write_text(xml_content, encoding="utf-8")
+
+        return _parse_qa_response(xml_content)
+
+    except Exception as general_error:
+        console.print(
+            f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] "
+            f"{general_error}"
+        )
+        console.print(
+            f"[dim]Genre: {ga_pair['genre']['title']}, "
+            f"Audience: {ga_pair['audience']['title']}[/dim]"
+        )
+        return []
+
+
+def generate_ga_definitions(text_content: str, model: str, num_ga_pairs: int = None) -> str:
+    """litellmã‚’ä½¿ã„ã€å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã®XMLã‚’ç”Ÿæˆã™ã‚‹"""
+    # LLMã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã™ãã‚‹ã¨ã‚³ã‚¹ãƒˆã‚„æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ãŸã‚ã€å…ˆé ­éƒ¨åˆ†ã«é™å®šã™ã‚‹
+    context = text_content[:8000]
+    console.print(f"[dim]ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(context)} æ–‡å­—[/dim]")
+
+    prompt_template = get_ga_definition_generation_prompt()
+    prompt = prompt_template.format(
+        context=context,
+        num_ga_pairs=num_ga_pairs if num_ga_pairs is not None else "3-5å€‹ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
+    api_key = os.getenv("OPENROUTER_API_KEY", "")
+    if not api_key:
+        console.print("[bold red]OPENROUTER_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+        raise ValueError("OPENROUTER_API_KEYãŒå¿…è¦ã§ã™")
+
+    os.environ["OPENROUTER_API_KEY"] = api_key
+
+    # OpenRouterã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
+    if "openrouter" not in model and not model.startswith("openrouter/"):
+        if model.startswith("gpt-"):
+            model = f"openrouter/openai/{model}"
+        elif model.startswith("claude-"):
+            model = f"openrouter/anthropic/{model}"
+        else:
+            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§openrouterãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
+            model = f"openrouter/{model}"
+
+    try:
+        response = completion(model=model, messages=messages)
+        xml_content = response.choices[0].message.content
+        console.print(f"[dim]LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(xml_content)} æ–‡å­—[/dim]")
+        return xml_content
+    except Exception as error:
+        console.print(f"[bold red]GAå®šç¾©ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {error}")
+        raise
+
+
+def _parse_qa_response(xml_content: str) -> List[Dict[str, str]]:
+    """Q&Aç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®XMLã‚’è§£æã™ã‚‹ï¼ˆå…±é€šå‡¦ç†ï¼‰"""
+    qa_pairs = []
+
+    # LLMã‹ã‚‰ã®å‡ºåŠ›ã«ã¯ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€XMLéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
+    xml_start = xml_content.find("<QAPairs>")
+    xml_end = xml_content.rfind("</QAPairs>")
+
+    if xml_start != -1 and xml_end != -1:
+        clean_xml = xml_content[xml_start: xml_end + len("</QAPairs>")]
+
+        try:
+            root = ET.fromstring(clean_xml)
+
+            for pair_node in root.findall('Pair'):
+                question_node = pair_node.find('Question')
+                answer_node = pair_node.find('Answer')
+
+                if question_node is not None and answer_node is not None:
+                    qa_pairs.append({
+                        "question": question_node.text or "",
+                        "answer": answer_node.text or ""
+                    })
+
+        except ET.ParseError:
+            # XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æ
+            console.print("[yellow]XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€æ‰‹å‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+            qa_pairs = parse_qa_from_text_fallback(clean_xml)
+
+    if not qa_pairs:
+        console.print(f"[bold red]LLMãŒç”Ÿæˆã—ãŸXMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
+        console.print(f"[dim]å—ä¿¡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ: {xml_content[:200]}...[/dim]")
+
+    return qa_pairs
diff --git a/easy_dataset_cli/text_splitter.py b/easy_dataset_cli/text_splitter.py
new file mode 100644
index 0000000..2f72b69
--- /dev/null
+++ b/easy_dataset_cli/text_splitter.py
@@ -0,0 +1,17 @@
+# easy_dataset_cli/text_splitter.py
+"""ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²é–¢é€£æ©Ÿèƒ½"""
+
+from typing import List
+from langchain_text_splitters import RecursiveCharacterTextSplitter
+
+
+def split_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
+    """LangChainã®TextSplitterã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹"""
+    text_splitter = RecursiveCharacterTextSplitter(
+        chunk_size=chunk_size,
+        chunk_overlap=chunk_overlap,
+        length_function=len,
+        is_separator_regex=False,
+    )
+    docs = text_splitter.create_documents([text])
+    return [doc.page_content for doc in docs]
diff --git a/easy_dataset_cli/xml_utils.py b/easy_dataset_cli/xml_utils.py
new file mode 100644
index 0000000..ec5a370
--- /dev/null
+++ b/easy_dataset_cli/xml_utils.py
@@ -0,0 +1,154 @@
+# easy_dataset_cli/xml_utils.py
+"""XMLå‡¦ç†é–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
+
+import xml.etree.ElementTree as ET
+from xml.dom import minidom
+from collections import defaultdict
+from typing import List, Dict
+from rich.console import Console
+
+console = Console()
+
+
+def parse_ga_from_text_fallback(content: str) -> List[Dict[str, Dict[str, str]]]:
+    """XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è§£æ"""
+    pairs = []
+
+    try:
+        # <Pair>ã‚¿ã‚°ã§åˆ†å‰²
+        pair_sections = content.split('<Pair>')
+
+        for section in pair_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
+            if '</Pair>' not in section:
+                continue
+
+            pair_content = section.split('</Pair>')[0]
+
+            # Titleè¦ç´ ã‚’æŠ½å‡º
+            genre_title = extract_text_between_tags(pair_content, 'Genre', 'Title')
+            genre_desc = extract_text_between_tags(pair_content, 'Genre', 'Description')
+            audience_title = extract_text_between_tags(pair_content, 'Audience', 'Title')
+            audience_desc = extract_text_between_tags(pair_content, 'Audience', 'Description')
+
+            if all([genre_title, genre_desc, audience_title, audience_desc]):
+                pairs.append({
+                    "genre": {
+                        "title": genre_title.strip(),
+                        "description": genre_desc.strip()
+                    },
+                    "audience": {
+                        "title": audience_title.strip(),
+                        "description": audience_desc.strip()
+                    }
+                })
+                console.print(f"[green]âœ“[/green] (æ‰‹å‹•è§£æ) {genre_title} x {audience_title}")
+
+    except Exception as e:
+        console.print(f"[red]æ‰‹å‹•è§£æã‚‚å¤±æ•—:[/red] {e}")
+
+    return pairs
+
+
+def extract_text_between_tags(content: str, parent_tag: str, child_tag: str) -> str:
+    """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚°é–“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
+    try:
+        # è¦ªã‚¿ã‚°å†…ã‚’æ¢ã™
+        parent_start = content.find(f'<{parent_tag}>')
+        parent_end = content.find(f'</{parent_tag}>')
+
+        if parent_start == -1 or parent_end == -1:
+            return ""
+
+        parent_content = content[parent_start:parent_end]
+
+        # å­ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
+        child_start = parent_content.find(f'<{child_tag}>')
+        child_end = parent_content.find(f'</{child_tag}>')
+
+        if child_start == -1 or child_end == -1:
+            return ""
+
+        return parent_content[child_start + len(f'<{child_tag}>'):child_end]
+
+    except Exception:
+        return ""
+
+
+def parse_qa_from_text_fallback(content: str) -> List[Dict[str, str]]:
+    """Q&A XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è§£æ"""
+    qa_pairs = []
+
+    try:
+        # <Pair>ã‚¿ã‚°ã§åˆ†å‰²
+        pair_sections = content.split('<Pair>')
+
+        for section in pair_sections[1:]:  # æœ€åˆã®è¦ç´ ã¯ç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
+            if '</Pair>' not in section:
+                continue
+
+            pair_content = section.split('</Pair>')[0]
+
+            # Question ã¨ Answer ã‚’æŠ½å‡º
+            question = extract_simple_tag_content(pair_content, 'Question')
+            answer = extract_simple_tag_content(pair_content, 'Answer')
+
+            if question and answer:
+                qa_pairs.append({
+                    "question": question.strip(),
+                    "answer": answer.strip()
+                })
+                console.print("[green]âœ“[/green] (æ‰‹å‹•è§£æ) Q&Aè¿½åŠ ")
+
+    except Exception as e:
+        console.print(f"[red]Q&Aæ‰‹å‹•è§£æã‚‚å¤±æ•—:[/red] {e}")
+
+    return qa_pairs
+
+
+def extract_simple_tag_content(content: str, tag: str) -> str:
+    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
+    try:
+        start_tag = f'<{tag}>'
+        end_tag = f'</{tag}>'
+
+        start_pos = content.find(start_tag)
+        end_pos = content.find(end_tag)
+
+        if start_pos == -1 or end_pos == -1:
+            return ""
+
+        return content[start_pos + len(start_tag):end_pos]
+
+    except Exception:
+        return ""
+
+
+def convert_to_xml_by_genre(all_qa_pairs: List[Dict[str, str]]) -> Dict[str, str]:
+    """Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’Genreã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€æ•´å½¢ã•ã‚ŒãŸXMLæ–‡å­—åˆ—ã®è¾æ›¸ã«å¤‰æ›ã™ã‚‹"""
+    grouped_by_genre = defaultdict(list)
+
+    for item in all_qa_pairs:
+        grouped_by_genre[item["genre"]].append(item)
+
+    xml_outputs = {}
+    for genre, pairs in grouped_by_genre.items():
+        root = ET.Element("QAPairs")
+        root.set("genre", genre)
+
+        for item in pairs:
+            pair_elem = ET.SubElement(root, "Pair")
+
+            audience_elem = ET.SubElement(pair_elem, "Audience")
+            audience_elem.text = item["audience"]
+
+            question_elem = ET.SubElement(pair_elem, "Question")
+            question_elem.text = item["question"]
+
+            answer_elem = ET.SubElement(pair_elem, "Answer")
+            answer_elem.text = item["answer"]
+
+        rough_string = ET.tostring(root, 'utf-8')
+        reparsed = minidom.parseString(rough_string)
+        xml_outputs[genre] = reparsed.toprettyxml(indent="  ")
+
+    return xml_outputs
diff --git a/pyproject.toml b/pyproject.toml
index d9b2651..d5d958e 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -2,13 +2,16 @@
 name = "easy-dataset-cli"
 version = "1.0.0"
 description = "A simple CLI tool to generate QA pairs from a text file using LLMs and GA pairs, outputting genre-specific XML files."
+requires-python = ">=3.9"
 dependencies = [
-    "typer[all]",          # CLIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
+    "typer",               # CLIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
     "rich",                # ãƒªãƒƒãƒãªã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
     "litellm",             # LLMé€£æºãƒ©ã‚¤ãƒ–ãƒ©ãƒª
     "langchain-text-splitters", # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ç”¨
     "mistune",             # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è§£æç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
-    "python-dotenv"        # .env ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ç”¨
+    "python-dotenv",       # .env ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ç”¨
+    "huggingface-hub",     # Hugging Face Hub API
+    "datasets"             # Hugging Face Datasets
 ]
 
 [project.scripts]
@@ -17,4 +20,4 @@ easy-dataset = "easy_dataset_cli.main:app"
 
 [build-system]
 requires = ["setuptools>=61.0"]
-build-backend = "setuptools.build_meta"
\ No newline at end of file
+build-backend = "setuptools.build_meta"
```
