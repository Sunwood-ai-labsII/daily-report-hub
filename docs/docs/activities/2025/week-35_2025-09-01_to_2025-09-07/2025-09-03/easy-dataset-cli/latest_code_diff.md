# ğŸ”„ Latest Code Changes

```diff
diff --git a/.SourceSageignore b/.SourceSageignore
index ac8bce0..14d812a 100644
--- a/.SourceSageignore
+++ b/.SourceSageignore
@@ -53,3 +53,7 @@ venv
 .venv
 
 uv.lock
+tests/
+.github
+
+test_output/
diff --git a/.env.example b/.env.example
index 72d7c7d..07b7bba 100644
--- a/.env.example
+++ b/.env.example
@@ -1,2 +1,28 @@
-OPENROUTER_API_KEY=sk-or-xxxx
-HUGGINGFACE_TOKEN=hf_xxxx
+# ---------------------------------------------------------------------
+# easy-dataset-cli ç’°å¢ƒå¤‰æ•°è¨­å®š
+#
+# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ .env ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚
+# ä½¿ç”¨ã—ãŸã„å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
+# ---------------------------------------------------------------------
+
+# ===== OpenAI äº’æ›APIè¨­å®š =====
+# OpenAI äº’æ›APIã®APIã‚­ãƒ¼
+# (å¿…é ˆ)
+OPENAI_API_KEY=sk-or-xxxxxxxxxxxxx
+
+# ===== API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š =====
+# OpenAI äº’æ›APIã®base URL
+# OpenRouterã®å ´åˆ:
+OPENAI_BASE_URL=https://openrouter.ai/api/v1
+#
+# OpenAIå…¬å¼ã®å ´åˆ:
+# OPENAI_BASE_URL=https://api.openai.com/v1
+#
+# ã‚«ã‚¹ã‚¿ãƒ LLMã‚µãƒ¼ãƒãƒ¼ã®å ´åˆ:
+# OPENAI_BASE_URL=https://your-custom-llm-server/v1
+#
+# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: https://openrouter.ai/api/v1 (OpenRouter)
+
+# ===== Hugging Faceè¨­å®š =====
+# (ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«ä½¿ç”¨)
+HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxx
diff --git a/.gitignore b/.gitignore
index 04ce1f8..0d6eb6d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -209,3 +209,11 @@ __marimo__/
 uv.lock
 example/output/structured/logs/
 example/output/
+
+test_output/
+logs.txt
+
+example/input/toho/
+
+output/
+memo.md
diff --git a/README.md b/README.md
index 17cd67d..6687ba5 100644
--- a/README.md
+++ b/README.md
@@ -33,6 +33,10 @@
 - **ğŸ¤— HFçµ±åˆ**: Hugging Face Hubã¸ã®ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
 - **ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰**: è‡ªå‹•çš„ãªREADME.mdç”Ÿæˆã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’æ•´ç†
 - **ğŸ”„ å¤‰æ›æ©Ÿèƒ½**: æ—¢å­˜XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Alpacaå½¢å¼ã¸ã®å¤‰æ›ã‚³ãƒãƒ³ãƒ‰
+- **ğŸ” è‡ªå‹•GAæ¤œå‡º**: ãƒãƒƒãƒå‡¦ç†æ™‚ã«å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡º
+- **ğŸ“ ãƒãƒƒãƒå‡¦ç†å¼·åŒ–**: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®åŒæ™‚å‡¦ç†ã¨å€‹åˆ¥å‡ºåŠ›å¯¾å¿œ
+- **ğŸŒ å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰**: ãƒãƒ£ãƒ³ã‚¯å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ´»ç”¨ã—ã€æ–‡è„ˆç†è§£ã‚’å‘ä¸Š
+- **ğŸ“Œ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­æ´»ç”¨**: å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨æ™‚ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­3000æ–‡å­—ã‚’å¸¸ã«ä»˜ä¸ã—ã¦æ–‡è„ˆã®å®‰å®šæ€§ã‚’å‘ä¸Š
 
 ## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
 
@@ -56,14 +60,34 @@ pip install -e .
 # ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ã‚’è¨­å®š
 export OPENAI_API_KEY="your-api-key-here"
 
-# å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã‚’è‡ªå‹•ç”Ÿæˆ
-uv run easy-dataset create-ga .\example\input\documents\sample_document.txt --output-dir .\example\output\sample_document --num-ga-pairs 10
+# å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã‚’è‡ªå‹•ç”Ÿæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8000æ–‡å­—ã¾ã§ä½¿ç”¨ï¼‰
+uv run easy-dataset create-ga ./example/input/documents/sample_document.txt --output-dir ./example/output/sample_document --num-ga-pairs 2
+
+# ã‚ˆã‚Šå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™ã—ã¦å‡¦ç†æ™‚é–“ã‚’çŸ­ãã™ã‚‹
+uv run easy-dataset create-ga ./large_document.txt --output-dir ./output --num-ga-pairs 3 --max-context-length 4000
+
+# ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦GAãƒšã‚¢ã‚’ãƒãƒƒãƒç”Ÿæˆ
+uv run easy-dataset create-ga ./example/input/documents/ --output-dir ./example/output/batch_ga_output --num-ga-pairs 2 --max-context-length 6000
 \```
 
 2. **Q&Aãƒšã‚¢ã®ç”Ÿæˆ**
+
+#### å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
 \```bash
 # GAãƒšã‚¢å®šç¾©ã‚’ä½¿ã£ã¦Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ
-uv run easy-dataset generate .\example\input\documents\sample_document.txt --ga-file .\example\output\sample_document\ga\ga_definitions.xml --output-dir .\example\output\sample_document\ --chunk-size 500
+uv run easy-dataset generate ./example/input/documents/sample_document.txt --ga-file ./example/output/sample_document/ga/ga_definitions.xml --output-dir ./example/output/sample_document/ --chunk-size 2000
+\```
+
+#### è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰ã®å ´åˆ
+\```bash
+# è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†
+uv run easy-dataset generate ./example/input/documents/ --ga-file ./example/output/sample_document/ga/ga_definitions.xml --output-dir ./example/output/batch_output/ --chunk-size 2000 --use-surrounding-context 
+\```
+
+#### è‡ªå‹•GAæ¤œå‡ºæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸãƒãƒƒãƒå‡¦ç†
+\```bash
+# å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒãƒƒãƒå‡¦ç†
+uv run easy-dataset generate ./example/input/documents/ --ga-base-dir ./example/output/batch_ga_output/ --output-dir ./example/output/batch_qa_output/ --chunk-size 2000 --use-surrounding-context 
 \```
 
 ### ğŸ¦™ Alpacaå½¢å¼ã¨Hugging Faceé€£æºã®ä½¿ç”¨ä¾‹
@@ -91,6 +115,14 @@ uv run easy-dataset generate .\example\input\documents\sample_document.txt \
   --output-dir .\example\output\sample_document\ \
   --use-thinking \
   --use-fulltext
+
+# å‘¨è¾ºãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã£ãŸQ&Aç”Ÿæˆ
+uv run easy-dataset generate .\example\input\documents\sample_document.txt \
+  --ga-file .\example\output\sample_document\ga\ga_definitions.xml \
+  --output-dir .\example\output\sample_document\ \
+  --use-surrounding-context \
+  --context-before 1 \
+  --context-after 2
 \```
 
 #### Hugging Face Hubã¸ã®ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
@@ -124,13 +156,14 @@ uv run easy-dataset convert-to-alpaca .\example\output\sample_document\qa \
 uv run easy-dataset create-ga [OPTIONS] FILE_PATH
 
 Arguments:
-  FILE_PATH  GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« [required]
+  FILE_PATH  GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ [required]
 
 Options:
-  -o, --output-dir DIRECTORY  ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª [required]
-  -m, --model TEXT           GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«å [default: openrouter/openai/gpt-4o]
-  -g, --num-ga-pairs INTEGER ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™
-  -h, --help                 Show this message and exit
+  -o, --output-dir DIRECTORY        ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª [required]
+  -m, --model TEXT                 GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«å [default: openrouter/openai/gpt-oss-120b]
+  -g, --num-ga-pairs INTEGER       ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™
+  -l, --max-context-length INTEGER GAç”Ÿæˆæ™‚ã«LLMã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§æ–‡å­—æ•°[default: 8000]
+  -h, --help                       Show this message and exit
 \```
 
 #### ğŸ”§ generate ã‚³ãƒãƒ³ãƒ‰
@@ -141,16 +174,101 @@ Arguments:
   FILE_PATH  å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ [required]
 
 Options:
-  --ga-file PATH           Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLãƒ•ã‚¡ã‚¤ãƒ« [required]
+  --ga-file PATH           Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã€‚ãƒãƒƒãƒå‡¦ç†ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«å…±é€šã®å®šç¾©ã‚’é©ç”¨ã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚
+  --ga-base-dir PATH       GAå®šç¾©ãƒ•ã‚©ãƒ«ãƒ€ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒãƒƒãƒå‡¦ç†æ™‚ã«å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚
   -o, --output-dir PATH    XMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
   -m, --model TEXT         Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ« [default: openrouter/openai/gpt-4o]
   --chunk-size INTEGER     ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚º [default: 2000]
   --chunk-overlap INTEGER  ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º [default: 200]
   -f, --use-fulltext       å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
   -T, --use-thinking       å„Q&Aãƒšã‚¢ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã—ã¾ã™ã€‚ã‚ˆã‚Šæ·±ã„ç†è§£ã¨èª¬æ˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
+  -S, --use-surrounding-context å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
+  --context-before INTEGER å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹å‰æ–¹ãƒãƒ£ãƒ³ã‚¯æ•° [default: 1]
+  --context-after INTEGER  å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹å¾Œæ–¹ãƒãƒ£ãƒ³ã‚¯æ•° [default: 1]
   -h, --help               Show this message and exit
 \```
 
+#### ğŸ”— å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆ`--use-surrounding-context`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+
+`--use-surrounding-context`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚`--use-fulltext`ã‚ˆã‚Šã‚‚å‡¦ç†ã‚³ã‚¹ãƒˆãŒä½ãæŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚
+
+- **`--context-before INTEGER`**: å‰æ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
+- **`--context-after INTEGER`**: å¾Œæ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
+
+è¿½åŠ ã®ä»•æ§˜ï¼ˆv0.2.xä»¥é™ï¼‰:
+- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­ã‚’è‡ªå‹•ä»˜ä¸**: å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹æ™‚ã¯ã€å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…ˆé ­ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­3000æ–‡å­—ãŒè‡ªå‹•çš„ã«ä»˜ä¸ã•ã‚Œã¾ã™ã€‚
+  - ç›®çš„: å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œã ã‘ã§ã¯æ–‡è„ˆãŒæ›–æ˜§ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã‚’é˜²ãã€å…¨ä½“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚„ç”¨èªã®åŸºèª¿ã‚’å…±æœ‰ã™ã‚‹ãŸã‚
+  - ä¸Šé™: 3000æ–‡å­—ï¼ˆå›ºå®šï¼‰
+  - ã‚³ã‚¹ãƒˆ: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ãŒå¢—ãˆã‚‹ãŸã‚ã€ã‚ãšã‹ã«ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ãŒå¢—åŠ ã—ã¾ã™
+
+**ä½¿ç”¨ä¾‹:**
+\```bash
+# å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œ1ãƒãƒ£ãƒ³ã‚¯ãšã¤ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
+uv run easy-dataset generate document.txt \
+  --ga-file ga_definitions.xml \
+  --use-surrounding-context
+
+# å‰2ãƒãƒ£ãƒ³ã‚¯ã€å¾Œ1ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
+uv run easy-dataset generate document.txt \
+  --ga-file ga_definitions.xml \
+  --use-surrounding-context \
+  --context-before 2 \
+  --context-after 1
+\```
+
+ã“ã®ãƒ¢ãƒ¼ãƒ‰ã¯ã€é•·ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãŠã„ã¦å„ãƒãƒ£ãƒ³ã‚¯ã®æ„å‘³ã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚µã‚¤ã‚ºåˆ¶é™ã‚’å›é¿ã—ã¤ã¤æ–‡è„ˆç†è§£ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
+åŠ ãˆã¦ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­ï¼ˆæœ€å¤§3000æ–‡å­—ï¼‰ã‚’æ¯å›ä»˜ä¸ã™ã‚‹ã“ã¨ã§ã€ç”¨èªã‚„è©±é¡Œã®åŸºèª¿ãŒå…±æœ‰ã•ã‚Œã€è³ªå•ãƒ»å›ç­”ã®ä¸€è²«æ€§ãŒé«˜ã¾ã‚Šã¾ã™ã€‚
+
+#### ğŸ“ GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½
+
+`--ga-base-dir`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒãƒƒãƒå‡¦ç†æ™‚ã«å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚
+
+**å‹•ä½œä»•æ§˜:**
+- å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å„ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: `doc_A.txt`ï¼‰ã®åå‰ã‚’å–å¾—
+- `--ga-base-dir`ã§æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ„ã¿åˆã‚ã›ã¦å¯¾å¿œã™ã‚‹GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è‡ªå‹•ç”Ÿæˆï¼ˆä¾‹: `<ga-base-dir>/doc_A/ga/ga_definitions.xml`ï¼‰
+- ãã®GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«ã®Q&Aç”Ÿæˆã‚’è¡Œã†
+- å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ä¸Šè¨˜å‡¦ç†ã‚’ç¹°ã‚Šè¿”ã™
+
+**ä½¿ç”¨ä¾‹:**
+\```bash
+# å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒãƒƒãƒå‡¦ç†
+uv run easy-dataset generate ./example/input/documents/ \
+  --ga-base-dir ./example/output/batch_ga_output/ \
+  --output-dir ./example/output/batch_qa_output/
+\```
+
+**ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä¾‹:**
+\```
+example/
+â”œâ”€â”€ input/documents/
+â”‚   â”œâ”€â”€ doc_A.txt
+â”‚   â”œâ”€â”€ doc_B.txt
+â”‚   â””â”€â”€ doc_C.txt
+â”œâ”€â”€ output/batch_ga_output/
+â”‚   â”œâ”€â”€ doc_A/
+â”‚   â”‚   â””â”€â”€ ga/
+â”‚   â”‚       â””â”€â”€ ga_definitions.xml
+â”‚   â”œâ”€â”€ doc_B/
+â”‚   â”‚   â””â”€â”€ ga/
+â”‚   â”‚       â””â”€â”€ ga_definitions.xml
+â”‚   â””â”€â”€ doc_C/
+â”‚       â””â”€â”€ ga/
+â”‚           â””â”€â”€ ga_definitions.xml
+â””â”€â”€ output/batch_qa_output/
+    â”œâ”€â”€ doc_A/
+    â”‚   â”œâ”€â”€ ga/
+    â”‚   â”œâ”€â”€ logs/
+    â”‚   â””â”€â”€ qa/
+    â”œâ”€â”€ doc_B/
+    â”‚   â”œâ”€â”€ ga/
+    â”‚   â”œâ”€â”€ logs/
+    â”‚   â””â”€â”€ qa/
+    â””â”€â”€ doc_C/
+        â”œâ”€â”€ ga/
+        â”œâ”€â”€ logs/
+        â””â”€â”€ qa/
+\```
+
 ## ğŸ“„ GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼
 
 `create-ga`ã‚³ãƒãƒ³ãƒ‰ã§è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã¯XMLå½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ï¼š
@@ -227,6 +345,8 @@ Alpacaå½¢å¼ã§å‡ºåŠ›ã™ã‚‹éš›ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚€README.mdãŒè‡ªå‹•ç”Ÿ
 
 ### ğŸ“ ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 
 
+#### å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆ
+
 \```
 output_directory/
 â”œâ”€â”€ ga/
@@ -244,6 +364,59 @@ output_directory/
 â””â”€â”€ README.md                       # ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆ--export-alpacaã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ï¼‰
 \```
 
+#### ãƒãƒƒãƒå‡¦ç†ã®å ´åˆï¼ˆ--ga-fileã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨ï¼‰
+
+\```
+output_directory/
+â”œâ”€â”€ doc_A/                          # å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ•ã‚©ãƒ«ãƒ€
+â”‚   â”œâ”€â”€ ga/
+â”‚   â”œâ”€â”€ qa/
+â”‚   â”œâ”€â”€ logs/
+â”‚   â”œâ”€â”€ dataset_alpaca.json         # Alpacaå½¢å¼ï¼ˆ--export-alpacaä½¿ç”¨æ™‚ï¼‰
+â”‚   â””â”€â”€ README.md                   # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆ--export-alpacaä½¿ç”¨æ™‚ï¼‰
+â”œâ”€â”€ doc_B/
+â”‚   â”œâ”€â”€ ga/
+â”‚   â”œâ”€â”€ qa/
+â”‚   â”œâ”€â”€ logs/
+â”‚   â”œâ”€â”€ dataset_alpaca.json
+â”‚   â””â”€â”€ README.md
+â””â”€â”€ doc_C/
+    â”œâ”€â”€ ga/
+    â”œâ”€â”€ qa/
+    â”œâ”€â”€ logs/
+    â”œâ”€â”€ dataset_alpaca.json
+    â””â”€â”€ README.md
+\```
+
+#### ãƒãƒƒãƒå‡¦ç†ã®å ´åˆï¼ˆ--ga-base-dirã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨ï¼‰
+
+\```
+output_directory/
+â”œâ”€â”€ doc_A/                          # å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ•ã‚©ãƒ«ãƒ€
+â”‚   â”œâ”€â”€ ga/                         # ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆGAå®šç¾©ã¯è‡ªå‹•æ¤œå‡ºï¼‰
+â”‚   â”œâ”€â”€ qa/
+â”‚   â”œâ”€â”€ logs/
+â”‚   â”œâ”€â”€ dataset_alpaca.json         # Alpacaå½¢å¼ï¼ˆ--export-alpacaä½¿ç”¨æ™‚ï¼‰
+â”‚   â””â”€â”€ README.md                   # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ï¼ˆ--export-alpacaä½¿ç”¨æ™‚ï¼‰
+â”œâ”€â”€ doc_B/
+â”‚   â”œâ”€â”€ ga/
+â”‚   â”œâ”€â”€ qa/
+â”‚   â”œâ”€â”€ logs/
+â”‚   â”œâ”€â”€ dataset_alpaca.json
+â”‚   â””â”€â”€ README.md
+â””â”€â”€ doc_C/
+    â”œâ”€â”€ ga/
+    â”œâ”€â”€ qa/
+    â”œâ”€â”€ logs/
+    â”œâ”€â”€ dataset_alpaca.json
+    â””â”€â”€ README.md
+
+# GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‹ã‚‰è‡ªå‹•æ¤œå‡ºã•ã‚Œã¾ã™
+# <ga-base-dir>/doc_A/ga/ga_definitions.xml
+# <ga-base-dir>/doc_B/ga/ga_definitions.xml
+# <ga-base-dir>/doc_C/ga/ga_definitions.xml
+\```
+
 ## ğŸ¤– ã‚µãƒãƒ¼ãƒˆã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«
 
 ### ğŸ”‘ OpenAIï¼ˆç›´æ¥ï¼‰
@@ -355,4 +528,3 @@ MIT License
 
 ### ğŸ“„ å‚è€ƒè«–æ–‡
 - **[Dataset Generation for Instruction Tuning](https://arxiv.org/html/2507.04009v1)**
-
diff --git a/easy_dataset_cli/batch_process.py b/easy_dataset_cli/batch_process.py
new file mode 100644
index 0000000..edcec6d
--- /dev/null
+++ b/easy_dataset_cli/batch_process.py
@@ -0,0 +1,414 @@
+#!/usr/bin/env python3
+"""
+ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½
+"""
+
+from pathlib import Path
+from rich.console import Console
+from tqdm import tqdm
+
+from .core import (
+    parse_ga_file,
+    split_text,
+    get_chunk_with_surrounding_context,
+    create_augmented_chunks,
+    convert_to_xml_by_genre,
+    create_output_directories
+)
+from .ga_parser import parse_ga_definitions_from_xml_improved
+
+# generatorsãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
+from .generators import (
+    generate_qa_for_chunk_with_ga,
+    generate_qa_for_chunk_with_ga_and_fulltext,
+    generate_qa_for_chunk_with_ga_and_thinking,
+    generate_qa_for_chunk_with_surrounding_context,
+    generate_ga_definitions
+)
+
+console = Console()
+
+
+def _batch_create_ga_files(text_files, output_dir, model, num_ga_pairs, max_context_length=8000):
+    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã‚’ãƒãƒƒãƒç”Ÿæˆã™ã‚‹å†…éƒ¨é–¢æ•°ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼‰"""
+
+    from .core import create_output_directories, save_ga_definitions_by_genre, parse_ga_definitions_from_xml
+
+    total_files = len(text_files)
+    successful_files = []
+
+    for text_file in (tqdm(text_files, desc="GAç”Ÿæˆä¸­")):
+            console.print(f"\n[bold cyan]å‡¦ç†ä¸­: {text_file.name}[/bold cyan]")
+
+            try:
+                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
+                file_output_dir = output_dir / text_file.stem
+                dirs = create_output_directories(file_output_dir)
+                console.print(f"[dim]âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {file_output_dir}[/dim]")
+
+                text = text_file.read_text(encoding="utf-8")
+                console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—[/dim]")
+
+                with console.status(f"[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­... ({text_file.name})[/bold green]"):
+                    xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs, max_context_length=max_context_length)
+
+                # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+                raw_file_path = dirs["logs"] / "raw.md"
+                raw_file_path.write_text(xml_content, encoding="utf-8")
+                console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path.name}[/cyan]")
+
+                with console.status(f"[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­... ({text_file.name})[/bold green]"):
+                    # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
+                    ga_pairs = parse_ga_definitions_from_xml_improved(xml_content)
+
+                if not ga_pairs:
+                    console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã‹ã‚‰ã¯æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
+                    continue
+
+                # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
+                xml_file_path = dirs["ga"] / "ga_definitions.xml"
+                xml_start = xml_content.find("<GADefinitions>")
+                xml_end = xml_content.rfind("</GADefinitions>")
+                if xml_start != -1 and xml_end != -1:
+                    clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
+                    xml_file_path.write_text(clean_xml, encoding="utf-8")
+                    console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path}[/cyan]")
+
+                # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+                save_ga_definitions_by_genre(ga_pairs, dirs["ga"])
+
+                successful_files.append((text_file.name, file_output_dir, len(ga_pairs)))
+                console.print(f"[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
+
+            except Exception as e:
+                console.print(f"[red]ã‚¨ãƒ©ãƒ¼: {text_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
+                continue
+
+            try:
+                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
+                file_output_dir = output_dir / text_file.stem
+                dirs = create_output_directories(file_output_dir)
+                console.print(f"[dim]âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {file_output_dir}[/dim]")
+
+                text = text_file.read_text(encoding="utf-8")
+                console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—[/dim]")
+
+                with console.status(f"[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­... ({text_file.name})[/bold green]"):
+                    xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs, max_context_length=max_context_length)
+
+                # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+                raw_file_path = dirs["logs"] / "raw.md"
+                raw_file_path.write_text(xml_content, encoding="utf-8")
+                console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path.name}[/cyan]")
+
+                with console.status(f"[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­... ({text_file.name})[/bold green]"):
+                    # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
+                    ga_pairs = parse_ga_definitions_from_xml_improved(xml_content)
+
+                if not ga_pairs:
+                    console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã‹ã‚‰ã¯æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
+                    continue
+
+                # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
+                xml_file_path = dirs["ga"] / "ga_definitions.xml"
+                xml_start = xml_content.find("<GADefinitions>")
+                xml_end = xml_content.rfind("</GADefinitions>")
+                if xml_start != -1 and xml_end != -1:
+                    clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
+                    xml_file_path.write_text(clean_xml, encoding="utf-8")
+                    console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path}[/cyan]")
+
+                # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+                save_ga_definitions_by_genre(ga_pairs, dirs["ga"])
+
+                successful_files.append((text_file.name, file_output_dir, len(ga_pairs)))
+                console.print(f"[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
+
+            except Exception as e:
+                console.print(f"[red]ã‚¨ãƒ©ãƒ¼: {text_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
+                continue
+
+    # tqdmã§ãƒ«ãƒ¼ãƒ—æ¸ˆã¿
+
+    if not successful_files:
+        print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\nç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
+        raise typer.Exit(code=1)
+
+    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
+    total_ga_pairs = sum(count for _, _, count in successful_files)
+    details = [
+        f"{total_ga_pairs}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆ",
+        f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(successful_files)}/{total_files}å€‹",
+        f"å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"
+    ]
+
+    # å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
+    from rich.table import Table
+    files_table = Table(show_header=True, box=None)
+    files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
+    files_table.add_column("ãƒ•ã‚©ãƒ«ãƒ€", style="white")
+    files_table.add_column("GAãƒšã‚¢æ•°", style="green")
+
+    for file_name, output_path, ga_count in successful_files:
+        files_table.add_row(file_name, str(output_path), str(ga_count))
+
+    console.print(Table(title="[bold green]ğŸ“„ å‡¦ç†çµæœ[/bold green]", box=True))
+    console.print(files_table)
+
+    from .commands import print_success_summary
+    print_success_summary("ãƒãƒƒãƒGAãƒšã‚¢ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
+
+    from .commands import Panel
+    next_steps_panel = Panel(
+        "ğŸ” ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
+        "âœï¸ å¿…è¦ã«å¿œã˜ã¦ç·¨é›†\n"
+        "ğŸš€ `generate` ã‚³ãƒãƒ³ãƒ‰ã§Q&Aç”Ÿæˆã¸",
+        title="[bold yellow]ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—[/bold yellow]",
+        border_style="yellow"
+    )
+    console.print(next_steps_panel)
+
+
+def _batch_process_files(text_files, ga_file, ga_base_dir, output_dir, model, chunk_size, chunk_overlap,
+                        num_qa_pairs, use_fulltext, use_thinking, use_surrounding_context,
+                        context_before, context_after, append_mode,
+                        export_alpaca, upload_hf, hf_repo_name, hf_token, hf_private):
+    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹å†…éƒ¨é–¢æ•°ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼‰"""
+
+    # GAãƒšã‚¢ã®è§£æã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«è¡Œã†ï¼ˆga_base_dirãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
+    ga_pairs = None
+    if ga_file:
+        with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
+            ga_pairs = parse_ga_file(ga_file)
+
+        if not ga_pairs:
+            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
+            raise typer.Exit(code=1)
+
+        console.print(f"\n[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
+
+    # ãƒ¢ãƒ¼ãƒ‰è­¦å‘Šã‚’è¡¨ç¤º
+    warnings = []
+    if use_fulltext:
+        warnings.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰")
+    if use_thinking:
+        warnings.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰")
+    if use_surrounding_context:
+        warnings.append(f"ğŸ”— å‘¨è¾ºãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰ ({context_before}å‰+{context_after}å¾Œ)")
+
+    if warnings:
+        from .commands import Panel
+        warning_panel = Panel(
+            "\n".join(warnings) + "\n\nâš ï¸ å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
+            title="[bold yellow]âš ï¸ ãƒ¢ãƒ¼ãƒ‰è­¦å‘Š[/bold yellow]",
+            border_style="yellow"
+        )
+        console.print(warning_panel)
+
+    total_files = len(text_files)
+    successful_files = []
+    total_qa_pairs_generated = 0
+
+    for text_file in (tqdm(text_files, desc="ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­")):
+            console.print(f"\n[bold cyan]å‡¦ç†ä¸­: {text_file.name}[/bold cyan]")
+
+            try:
+                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
+                file_output_dir = output_dir / text_file.stem
+                dirs = create_output_directories(file_output_dir)
+                console.print(f"[dim]âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {file_output_dir}[/dim]")
+
+                text = text_file.read_text(encoding="utf-8")
+                console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—[/dim]")
+
+                # GAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ±ºå®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
+                current_ga_path = None
+                if ga_file:
+                    # å¾“æ¥é€šã‚Šã€æŒ‡å®šã•ã‚ŒãŸå˜ä¸€ã®GAãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
+                    current_ga_path = ga_file
+                    console.print(f"[dim]âœ“ ä½¿ç”¨ã™ã‚‹GAå®šç¾©: {current_ga_path}[/dim]")
+                elif ga_base_dir:
+                    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¯¾å¿œã™ã‚‹GAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’çµ„ã¿ç«‹ã¦ã‚‹
+                    file_stem = text_file.stem
+                    inferred_ga_path = ga_base_dir / file_stem / "ga" / "ga_definitions.xml"
+
+                    if inferred_ga_path.exists():
+                        current_ga_path = inferred_ga_path
+                        console.print(f"[dim]âœ“ GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡º: {current_ga_path}[/dim]")
+                    else:
+                        console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚[/yellow]")
+                        console.print(f"[dim]æ¤œç´¢ãƒ‘ã‚¹: {inferred_ga_path}[/dim]")
+                        continue  # æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸
+
+                # GAãƒšã‚¢ã‚’è§£æ
+                with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
+                    current_ga_pairs = parse_ga_file(current_ga_path)
+
+                if not current_ga_pairs:
+                    console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã®GAå®šç¾©ã‹ã‚‰æœ‰åŠ¹ãªGAãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚[/yellow]")
+                    continue
+
+                console.print(f"[green]âœ“[/green] {len(current_ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹")
+
+                with console.status(f"âœ‚ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­... ({text_file.name})"):
+                    chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
+                console.print(f"[green]âœ“[/green] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ")
+
+                # å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒãƒ£ãƒ³ã‚¯ã‚’æ‹¡å¼µ
+                if use_surrounding_context:
+                    with console.status(f"ğŸ”— å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆä¸­... ({text_file.name})"):
+                        augmented_chunks = create_augmented_chunks(chunks, context_before, context_after)
+                    console.print(f"[green]âœ“[/green] {len(augmented_chunks)}å€‹ã®æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ")
+
+                all_qa_pairs_with_ga = []
+                total_tasks_for_file = len(chunks) * len(current_ga_pairs)
+                # tqdmã‚µãƒ–ãƒãƒ¼
+                from tqdm import tqdm as _tqdm
+                pbar_ctx = _tqdm(total=total_tasks_for_file, desc=text_file.name, leave=False)
+                # é€²æ—ã®æ›´æ–°ã¯å¾Œç¶šã®ãƒ«ãƒ¼ãƒ—å†…ã§è¡Œã†
+
+                if use_surrounding_context:
+                    # å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
+                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­ï¼ˆæœ€å¤§3000æ–‡å­—ï¼‰ã‚’ä»˜ä¸ã—ã¦æ–‡è„ˆã®å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹
+                    doc_head = text[:3000]
+                    for i, (target_chunk, augmented_content, _) in enumerate(augmented_chunks):
+                        for ga_pair in current_ga_pairs:
+                            content_with_head = (
+                                f"ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­ï¼ˆæœ€å¤§3000æ–‡å­—ï¼‰ã€‘:\n{doc_head}\n\n" +
+                                augmented_content
+                            )
+                            qa_pairs = generate_qa_for_chunk_with_surrounding_context(
+                                content=content_with_head,
+                                model=model,
+                                ga_pair=ga_pair,
+                                logs_dir=dirs["logs"],
+                                num_qa_pairs=num_qa_pairs
+                            )
+
+                            for pair in qa_pairs:
+                                qa_entry = {
+                                    "genre": ga_pair['genre']['title'],
+                                    "audience": ga_pair['audience']['title'],
+                                    "question": pair['question'],
+                                    "answer": pair['answer']
+                                }
+                                all_qa_pairs_with_ga.append(qa_entry)
+
+                            pbar_ctx.update(1)
+                else:
+                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
+                    for chunk in chunks:
+                        for ga_pair in current_ga_pairs:
+                            if use_thinking:
+                                qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
+                                    chunk=chunk,
+                                    full_text=text if use_fulltext else "",
+                                    model=model,
+                                    ga_pair=ga_pair,
+                                    logs_dir=dirs["logs"],
+                                    num_qa_pairs=num_qa_pairs
+                                )
+                            elif use_fulltext:
+                                qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
+                                    chunk=chunk,
+                                    full_text=text,
+                                    model=model,
+                                    ga_pair=ga_pair,
+                                    logs_dir=dirs["logs"],
+                                    num_qa_pairs=num_qa_pairs
+                                )
+                            else:
+                                qa_pairs = generate_qa_for_chunk_with_ga(
+                                    chunk, model=model, ga_pair=ga_pair,
+                                    logs_dir=dirs["logs"],
+                                    num_qa_pairs=num_qa_pairs
+                                )
+
+                            for pair in qa_pairs:
+                                qa_entry = {
+                                    "genre": ga_pair['genre']['title'],
+                                    "audience": ga_pair['audience']['title'],
+                                    "question": pair['question'],
+                                    "answer": pair['answer']
+                                }
+                                all_qa_pairs_with_ga.append(qa_entry)
+
+                            pbar_ctx.update(1)
+                # close tqdm sub-bar if used
+                pbar_ctx.close()
+
+                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®Q&Aãƒšã‚¢ã‚’XMLã«å¤‰æ›ã—ã¦ä¿å­˜
+                xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"], append_mode)
+
+                saved_files = []
+                for genre, xml_content in xml_outputs_by_genre.items():
+                    from .core import sanitize_filename
+                    safe_genre_name = sanitize_filename(genre)
+                    output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
+                    output_file_path.write_text(xml_content, encoding="utf-8")
+                    saved_files.append(output_file_path.name)
+
+                # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å€‹åˆ¥ï¼‰
+                if export_alpaca:
+                    from .core import convert_all_xml_to_alpaca, create_dataset_card
+                    alpaca_file = dirs["base"] / "dataset_alpaca.json"
+                    alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
+
+                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
+                    readme_file = dirs["base"] / "README.md"
+                    create_dataset_card(alpaca_data, readme_file, f"Generated QA Dataset from {text_file.name}")
+
+                successful_files.append((text_file.name, file_output_dir, len(all_qa_pairs_with_ga), saved_files))
+                total_qa_pairs_generated += len(all_qa_pairs_with_ga)
+                console.print(f"[green]âœ“[/green] {len(all_qa_pairs_with_ga)}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ")
+
+            except Exception as e:
+                console.print(f"[red]ã‚¨ãƒ©ãƒ¼: {text_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
+                continue
+
+    # tqdmã§å¤–å´ãƒ«ãƒ¼ãƒ—æ¸ˆã¿
+
+    if not successful_files:
+        from .commands import print_error_panel
+        print_error_panel("æœ‰åŠ¹ãªQ&Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
+        import typer
+        raise typer.Exit(code=1)
+
+    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
+    details = [
+        f"{total_qa_pairs_generated}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ",
+        f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(successful_files)}/{total_files}å€‹",
+        f"å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"
+    ]
+
+    # å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
+    from rich.table import Table
+    files_table = Table(show_header=True, box=None)
+    files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
+    files_table.add_column("ãƒ•ã‚©ãƒ«ãƒ€", style="white")
+    files_table.add_column("Q&Aãƒšã‚¢æ•°", style="green")
+
+    for file_name, output_path, qa_count, _ in successful_files:
+        files_table.add_row(file_name, str(output_path), str(qa_count))
+
+    console.print(Table(title="[bold green]ğŸ“„ å‡¦ç†çµæœ[/bold green]", box=True))
+    console.print(files_table)
+
+    from .commands import print_success_summary
+    print_success_summary("ãƒãƒƒãƒQ&Aç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
+
+    # Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆæœ€åˆã®æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå€‹åˆ¥æŒ‡å®šï¼‰
+    if upload_hf and export_alpaca:
+        if not hf_repo_name:
+            console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+            console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
+        else:
+            console.print(f"\n[bold blue]Hugging Face Hub ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«ã¤ã„ã¦[/bold blue]")
+            console.print("[yellow]æ³¨æ„: ç¾åœ¨ã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ãŒå€‹åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€")
+            console.print("å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€çµ±åˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚[/yellow]")
+
+
+def print_error_panel(error_msg: str):
+    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
+    panel = f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼[/bold red]\n{error_msg}"
+    console.print(panel)
diff --git a/easy_dataset_cli/commands.py b/easy_dataset_cli/commands.py
new file mode 100644
index 0000000..3c96394
--- /dev/null
+++ b/easy_dataset_cli/commands.py
@@ -0,0 +1,753 @@
+#!/usr/bin/env python3
+"""
+ã‚³ãƒãƒ³ãƒ‰é–¢æ•°ç¾¤ - CLIã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…
+"""
+
+from pathlib import Path
+from typing_extensions import Annotated
+import typer
+from rich.console import Console
+from rich.text import Text
+from rich.panel import Panel
+from rich.columns import Columns
+from rich.table import Table
+from dotenv import load_dotenv
+
+from .generators import (
+    generate_qa_for_chunk_with_ga,
+    generate_qa_for_chunk_with_ga_and_fulltext,
+    generate_qa_for_chunk_with_ga_and_thinking,
+    generate_qa_for_chunk_with_surrounding_context,
+    generate_ga_definitions
+)
+from .xml_utils import load_existing_xml_file
+from .core import (
+    parse_ga_file,
+    parse_ga_definitions_from_xml,
+    save_ga_definitions_by_genre,
+    create_output_directories,
+    sanitize_filename,
+    convert_all_xml_to_alpaca,
+    upload_to_huggingface,
+    create_dataset_card,
+    find_text_files,
+    get_chunk_with_surrounding_context,
+    create_augmented_chunks,
+    split_text
+)
+from .ga_parser import parse_ga_definitions_from_xml_improved
+from .batch_process import (
+    _batch_create_ga_files,
+    _batch_process_files
+)
+
+# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
+load_dotenv()
+
+app = typer.Typer(
+    help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ãŠã—ã‚ƒã‚ŒãªCLIãƒ„ãƒ¼ãƒ«ã€‚",
+    context_settings={"help_option_names": ["-h", "--help"]}
+)
+console = Console()
+
+
+def print_success_summary(message: str, details: list = None):
+    """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
+    panel = Panel(
+        f"[bold green]âœ¨ {message}[/bold green]",
+        border_style="green",
+        padding=(1, 2)
+    )
+    console.print(panel)
+
+    if details:
+        table = Table(show_header=False, box=None)
+        table.add_column("Item", style="cyan")
+        for detail in details:
+            table.add_row(f"  â€¢ {detail}")
+        console.print(table)
+
+
+def print_error_panel(error_msg: str):
+    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
+    panel = Panel(
+        f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼[/bold red]\n{error_msg}",
+        border_style="red",
+        padding=(1, 2)
+    )
+    console.print(panel)
+
+
+@app.command()
+def create_ga(
+    file_path: Annotated[Path, typer.Argument(
+        exists=True, readable=True,
+        help="GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ãŸå ´åˆã€å†…éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™ã€‚"
+    )],
+    output_dir: Annotated[Path, typer.Option(
+        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
+        help="ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚"
+    )],
+    model: Annotated[str, typer.Option(
+        "--model", "-m",
+        help="GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
+    )] = "openai/gpt-oss-120b",
+    num_ga_pairs: Annotated[int, typer.Option(
+        "--num-ga-pairs", "-g",
+        help="ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
+    )] = 5,
+    max_context_length: Annotated[int, typer.Option(
+        "--max-context-length", "-l",
+        help="GAç”Ÿæˆæ™‚ã«LLMã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§æ–‡å­—æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯8000æ–‡å­—ã§ã™ã€‚å‡¦ç†æ™‚é–“ã‚’çŸ­ãã—ãŸã„å ´åˆã‚„ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆãŸã„å ´åˆã«å°ã•ãè¨­å®šã—ã¦ãã ã•ã„ã€‚"
+    )] = 8000,
+):
+    """å…ƒã®æ–‡ç« ã‚’åˆ†æã—ã€GAãƒšã‚¢å®šç¾©ã‚’XMLå½¢å¼ã§ç”Ÿæˆã—ã€Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
+
+    try:
+        # ãƒ•ã‚©ãƒ«ãƒ€ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
+        if file_path.is_dir():
+            # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆï¼šãƒãƒƒãƒå‡¦ç†
+            console.print(f"[bold blue]ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {file_path}[/bold blue]")
+            text_files = find_text_files(file_path)
+
+            if not text_files:
+                print_error_panel(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
+                raise typer.Exit(code=1)
+
+            console.print(f"[green]âœ“[/green] {len(text_files)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
+
+            # ãƒãƒƒãƒå‡¦ç†ç”¨ã®è¨­å®šãƒ†ãƒ¼ãƒ–ãƒ«
+            batch_info_table = Table(show_header=False, box=None)
+            batch_info_table.add_column("Key", style="bold cyan")
+            batch_info_table.add_column("Value", style="white")
+            batch_info_table.add_row("ğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€", str(file_path))
+            batch_info_table.add_row("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°", str(len(text_files)))
+            batch_info_table.add_row("ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(output_dir))
+            batch_info_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
+            batch_info_table.add_row("ğŸ”¢ GAãƒšã‚¢æ•°", str(num_ga_pairs))
+            batch_info_table.add_row("ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°ä¸Šé™", f"{max_context_length:,}")
+
+            console.print(Panel(batch_info_table, title="[bold blue]ğŸš€ ãƒãƒƒãƒGAãƒšã‚¢ç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
+
+            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
+            files_table = Table(show_header=False, box=None)
+            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
+            for text_file in text_files[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
+                files_table.add_row(f"â€¢ {text_file.name}")
+            if len(text_files) > 10:
+                files_table.add_row(f"... and {len(text_files) - 10} more files")
+
+            console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
+
+            return _batch_create_ga_files(text_files, output_dir, model, num_ga_pairs, max_context_length)
+        else:
+            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼šæ—¢å­˜ã®å‡¦ç†
+            info_table = Table(show_header=False, box=None)
+            info_table.add_column("Key", style="bold cyan")
+            info_table.add_column("Value", style="white")
+            info_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
+            info_table.add_row("ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(output_dir))
+            info_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
+            info_table.add_row("ğŸ”¢ GAãƒšã‚¢æ•°", str(num_ga_pairs))
+            info_table.add_row("ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°ä¸Šé™", f"{max_context_length:,}")
+
+            console.print(Panel(info_table, title="[bold blue]ğŸš€ GAãƒšã‚¢ç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
+
+            text = file_path.read_text(encoding="utf-8")
+            console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]\n")
+
+        with console.status("[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­...[/bold green]"):
+            xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs, max_context_length=max_context_length)
+
+        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
+        dirs = create_output_directories(output_dir)
+        console.print(f"\n[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")
+
+        # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+        raw_file_path = dirs["logs"] / "raw.md"
+        raw_file_path.write_text(xml_content, encoding="utf-8")
+        console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path.name}[/cyan]")
+
+        with console.status("[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­...[/bold green]"):
+            # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
+            ga_pairs = parse_ga_definitions_from_xml_improved(xml_content)
+
+        if not ga_pairs:
+            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\nç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
+            console.print(Panel(xml_content, title="ç”Ÿæˆã•ã‚ŒãŸXML", border_style="yellow"))
+            raise typer.Exit(code=1)
+
+        # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
+        xml_file_path = dirs["ga"] / "ga_definitions.xml"
+        # XMLã‚¿ã‚°éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
+        xml_start = xml_content.find("<GADefinitions>")
+        xml_end = xml_content.rfind("</GADefinitions>")
+        if xml_start != -1 and xml_end != -1:
+            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
+            xml_file_path.write_text(clean_xml, encoding="utf-8")
+            console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path.name}[/cyan]")
+
+        # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
+        save_ga_definitions_by_genre(ga_pairs, dirs["ga"])
+
+        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
+        details = [
+            f"{len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆ",
+            f"ä¿å­˜å…ˆ: {dirs['ga']}",
+            "XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"
+        ]
+        print_success_summary("GAãƒšã‚¢ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
+
+        next_steps_panel = Panel(
+            "ğŸ” ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
+            "âœï¸ å¿…è¦ã«å¿œã˜ã¦ç·¨é›†\n"
+            "ğŸš€ `generate` ã‚³ãƒãƒ³ãƒ‰ã§Q&Aç”Ÿæˆã¸",
+            title="[bold yellow]ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—[/bold yellow]",
+            border_style="yellow"
+        )
+        console.print(next_steps_panel)
+
+    except Exception as e:
+        print_error_panel(f"GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e}")
+        raise typer.Exit(code=1)
+
+
+@app.command()
+def generate(
+    file_path: Annotated[Path, typer.Argument(
+        exists=True, readable=True,
+        help="å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ãƒ‘ã‚¹ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ãŸå ´åˆã€å†…éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™ã€‚"
+    )],
+    ga_file: Annotated[Path, typer.Option(
+        "--ga-file", "-g", exists=True, dir_okay=False, readable=True,
+        help="Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLã¾ãŸã¯Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚ãƒãƒƒãƒå‡¦ç†ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«å…±é€šã®å®šç¾©ã‚’é©ç”¨ã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚"
+    )] = None,
+    ga_base_dir: Annotated[Path, typer.Option(
+        "--ga-base-dir", "-gb", exists=True, file_okay=False, dir_okay=True, readable=True,
+        help="GAå®šç¾©ãƒ•ã‚©ãƒ«ãƒ€ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒãƒƒãƒå‡¦ç†æ™‚ã«å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚"
+    )] = None,
+    output_dir: Annotated[Path, typer.Option(
+        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
+        help="ç”Ÿæˆã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚"
+    )] = None,
+    model: Annotated[str, typer.Option(
+        "--model", "-m",
+        help="Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
+    )] = "openai/gpt-oss-120b",
+    chunk_size: Annotated[int, typer.Option(
+        help="ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚ºã€‚"
+    )] = 2000,
+    chunk_overlap: Annotated[int, typer.Option(
+        help="ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚"
+    )] = 200,
+    num_qa_pairs: Annotated[int, typer.Option(
+        "--num-qa-pairs", "-q",
+        help="å„ãƒãƒ£ãƒ³ã‚¯ãƒ»GAãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
+    )] = 10,
+    use_fulltext: Annotated[bool, typer.Option(
+        "--use-fulltext", "-f",
+        help="å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
+    )] = False,
+    use_thinking: Annotated[bool, typer.Option(
+        "--use-thinking", "-T",
+        help="å„Q&Aãƒšã‚¢ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã—ã¾ã™ã€‚ã‚ˆã‚Šæ·±ã„ç†è§£ã¨èª¬æ˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
+    )] = False,
+    use_surrounding_context: Annotated[bool, typer.Option(
+        "--use-surrounding-context", "-S",
+        help="å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
+    )] = False,
+    context_before: Annotated[int, typer.Option(
+        help="å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹å‰æ–¹ãƒãƒ£ãƒ³ã‚¯æ•°ã€‚"
+    )] = 1,
+    context_after: Annotated[int, typer.Option(
+        help="å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹å¾Œæ–¹ãƒãƒ£ãƒ³ã‚¯æ•°ã€‚"
+    )] = 1,
+    append_mode: Annotated[bool, typer.Option(
+        "--append", "-A",
+        help="æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„Q&Aã‚’è¿½åŠ ã—ã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä¸Šæ›¸ãã—ã¾ã™ã€‚"
+    )] = False,
+    export_alpaca: Annotated[bool, typer.Option(
+        "--export-alpaca", "-a",
+        help="ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"
+    )] = False,
+    upload_hf: Annotated[bool, typer.Option(
+        "--upload-hf", "-u",
+        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
+    )] = False,
+    hf_repo_name: Annotated[str, typer.Option(
+        "--hf-repo-name", "-r",
+        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
+    )] = "",
+    hf_token: Annotated[str, typer.Option(
+        "--hf-token", "-t",
+        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
+    )] = "",
+    hf_private: Annotated[bool, typer.Option(
+        "--hf-private",
+        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
+    )] = False,
+):
+    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
+
+    --use-fulltextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†æ™‚ã«å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
+    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ãŸã ã—ã€å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
+
+    --use-surrounding-contextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
+    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚--use-fulltextã‚ˆã‚Šã‚‚å‡¦ç†ã‚³ã‚¹ãƒˆãŒä½ãæŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚
+    --context-beforeã¨--context-afterã§å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’èª¿æ•´å¯èƒ½ã§ã™ã€‚
+    """
+
+    try:
+        # ãƒ•ã‚©ãƒ«ãƒ€ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
+        if file_path.is_dir():
+            # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆï¼šãƒãƒƒãƒå‡¦ç†
+            console.print(f"[bold blue]ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {file_path}[/bold blue]")
+            text_files = find_text_files(file_path)
+
+            if not text_files:
+                print_error_panel(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
+                raise typer.Exit(code=1)
+
+            console.print(f"[green]âœ“[/green] {len(text_files)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
+
+            # ãƒãƒƒãƒå‡¦ç†ç”¨ã®è¨­å®šãƒ†ãƒ¼ãƒ–ãƒ«
+            batch_settings_table = Table(show_header=False, box=None)
+            batch_settings_table.add_column("é …ç›®", style="bold cyan")
+            batch_settings_table.add_column("å€¤", style="white")
+            batch_settings_table.add_row("ğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€", str(file_path))
+            batch_settings_table.add_row("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°", str(len(text_files)))
+
+            # GAå®šç¾©ã®è¡¨ç¤º
+            if ga_file:
+                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©", str(ga_file))
+            elif ga_base_dir:
+                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(ga_base_dir))
+            else:
+                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©", "æœªæŒ‡å®š")
+
+            batch_settings_table.add_row("ğŸ“ å‡ºåŠ›å…ˆ", str(output_dir) if output_dir else "ã‚³ãƒ³ã‚½ãƒ¼ãƒ«")
+            batch_settings_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
+            batch_settings_table.add_row("ğŸ”¢ Q&Aæ•°/ãƒãƒ£ãƒ³ã‚¯", str(num_qa_pairs))
+
+            mode_options = []
+            if use_fulltext: mode_options.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
+            if use_thinking: mode_options.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼")
+            if use_surrounding_context: mode_options.append(f"ğŸ”— å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ({context_before}å‰+{context_after}å¾Œ)")
+            if append_mode: mode_options.append("â• è¿½åŠ ãƒ¢ãƒ¼ãƒ‰")
+            if export_alpaca: mode_options.append("ğŸ¤™ Alpacaå½¢å¼")
+            if upload_hf: mode_options.append("ğŸ¤— HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
+
+            if mode_options:
+                batch_settings_table.add_row("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ", ".join(mode_options))
+
+            console.print(Panel(batch_settings_table, title="[bold blue]ğŸš€ ãƒãƒƒãƒQ&Aç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
+
+            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
+            files_table = Table(show_header=False, box=None)
+            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
+            for text_file in text_files[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
+                files_table.add_row(f"â€¢ {text_file.name}")
+            if len(text_files) > 10:
+                files_table.add_row(f"... and {len(text_files) - 10} more files")
+
+            console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
+
+            # ãƒãƒƒãƒå‡¦ç†ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
+            if not ga_file and not ga_base_dir:
+                print_error_panel("ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã†ã«ã¯ã€--ga-file ã¾ãŸã¯ --ga-base-dir ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
+                raise typer.Exit(code=1)
+
+            if ga_file and ga_base_dir:
+                print_error_panel("--ga-file ã¨ --ga-base-dir ã¯åŒæ™‚ã«ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
+                raise typer.Exit(code=1)
+
+            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†
+            return _batch_process_files(text_files, ga_file, ga_base_dir, output_dir, model, chunk_size, chunk_overlap,
+                                      num_qa_pairs, use_fulltext, use_thinking, use_surrounding_context,
+                                      context_before, context_after, append_mode,
+                                      export_alpaca, upload_hf, hf_repo_name, hf_token, hf_private)
+        else:
+            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼šæ—¢å­˜ã®å‡¦ç†
+            # è¨­å®šæƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
+            settings_table = Table(show_header=False, box=None)
+            settings_table.add_column("é …ç›®", style="bold cyan")
+            settings_table.add_column("å€¤", style="white")
+            settings_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
+            settings_table.add_row("ğŸ“Š GAå®šç¾©", str(ga_file) if ga_file else "æœªæŒ‡å®š")
+            settings_table.add_row("ğŸ“ å‡ºåŠ›å…ˆ", str(output_dir) if output_dir else "ã‚³ãƒ³ã‚½ãƒ¼ãƒ«")
+            settings_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
+            settings_table.add_row("ğŸ”¢ Q&Aæ•°/ãƒãƒ£ãƒ³ã‚¯", str(num_qa_pairs))
+
+            mode_options = []
+            if use_fulltext: mode_options.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
+            if use_thinking: mode_options.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼")
+            if use_surrounding_context: mode_options.append(f"ğŸ”— å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ({context_before}å‰+{context_after}å¾Œ)")
+            if append_mode: mode_options.append("â• è¿½åŠ ãƒ¢ãƒ¼ãƒ‰")
+            if export_alpaca: mode_options.append("ğŸ¤™ Alpacaå½¢å¼")
+            if upload_hf: mode_options.append("ğŸ¤— HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
+
+            if mode_options:
+                settings_table.add_row("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ", ".join(mode_options))
+
+            console.print(Panel(settings_table, title="[bold blue]ğŸš€ Q&Aç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
+
+            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
+            if not ga_file:
+                print_error_panel("å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã«ã¯ --ga-file ã®æŒ‡å®šãŒå¿…é ˆã§ã™ã€‚")
+                raise typer.Exit(code=1)
+
+            text = file_path.read_text(encoding="utf-8")
+            console.print(f"\n[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]")
+
+        with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
+            ga_pairs = parse_ga_file(ga_file)
+
+        if not ga_pairs:
+            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
+            raise typer.Exit(code=1)
+
+        console.print(f"\n[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
+
+        with console.status("âœ‚ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­..."):
+            chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
+        console.print(f"[green]âœ“[/green] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")
+
+        # å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒãƒ£ãƒ³ã‚¯ã‚’æ‹¡å¼µ
+        if use_surrounding_context:
+            with console.status("ğŸ”— å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆä¸­..."):
+                augmented_chunks = create_augmented_chunks(chunks, context_before, context_after)
+            console.print(f"[green]âœ“[/green] {len(augmented_chunks)}å€‹ã®æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")
+
+        all_qa_pairs_with_ga = []
+        total_tasks = len(chunks) * len(ga_pairs)
+
+        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯æ§‹é€ ã‚’ä½œæˆ
+        dirs = None
+        if output_dir:
+            dirs = create_output_directories(output_dir)
+            console.print(f"[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")
+
+        # ãƒ¢ãƒ¼ãƒ‰è­¦å‘Šã‚’è¡¨ç¤º
+        warnings = []
+        if use_fulltext:
+            warnings.append(f"ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ ({len(text):,} æ–‡å­—)")
+        if use_thinking:
+            warnings.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰")
+        if use_surrounding_context:
+            warnings.append(f"ğŸ”— å‘¨è¾ºãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰ ({context_before}å‰+{context_after}å¾Œ)")
+
+        if warnings:
+            warning_panel = Panel(
+                "\n".join(warnings) + "\n\nâš ï¸ å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
+                title="[bold yellow]âš ï¸ ãƒ¢ãƒ¼ãƒ‰è­¦å‘Š[/bold yellow]",
+                border_style="yellow"
+            )
+            console.print(warning_panel)
+
+        # tqdmãƒ™ãƒ¼ã‚¹ã®é€²æ—è¡¨ç¤ºã«çµ±ä¸€
+        from tqdm import tqdm
+        desc = "Q&Aç”Ÿæˆä¸­"
+        iterable_outer = augmented_chunks if use_surrounding_context else chunks
+        with tqdm(total=total_tasks, desc=desc) as pbar:
+            if use_surrounding_context:
+                doc_head = text[:3000]
+                for i, (target_chunk, augmented_content, _) in enumerate(iterable_outer):
+                    for ga_pair in ga_pairs:
+                        content_with_head = (
+                            f"### ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†’é ­ï¼ˆæœ€å¤§3000æ–‡å­—ï¼‰ã€‘-----------:\n\```\n{doc_head}\n\```\n" +
+                            augmented_content
+                        )
+                        qa_pairs = generate_qa_for_chunk_with_surrounding_context(
+                            content=content_with_head,
+                            model=model,
+                            ga_pair=ga_pair,
+                            logs_dir=dirs["logs"] if dirs else None,
+                            num_qa_pairs=num_qa_pairs
+                        )
+
+                        for pair in qa_pairs:
+                            qa_entry = {
+                                "genre": ga_pair['genre']['title'],
+                                "audience": ga_pair['audience']['title'],
+                                "question": pair['question'],
+                                "answer": pair['answer']
+                            }
+                            all_qa_pairs_with_ga.append(qa_entry)
+
+                        pbar.update(1)
+            else:
+                for chunk in iterable_outer:
+                    for ga_pair in ga_pairs:
+                        if use_thinking:
+                            qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
+                                chunk=chunk,
+                                full_text=text if use_fulltext else "",
+                                model=model,
+                                ga_pair=ga_pair,
+                                logs_dir=dirs["logs"] if dirs else None,
+                                num_qa_pairs=num_qa_pairs
+                            )
+                        elif use_fulltext:
+                            qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
+                                chunk=chunk,
+                                full_text=text,
+                                model=model,
+                                ga_pair=ga_pair,
+                                logs_dir=dirs["logs"] if dirs else None,
+                                num_qa_pairs=num_qa_pairs
+                            )
+                        else:
+                            qa_pairs = generate_qa_for_chunk_with_ga(
+                                chunk, model=model, ga_pair=ga_pair,
+                                logs_dir=dirs["logs"] if dirs else None,
+                                num_qa_pairs=num_qa_pairs
+                            )
+
+                        for pair in qa_pairs:
+                            qa_entry = {
+                                "genre": ga_pair['genre']['title'],
+                                "audience": ga_pair['audience']['title'],
+                                "question": pair['question'],
+                                "answer": pair['answer']  # <think>...</think>å›ç­”...å½¢å¼ãŒãã®ã¾ã¾å…¥ã‚‹
+                            }
+                            all_qa_pairs_with_ga.append(qa_entry)
+
+                        pbar.update(1)
+
+        generation_summary = Panel(
+            f"âœ¨ [bold green]{len(all_qa_pairs_with_ga)}[/bold green] å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆå®Œäº†ï¼",
+            title="[bold green]âœ… ç”Ÿæˆçµæœ[/bold green]",
+            border_style="green"
+        )
+        console.print(generation_summary)
+
+        if dirs:
+            from .xml_utils import convert_to_xml_by_genre
+
+            xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"], append_mode)
+
+            with console.status(f"ğŸ’¾ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {dirs['qa']} ã«ä¿å­˜ä¸­..."):
+                saved_files = []
+                for genre, xml_content in xml_outputs_by_genre.items():
+                    safe_genre_name = sanitize_filename(genre)
+                    output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
+                    output_file_path.write_text(xml_content, encoding="utf-8")
+                    saved_files.append(output_file_path.name)
+
+            files_table = Table(show_header=False, box=None)
+            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
+            for file_name in saved_files:
+                files_table.add_row(f"âœ“ {file_name}")
+
+                console.print(Panel(files_table, title="[bold green]ğŸ’¾ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
+
+            # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
+            if export_alpaca:
+                console.print("\n[bold blue]Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...[/bold blue]")
+                alpaca_file = dirs["base"] / "dataset_alpaca.json"
+                alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
+
+                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
+                readme_file = dirs["base"] / "README.md"
+                create_dataset_card(alpaca_data, readme_file, "Generated QA Dataset")
+
+                # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+                if upload_hf:
+                    if not hf_repo_name:
+                        console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+                        console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
+                    else:
+                        console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
+                        success = upload_to_huggingface(
+                            dataset_data=alpaca_data,
+                            repo_name=hf_repo_name,
+                            hf_token=hf_token if hf_token else None,
+                            private=hf_private,
+                            commit_message=f"Upload QA dataset with {len(alpaca_data)} entries",
+                            readme_file=readme_file
+                        )
+                        if not success:
+                            console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
+        else:
+            console.print("\n--- ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ (Genreåˆ¥XML) ---")
+            if dirs:
+                from .xml_utils import convert_to_xml_by_genre
+                xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, None, append_mode)
+                for genre, xml_content in xml_outputs_by_genre.items():
+                    console.print(f"\n[bold yellow]## Genre: {genre} ##[/bold yellow]")
+                    console.print(xml_content, overflow="fold")
+
+    except Exception as e:
+        import traceback
+        error_details = f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}\n\nãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}"
+        print_error_panel(error_details)
+        raise typer.Exit(code=1)
+
+
+@app.command()
+def convert_to_alpaca(
+    qa_dir: Annotated[Path, typer.Argument(
+        exists=True, dir_okay=True, readable=True,
+        help="XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹qaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
+    )],
+    output_file: Annotated[Path, typer.Option(
+        "--output-file", "-o", file_okay=True, dir_okay=False,
+        help="å‡ºåŠ›ã™ã‚‹Alpacaå½¢å¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚"
+    )] = None,
+    upload_hf: Annotated[bool, typer.Option(
+        "--upload-hf", "-u",
+        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
+    )] = False,
+    hf_repo_name: Annotated[str, typer.Option(
+        "--hf-repo-name", "-r",
+        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
+    )] = "",
+    hf_token: Annotated[str, typer.Option(
+        "--hf-token", "-t",
+        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
+    )] = "",
+    hf_private: Annotated[bool, typer.Option(
+        "--hf-private",
+        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
+    )] = False,
+):
+    """æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã®JSONã«å¤‰æ›ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
+
+    conversion_table = Table(show_header=False, box=None)
+    conversion_table.add_column("é …ç›®", style="bold cyan")
+    conversion_table.add_column("å€¤", style="white")
+    conversion_table.add_row("ğŸ“ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(qa_dir))
+    conversion_table.add_row("ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(output_file) if output_file else "è‡ªå‹•")
+    if upload_hf:
+        conversion_table.add_row("ğŸ¤— HFãƒªãƒã‚¸ãƒˆãƒª", hf_repo_name or "æœªæŒ‡å®š")
+
+    console.print(Panel(conversion_table, title="[bold blue]ğŸ”„ Alpacaå½¢å¼å¤‰æ›[/bold blue]", border_style="blue"))
+
+    try:
+        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
+        if output_file is None:
+            output_file = qa_dir.parent / "dataset_alpaca.json"
+
+        with console.status(f"ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã«å¤‰æ›ä¸­..."):
+            alpaca_data = convert_all_xml_to_alpaca(qa_dir, output_file)
+
+        if not alpaca_data:
+            print_error_panel("å¤‰æ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
+            raise typer.Exit(code=1)
+
+        with console.status("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
+            readme_file = output_file.parent / "README.md"
+            create_dataset_card(alpaca_data, readme_file, "Converted QA Dataset")
+
+        # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
+        if upload_hf:
+            if not hf_repo_name:
+                print_error_panel("--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼\nä¾‹: --hf-repo-name username/my-qa-dataset")
+                raise typer.Exit(code=1)
+
+            with console.status(f"ğŸ¤— Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
+                success = upload_to_huggingface(
+                    dataset_data=alpaca_data,
+                    repo_name=hf_repo_name,
+                    hf_token=hf_token if hf_token else None,
+                    private=hf_private,
+                    commit_message=f"Upload converted QA dataset with {len(alpaca_data)} entries",
+                    readme_file=readme_file
+                )
+
+            if not success:
+                print_error_panel("Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
+                raise typer.Exit(code=1)
+
+        details = [
+            f"{len(alpaca_data)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å¤‰æ›",
+            f"å‡ºåŠ›å…ˆ: {output_file}",
+            f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰: {readme_file}"
+        ]
+        if upload_hf and hf_repo_name:
+            details.append(f"Hugging Face: {hf_repo_name}")
+
+        print_success_summary("Alpacaå½¢å¼ã¸ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
+
+    except Exception as e:
+        print_error_panel(f"å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
+        raise typer.Exit(code=1)
+
+
+@app.command()
+def aggregate_logs(
+    output_dir: Annotated[Path, typer.Argument(
+        exists=True, dir_okay=True, readable=True,
+        help="logsãƒ•ã‚©ãƒ«ãƒ€ãŒå«ã¾ã‚Œã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
+    )]
+):
+    """logsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ã—ã¦qaãƒ•ã‚©ãƒ«ãƒ€ã®XMLã‚’ç”Ÿæˆã—ã¾ã™."""
+
+    try:
+        logs_dir = output_dir / "logs"
+        qa_dir = output_dir / "qa"
+
+        if not logs_dir.exists():
+            print_error_panel(f"logsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {logs_dir}")
+            raise typer.Exit(code=1)
+
+        aggregation_table = Table(show_header=False, box=None)
+        aggregation_table.add_column("é …ç›®", style="bold cyan")
+        aggregation_table.add_column("ãƒ‘ã‚¹", style="white")
+        aggregation_table.add_row("ğŸ“ logsãƒ•ã‚©ãƒ«ãƒ€", str(logs_dir))
+        aggregation_table.add_row("ğŸ¯ å‡ºåŠ›å…ˆ", str(qa_dir))
+
+        console.print(Panel(aggregation_table, title="[bold blue]ğŸ“„ ãƒ­ã‚°é›†ç´„[/bold blue]", border_style="blue"))
+
+        with console.status("ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ä¸­..."):
+            from easy_dataset_cli.core import aggregate_logs_xml_to_qa
+            aggregate_logs_xml_to_qa(logs_dir, qa_dir)
+
+        print_success_summary("ãƒ­ã‚°é›†ç´„ãŒå®Œäº†ã—ã¾ã—ãŸï¼", [f"å‡ºåŠ›å…ˆ: {qa_dir}"])
+
+    except Exception as e:
+        print_error_panel(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
+        raise typer.Exit(code=1)
+
+
+
+
+def print_logo():
+    """ãŠã—ã‚ƒã‚Œãªãƒ­ã‚´ã‚’è¡¨ç¤º"""
+    try:
+        from art import tprint, text2art
+        ART_AVAILABLE = True
+    except ImportError:
+        ART_AVAILABLE = False
+
+    if ART_AVAILABLE:
+        console.print("\n")
+        # ã‚·ãƒ³ãƒ—ãƒ«ã§èª­ã¿ã‚„ã™ã„ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
+        try:
+            logo_text = text2art("Easy Dataset CLI", font="colossal")
+        except:
+            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆ
+            logo_text = text2art("Easy Dataset CLI")
+
+        # å„è¡Œã‚’ä¸­å¤®æƒãˆã«èª¿æ•´
+        lines = logo_text.strip().split('\n')
+        max_width = max(len(line.rstrip()) for line in lines) if lines else 0
+
+        # ãƒ‘ãƒãƒ«å†…ã§ä¸­å¤®æƒãˆã™ã‚‹ãŸã‚ã€Textã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨
+        logo_panel = Panel(
+            Text(logo_text.strip(), style="bold cyan", justify="center"),
+            title="[bold green]ğŸš€ Easy Dataset CLI[/bold green]",
+            subtitle="[italic]Powered by AI[/italic]",
+            border_style="bright_blue",
+            padding=(1, 2),
+            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
+        )
+        console.print(logo_panel)
+    else:
+        header = Panel(
+            Text("ğŸš€ Easy Dataset CLI\nãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Q&Aãƒšã‚¢ã‚’è‡ªå‹•ç”Ÿæˆ", style="bold cyan", justify="center"),
+            border_style="bright_blue",
+            padding=(1, 2),
+            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
+        )
+        console.print(header)
diff --git a/easy_dataset_cli/core.py b/easy_dataset_cli/core.py
index b308563..4764394 100644
--- a/easy_dataset_cli/core.py
+++ b/easy_dataset_cli/core.py
@@ -6,18 +6,25 @@ from .ga_parser import (
     parse_ga_file,
     parse_ga_definitions_from_xml
 )
-from .qa_generator import (
+from .generators import (
     generate_qa_for_chunk_with_ga,
     generate_qa_for_chunk_with_ga_and_fulltext,
     generate_qa_for_chunk_with_ga_and_thinking,
+    generate_qa_for_chunk_with_surrounding_context,
     generate_ga_definitions
 )
-from .text_splitter import split_text
+from .text_splitter import (
+    split_text,
+    get_chunk_with_surrounding_context,
+    create_augmented_chunks
+)
 from .xml_utils import convert_to_xml_by_genre, load_existing_xml_file, aggregate_logs_xml_to_qa
 from .file_utils import (
     create_output_directories,
     save_ga_definitions_by_genre,
-    sanitize_filename
+    sanitize_filename,
+    find_text_files,
+    batch_process_files
 )
 from .alpaca_converter import (
     convert_all_xml_to_alpaca,
@@ -35,10 +42,13 @@ __all__ = [
     'generate_qa_for_chunk_with_ga',
     'generate_qa_for_chunk_with_ga_and_fulltext',
     'generate_qa_for_chunk_with_ga_and_thinking',
+    'generate_qa_for_chunk_with_surrounding_context',
     'generate_ga_definitions',
-    
+
     # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
     'split_text',
+    'get_chunk_with_surrounding_context',
+    'create_augmented_chunks',
     
     # XMLå‡¦ç†
     'convert_to_xml_by_genre',
@@ -49,6 +59,8 @@ __all__ = [
     'create_output_directories',
     'save_ga_definitions_by_genre',
     'sanitize_filename',
+    'find_text_files',
+    'batch_process_files',
     
     # ã‚¢ãƒ«ãƒ‘ã‚«å¤‰æ›ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
     'convert_all_xml_to_alpaca',
diff --git a/easy_dataset_cli/file_utils.py b/easy_dataset_cli/file_utils.py
index f42280a..03bab5c 100644
--- a/easy_dataset_cli/file_utils.py
+++ b/easy_dataset_cli/file_utils.py
@@ -57,3 +57,30 @@ def save_ga_definitions_by_genre(ga_pairs: List[Dict[str, Dict[str, str]]], ga_d
 def sanitize_filename(name: str) -> str:
     """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
     return "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).rstrip()
+
+
+def find_text_files(directory: Path) -> List[Path]:
+    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢ã™ã‚‹"""
+    text_extensions = {'.txt', '.md', '.rst', '.org', '.tex', '.text'}
+    text_files = []
+    
+    for file_path in directory.rglob('*'):
+        if file_path.is_file() and file_path.suffix.lower() in text_extensions:
+            text_files.append(file_path)
+    
+    return sorted(text_files)
+
+
+def batch_process_files(files: List[Path], processor_func, *args, **kwargs) -> Dict[str, any]:
+    """è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒã§å‡¦ç†ã™ã‚‹"""
+    results = {}
+    
+    for file_path in files:
+        try:
+            result = processor_func(file_path, *args, **kwargs)
+            results[str(file_path)] = result
+        except Exception as e:
+            console.print(f"[red]Error processing {file_path}: {e}[/red]")
+            results[str(file_path)] = None
+    
+    return results
diff --git a/easy_dataset_cli/ga_parser.py b/easy_dataset_cli/ga_parser.py
index cdeb17d..06d1205 100644
--- a/easy_dataset_cli/ga_parser.py
+++ b/easy_dataset_cli/ga_parser.py
@@ -6,6 +6,7 @@ from pathlib import Path
 from typing import List, Dict
 import mistune
 from rich.console import Console
+import re
 
 console = Console()
 
@@ -68,9 +69,10 @@ def parse_ga_file(file_path: Path) -> List[Dict[str, Dict[str, str]]]:
             pairs = parse_ga_markdown_fallback(text)
 
     except ET.ParseError as e:
-        console.print(f"[yellow]XMLè§£æã‚¨ãƒ©ãƒ¼: {e}[/yellow]")
-        # XMLè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è§£æã‚’è©¦è¡Œ
-        pairs = parse_ga_markdown_fallback(text)
+        console.print(f"[red]XMLè§£æã‚¨ãƒ©ãƒ¼: {e}[/red]")
+        # XMLè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯æ”¹è‰¯ç‰ˆã‚’è©¦è¡Œ
+        console.print("[yellow]æ”¹è‰¯ç‰ˆXMLè§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+        pairs = parse_ga_definitions_from_xml_improved(text)
 
     console.print(f"[dim]æœ€çµ‚çš„ã«è§£æã•ã‚ŒãŸãƒšã‚¢æ•°: {len(pairs)}[/dim]")
     return pairs
@@ -92,7 +94,7 @@ def parse_ga_markdown_fallback(text: str) -> List[Dict[str, Dict[str, str]]]:
 
         for node in ast:
             if node['type'] == 'heading':
-                header_text = "".join(child['text'] for child in node['children'])
+                header_text = "".join(child.get('text', '') for child in node['children'])
                 if 'genre' in header_text.lower():
                     current_type = 'genre'
                     genre['title'] = header_text.replace('Genre:', '').strip()
@@ -100,7 +102,7 @@ def parse_ga_markdown_fallback(text: str) -> List[Dict[str, Dict[str, str]]]:
                     current_type = 'audience'
                     audience['title'] = header_text.replace('Audience:', '').strip()
             elif node['type'] == 'paragraph':
-                description = "".join(child['text'] for child in node['children'])
+                description = "".join(child.get('text', '') for child in node['children'])
                 if current_type == 'genre':
                     genre['description'] = description
                 elif current_type == 'audience':
@@ -180,3 +182,75 @@ def parse_ga_definitions_from_xml(xml_content: str) -> List[Dict[str, Dict[str,
         console.print(f"[bold red]äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
 
     return pairs
+
+
+def parse_ga_definitions_from_xml_improved(xml_content: str) -> List[Dict[str, Dict[str, str]]]:
+    """æ”¹è‰¯ç‰ˆ: æ­£è¦è¡¨ç¾ã‚’ä½¿ã„ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„ä¸æ­£ãªXMLã‚’å‡¦ç†ã§ãã‚‹GAå®šç¾©è§£æé–¢æ•°"""
+    pairs = []
+    console.print(f"[dim]XMLè§£æé–‹å§‹: å†…å®¹é•· {len(xml_content)} æ–‡å­—[/dim]")
+
+    try:
+        # Step 1: æ­£è¦è¡¨ç¾ã§<GADefinitions>ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
+        # re.DOTALLãƒ•ãƒ©ã‚°ã«ã‚ˆã‚Šã€æ”¹è¡Œæ–‡å­—ã‚’å«ã‚€æ–‡å­—åˆ—å…¨ä½“ã‚’æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
+        match = re.search(r"<GADefinitions>.*?</GADefinitions>", xml_content, re.DOTALL)
+
+        if not match:
+            console.print("[yellow]GADefinitionsã‚¿ã‚°ã§å›²ã¾ã‚ŒãŸXMLãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
+            console.print("[yellow]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+            return parse_ga_definitions_from_xml(xml_content)
+
+        raw_xml = match.group(0)
+        console.print(f"[dim]æ­£è¦è¡¨ç¾ã§XMLãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºå®Œäº†: {len(raw_xml)} æ–‡å­—[/dim]")
+
+        # Step 2: XMLå®£è¨€ã‚’è¿½åŠ 
+        if not raw_xml.startswith("<?xml"):
+            raw_xml = '<?xml version="1.0" encoding="utf-8"?>\n' + raw_xml
+
+        # Step 3: XMLè§£æã‚’å®Ÿè¡Œ
+        root = ET.fromstring(raw_xml)
+        pair_nodes = root.findall('Pair')
+        console.print(f"[dim]è¦‹ã¤ã‹ã£ãŸPairãƒãƒ¼ãƒ‰æ•°: {len(pair_nodes)}[/dim]")
+
+        for i, pair_node in enumerate(pair_nodes):
+            genre_node = pair_node.find('Genre')
+            audience_node = pair_node.find('Audience')
+            
+            if genre_node is not None and audience_node is not None:
+                genre_title_node = genre_node.find('Title')
+                genre_desc_node = genre_node.find('Description')
+                audience_title_node = audience_node.find('Title')
+                audience_desc_node = audience_node.find('Description')
+
+                # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
+                if all(node is not None and node.text and node.text.strip() for node in [genre_title_node, genre_desc_node, audience_title_node, audience_desc_node]):
+                    genre_title = genre_title_node.text.strip()
+                    audience_title = audience_title_node.text.strip()
+                    pairs.append({
+                        "genre": {
+                            "title": genre_title,
+                            "description": genre_desc_node.text.strip()
+                        },
+                        "audience": {
+                            "title": audience_title,
+                            "description": audience_desc_node.text.strip()
+                        }
+                    })
+                    console.print(f"[green]âœ“[/green] {genre_title} x {audience_title}")
+                else:
+                    console.print(f"[yellow]âš [/yellow] Pair {i+1}: è¦ç´ ãŒç©ºã¾ãŸã¯ç„¡åŠ¹")
+            else:
+                console.print(f"[yellow]âš [/yellow] Pair {i+1}: Genreã¾ãŸã¯Audienceãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
+
+    except ET.ParseError as parse_error:
+        console.print(f"[bold red]XMLè§£æã‚¨ãƒ©ãƒ¼:[/bold red] {parse_error}")
+        console.print(f"[dim]ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã—ãŸXMLå†…å®¹: {raw_xml if 'raw_xml' in locals() else 'N/A'}[/dim]")
+        console.print("[yellow]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+        return parse_ga_definitions_from_xml(xml_content)
+
+    except Exception as e:
+        console.print(f"[bold red]äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:[/bold red] {e}")
+        console.print("[yellow]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+        return parse_ga_definitions_from_xml(xml_content)
+
+    console.print(f"[dim]æœ€çµ‚çš„ã«è§£æã•ã‚ŒãŸãƒšã‚¢æ•°: {len(pairs)}[/dim]")
+    return pairs
diff --git a/easy_dataset_cli/generators/__init__.py b/easy_dataset_cli/generators/__init__.py
new file mode 100644
index 0000000..48d99df
--- /dev/null
+++ b/easy_dataset_cli/generators/__init__.py
@@ -0,0 +1,20 @@
+#!/usr/bin/env python3
+"""
+Q&Aç”Ÿæˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
+"""
+
+from .qa_generator import generate_qa_for_chunk_with_ga
+from .qa_generator_fulltext import generate_qa_for_chunk_with_ga_and_fulltext
+from .qa_generator_thinking import (
+    generate_qa_for_chunk_with_ga_and_thinking,
+    generate_qa_for_chunk_with_surrounding_context
+)
+from .ga_generator import generate_ga_definitions
+
+__all__ = [
+    'generate_qa_for_chunk_with_ga',
+    'generate_qa_for_chunk_with_ga_and_fulltext',
+    'generate_qa_for_chunk_with_ga_and_thinking',
+    'generate_qa_for_chunk_with_surrounding_context',
+    'generate_ga_definitions'
+]
diff --git a/easy_dataset_cli/generators/ga_generator.py b/easy_dataset_cli/generators/ga_generator.py
new file mode 100644
index 0000000..740e26f
--- /dev/null
+++ b/easy_dataset_cli/generators/ga_generator.py
@@ -0,0 +1,60 @@
+#!/usr/bin/env python3
+"""
+GAå®šç¾©ç”Ÿæˆæ©Ÿèƒ½
+"""
+
+import os
+from pathlib import Path
+from openai import OpenAI
+from rich.console import Console
+from dotenv import load_dotenv
+import traceback
+import json
+
+# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
+load_dotenv()
+
+console = Console()
+
+
+def generate_ga_definitions(text_content: str, model: str, num_ga_pairs: int = None, max_context_length: int = 8000) -> str:
+    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã„ã€å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã®XMLã‚’ç”Ÿæˆã™ã‚‹"""
+    # LLMã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã™ãã‚‹ã¨ã‚³ã‚¹ãƒˆã‚„æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ãŸã‚ã€å…ˆé ­éƒ¨åˆ†ã«é™å®šã™ã‚‹
+    context = text_content[:max_context_length]
+    console.print(f"[dim]ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(context)} æ–‡å­— (ä¸Šé™: {max_context_length})[/dim]")
+
+    from ..prompts import get_ga_definition_generation_prompt
+    prompt_template = get_ga_definition_generation_prompt()
+    prompt = prompt_template.format(
+        context=context,
+        num_ga_pairs=num_ga_pairs if num_ga_pairs is not None else "3-5å€‹ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # APIã‚­ãƒ¼ã®ç¢ºèª
+    api_key = os.getenv("OPENAI_API_KEY", "")
+    if not api_key:
+        console.print("[bold red]OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
+        raise ValueError("OPENAI_API_KEYãŒå¿…è¦ã§ã™")
+
+    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
+    client = OpenAI(
+        base_url=os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
+        api_key=api_key,
+    )
+
+    try:
+        response = client.chat.completions.create(
+            model=model,
+            messages=messages
+        )
+        xml_content = response.choices[0].message.content
+        console.print(f"[dim]LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(xml_content)} æ–‡å­—[/dim]")
+        return xml_content
+    except Exception as error:
+        console.print(f"[bold red]GAå®šç¾©ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {error}")
+        raise
diff --git a/easy_dataset_cli/generators/qa_generator.py b/easy_dataset_cli/generators/qa_generator.py
new file mode 100644
index 0000000..99293ac
--- /dev/null
+++ b/easy_dataset_cli/generators/qa_generator.py
@@ -0,0 +1,547 @@
+#!/usr/bin/env python3
+"""
+åŸºæœ¬çš„ãªQ&Aç”Ÿæˆæ©Ÿèƒ½
+"""
+
+import os
+import xml.etree.ElementTree as ET
+from xml.dom import minidom
+from pathlib import Path
+from typing import List, Dict
+from openai import OpenAI
+from rich.console import Console
+from dotenv import load_dotenv
+import traceback
+import json
+from datetime import datetime
+
+from ..prompts import get_qa_generation_prompt
+from ..xml_utils import parse_qa_from_text_fallback
+
+# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
+load_dotenv()
+
+console = Console()
+
+
+def generate_qa_for_chunk_with_ga(
+    chunk: str,
+    model: str,
+    ga_pair: Dict[str, Dict[str, str]],
+    logs_dir: Path = None,
+    num_qa_pairs: int = None
+) -> List[Dict[str, str]]:
+    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_prompt()
+    prompt = prompt_template.format(
+        context=chunk,
+        genre_title=ga_pair['genre']['title'],
+        genre_description=ga_pair['genre']['description'],
+        audience_title=ga_pair['audience']['title'],
+        audience_description=ga_pair['audience']['description'],
+        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é€šå¸¸ã®XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€<Question>ã€<Answer>ã€<think>ã‚¿ã‚°ã¯ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
+    client = OpenAI(
+        base_url=os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
+        api_key=os.getenv("OPENAI_API_KEY"),
+    )
+
+    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+    audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+
+    try:
+        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir:
+            request_log = {
+                "timestamp": timestamp,
+                "model": model,
+                "genre": ga_pair['genre']['title'],
+                "audience": ga_pair['audience']['title'],
+                "prompt_length": len(prompt),
+                "messages": messages
+            }
+            request_filename = f"request_{genre_safe}_{audience_safe}_{timestamp}.json"
+            request_file_path = logs_dir / request_filename
+            with open(request_file_path, 'w', encoding='utf-8') as f:
+                json.dump(request_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜: {request_filename}[/dim]")
+
+            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
+            prompt_filename = f"prompt_{genre_safe}_{audience_safe}_{timestamp}.md"
+            prompt_file_path = logs_dir / prompt_filename
+            prompt_content = f"""# QAç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+
+**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** {timestamp}  
+**ãƒ¢ãƒ‡ãƒ«:** {model}  
+**ã‚¸ãƒ£ãƒ³ãƒ«:** {ga_pair['genre']['title']}  
+**ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:** {ga_pair['audience']['title']}  
+**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:** {len(prompt)} æ–‡å­—
+
+---
+
+## ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
+
+{messages[0]['content']}
+
+---
+
+## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+
+{prompt}
+"""
+            prompt_file_path.write_text(prompt_content, encoding='utf-8')
+            console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {prompt_filename}[/dim]")
+
+        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
+        request_start = datetime.now()
+        
+        response = client.chat.completions.create(
+            model=model,
+            messages=messages
+        )
+        
+        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ²
+        request_end = datetime.now()
+        processing_time = (request_end - request_start).total_seconds()
+        
+        xml_content = response.choices[0].message.content
+
+        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
+        if logs_dir:
+            response_log = {
+                "metadata": {
+                    "timestamp": timestamp,
+                    "request_start": request_start.isoformat(),
+                    "request_end": request_end.isoformat(),
+                    "processing_time_seconds": processing_time,
+                    "model": model
+                },
+                "generation_context": {
+                    "genre": {
+                        "title": ga_pair['genre']['title'],
+                        "description": ga_pair['genre']['description'][:100] + "..." if len(ga_pair['genre']['description']) > 100 else ga_pair['genre']['description']
+                    },
+                    "audience": {
+                        "title": ga_pair['audience']['title'],
+                        "description": ga_pair['audience']['description'][:100] + "..." if len(ga_pair['audience']['description']) > 100 else ga_pair['audience']['description']
+                    },
+                    "chunk_length": len(chunk),
+                    "prompt_length": len(prompt)
+                },
+                "api_response": {
+                    "response_length": len(xml_content),
+                    "response_content": xml_content
+                }
+            }
+            response_filename = f"response_{genre_safe}_{audience_safe}_{timestamp}.json"
+            response_file_path = logs_dir / response_filename
+            with open(response_file_path, 'w', encoding='utf-8') as f:
+                json.dump(response_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜: {response_filename} (å‡¦ç†æ™‚é–“: {processing_time:.2f}s)[/dim]")
+
+        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+        if logs_dir:
+            raw_filename = f"qa_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
+            raw_file_path = logs_dir / raw_filename
+            raw_file_path.write_text(xml_content, encoding="utf-8")
+
+        qa_pairs = _parse_qa_response(xml_content, logs_dir, genre_safe, audience_safe, timestamp)
+
+        # ç”Ÿæˆã—ãŸQAã‚’ä¿å­˜
+        if qa_pairs and logs_dir:
+            qa_filename = f"qa_pairs_{genre_safe}_{audience_safe}_{timestamp}.xml"
+            _save_qa_pairs_to_xml(qa_pairs, logs_dir, qa_filename)
+
+        # å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
+        if logs_dir:
+            _save_execution_summary(logs_dir, timestamp, genre_safe, audience_safe, {
+                "processing_time": processing_time,
+                "qa_count": len(qa_pairs),
+                "success": True,
+                "chunk_length": len(chunk),
+                "prompt_length": len(prompt),
+                "response_length": len(xml_content)
+            })
+
+        return qa_pairs
+
+    except Exception as general_error:
+        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
+        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
+        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(general_error).__name__}")
+        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(general_error)}")
+        console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
+        console.print(traceback.format_exc())
+        console.print(f"[dim]Genre: {ga_pair['genre']['title']}, Audience: {ga_pair['audience']['title']}[/dim]")
+
+        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir:
+            error_log = {
+                "timestamp": timestamp,
+                "model": model,
+                "genre": ga_pair['genre']['title'],
+                "audience": ga_pair['audience']['title'],
+                "error_type": type(general_error).__name__,
+                "error_message": str(general_error),
+                "traceback": traceback.format_exc()
+            }
+            error_filename = f"error_{genre_safe}_{audience_safe}_{timestamp}.json"
+            error_file_path = logs_dir / error_filename
+            with open(error_file_path, 'w', encoding='utf-8') as f:
+                json.dump(error_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {error_filename}[/dim]")
+
+        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
+        if logs_dir:
+            _save_execution_summary(logs_dir, timestamp, genre_safe, audience_safe, {
+                "processing_time": 0,
+                "qa_count": 0,
+                "success": False,
+                "error_type": type(general_error).__name__,
+                "error_message": str(general_error),
+                "chunk_length": len(chunk) if chunk else 0,
+                "prompt_length": len(prompt) if prompt else 0,
+                "response_length": 0
+            })
+
+        return []
+
+
+def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str = None, audience_safe: str = None, timestamp: str = None) -> List[Dict[str, str]]:
+    """Q&Aç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®XMLã‚’è§£æã™ã‚‹ï¼ˆå…±é€šå‡¦ç†ï¼‰"""
+    qa_pairs = []
+
+    # LLMã‹ã‚‰ã®å‡ºåŠ›ã®å‰å‡¦ç†ï¼šä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
+    cleaned_content = _clean_llm_response(xml_content)
+
+    # XMLéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º - å„ªå…ˆçš„ã«<QAPairs>ã‚¿ã‚°ã‚’æ¢ã™
+    xml_start = cleaned_content.find("<QAPairs>")
+    xml_end = cleaned_content.rfind("</QAPairs>")
+
+    # <QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œ
+    if xml_start == -1 or xml_end == -1:
+        console.print("[yellow]<QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œã—ã¾ã™...[/yellow]")
+        xml_start = cleaned_content.find("<Pair>")
+        xml_end = cleaned_content.rfind("</Pair>")
+
+        # <Pair>ã‚¿ã‚°ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æŠ½å‡º
+        if xml_start != -1 and xml_end != -1:
+            # ã™ã¹ã¦ã®<Pair>...</Pair>ã‚’æŠ½å‡º
+            import re
+            pair_pattern = r'<Pair>.*?</Pair>'
+            pair_matches = re.findall(pair_pattern, cleaned_content, re.DOTALL)
+
+            for pair_match in pair_matches:
+                # å„Pairã‹ã‚‰Questionã¨Answerã‚’æŠ½å‡º
+                question_match = re.search(r'<Question>(.*?)</Question>', pair_match, re.DOTALL)
+                answer_match = re.search(r'<Answer>(.*?)</Answer>', pair_match, re.DOTALL)
+
+                if question_match and answer_match:
+                    qa_pairs.append({
+                        "question": _decode_xml_entities(question_match.group(1).strip()),
+                        "answer": _decode_xml_entities(answer_match.group(1).strip())
+                    })
+
+            if qa_pairs:
+                console.print(f"[green]âœ“[/green] <Pair>ã‚¿ã‚°ã‹ã‚‰{len(qa_pairs)}ä»¶ã®Q&Aã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
+                return qa_pairs
+
+    if xml_start != -1 and xml_end != -1:
+        clean_xml = cleaned_content[xml_start: xml_end + len("</QAPairs>")]
+
+        # XMLè§£æç”¨ã®ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir and genre_safe and audience_safe and timestamp:
+            xml_debug_log = {
+                "timestamp": timestamp,
+                "original_xml_content": xml_content[:500],
+                "cleaned_xml_content": cleaned_content[:500],
+                "final_xml_content": clean_xml,
+                "xml_length": len(clean_xml)
+            }
+            xml_debug_filename = f"xml_debug_{genre_safe}_{audience_safe}_{timestamp}.json"
+            xml_debug_file_path = logs_dir / xml_debug_filename
+            with open(xml_debug_file_path, 'w', encoding='utf-8') as f:
+                json.dump(xml_debug_log, f, ensure_ascii=False, indent=2)
+
+        try:
+            root = ET.fromstring(clean_xml)
+
+            for pair_node in root.findall('Pair'):
+                question_node = pair_node.find('Question')
+                answer_node = pair_node.find('Answer')
+
+                if question_node is not None and answer_node is not None:
+                    question_text = question_node.text or ""
+
+                    # <Answer>è¦ç´ å†…ã®å†…å®¹ã‚’é©åˆ‡ã«å–å¾—
+                    if len(answer_node) > 0:
+                        # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼ˆ<think>ã‚¿ã‚°ãªã©ï¼‰
+                        answer_parts = []
+
+                        # Answerè¦ç´ ã®ç›´æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ<think>ã‚ˆã‚Šå‰ï¼‰
+                        if answer_node.text:
+                            answer_parts.append(answer_node.text.strip())
+
+                        # å„ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®tailï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
+                        for child in answer_node:
+                            if child.tag == 'think':
+                                # <think>ã‚¿ã‚°ã®å†…å®¹ã‚’å–å¾—
+                                think_content = child.text or ""
+                                answer_parts.append(f"<think>{think_content}</think>")
+
+                            # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
+                            if child.tail:
+                                answer_parts.append(child.tail.strip())
+
+                        answer_text = "".join(answer_parts)
+                    else:
+                        # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
+                        answer_text = answer_node.text or ""
+
+                    # XMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰
+                    question_text = _decode_xml_entities(question_text)
+                    answer_text = _decode_xml_entities(answer_text)
+
+                    qa_pairs.append({
+                        "question": question_text,
+                        "answer": answer_text
+                    })
+
+        except ET.ParseError as parse_error:
+            # XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æ
+            console.print("[yellow]XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€è‡ªå‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+            console.print(f"[dim]ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(parse_error)}[/dim]")
+
+            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
+            if logs_dir and genre_safe and audience_safe and timestamp:
+                parse_error_log = {
+                    "timestamp": timestamp,
+                    "error_type": "XML_ParseError",
+                    "error_message": str(parse_error),
+                    "xml_content": clean_xml
+                }
+                parse_error_filename = f"xml_parse_error_{genre_safe}_{audience_safe}_{timestamp}.json"
+                parse_error_file_path = logs_dir / parse_error_filename
+                with open(parse_error_file_path, 'w', encoding='utf-8') as f:
+                    json.dump(parse_error_log, f, ensure_ascii=False, indent=2)
+
+            # è‡ªå‹•è§£æã‚’è©¦è¡Œ
+            qa_pairs = parse_qa_from_text_fallback(clean_xml)
+
+            # è‡ªå‹•è§£æã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
+            if not qa_pairs:
+                console.print("[yellow]è‡ªå‹•è§£æã‚‚å¤±æ•—ã—ãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã—ã¾ã™...[/yellow]")
+                qa_pairs = _extract_qa_from_fallback_text(cleaned_content)
+
+    if not qa_pairs:
+        console.print(f"[bold red]LLMãŒç”Ÿæˆã—ãŸXMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
+        console.print(f"[dim]å—ä¿¡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ: {cleaned_content[:500]}...[/dim]")
+
+        # è§£æå¤±æ•—ã®ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir and genre_safe and audience_safe and timestamp:
+            failure_log = {
+                "timestamp": timestamp,
+                "failure_reason": "XMLè§£æå¤±æ•—",
+                "original_content": xml_content[:1000],
+                "cleaned_content": cleaned_content[:1000]
+            }
+            failure_filename = f"xml_parse_failure_{genre_safe}_{audience_safe}_{timestamp}.json"
+            failure_file_path = logs_dir / failure_filename
+            with open(failure_file_path, 'w', encoding='utf-8') as f:
+                json.dump(failure_log, f, ensure_ascii=False, indent=2)
+
+    return qa_pairs
+
+
+def _clean_llm_response(response: str) -> str:
+    """LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹"""
+    import re
+
+    # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
+    cleaned = response
+
+    # \```xml ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
+    cleaned = re.sub(r'\```xml\s*|\s*\```', '', cleaned, flags=re.IGNORECASE)
+
+    # \``` ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
+    cleaned = re.sub(r'\```\s*|\s*\```', '', cleaned)
+
+    # <xml> ... </xml> ã®ã‚ˆã†ãªã‚¿ã‚°ã‚’é™¤å»
+    cleaned = re.sub(r'<xml>\s*|\s*</xml>', '', cleaned, flags=re.IGNORECASE)
+
+    # ä¸è¦ãªç©ºç™½ã‚„æ”¹è¡Œã‚’æ•´ç†
+    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
+
+    return cleaned
+
+
+def _extract_qa_from_fallback_text(text: str) -> List[Dict[str, str]]:
+    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°"""
+    qa_pairs = []
+
+    try:
+        # <Question>ã¨<Answer>ã‚¿ã‚°ã§åˆ†å‰²
+        import re
+
+        # Questionã‚¿ã‚°ã‚’æ¤œç´¢
+        question_pattern = r'<Question>(.*?)</Question>'
+        answer_pattern = r'<Answer>(.*?)</Answer>'
+
+        questions = re.findall(question_pattern, text, re.DOTALL)
+        answers = re.findall(answer_pattern, text, re.DOTALL)
+
+        # åŒã˜æ•°ã®è³ªå•ã¨å›ç­”ãŒã‚ã‚‹å ´åˆã®ã¿ãƒšã‚¢ã‚’ä½œæˆ
+        min_count = min(len(questions), len(answers))
+        for i in range(min_count):
+            qa_pairs.append({
+                "question": _decode_xml_entities(questions[i].strip()),
+                "answer": _decode_xml_entities(answers[i].strip())
+            })
+
+    except Exception as e:
+        console.print(f"[red]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚‚å¤±æ•—:[/red] {e}")
+
+    return qa_pairs
+
+
+def _decode_xml_entities(text: str) -> str:
+    """XMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
+    import html
+    if text:
+        return html.unescape(text)
+    return text
+
+
+def _save_qa_pairs_to_xml(qa_pairs: List[Dict[str, str]], logs_dir: Path, qa_filename: str) -> None:
+    """Q&Aãƒšã‚¢ã‚’ãã‚Œã„ã«æ•´å½¢ã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ–¹å¼ï¼‰"""
+    if not qa_pairs or not logs_dir:
+        return
+
+    qa_file_path = logs_dir / qa_filename
+
+    # ElementTreeã§æ§‹é€ åŒ–ç”Ÿæˆ
+    root = ET.Element("QAPairs")
+    for qa in qa_pairs:
+        pair_elem = ET.SubElement(root, "Pair")
+        question_elem = ET.SubElement(pair_elem, "Question")
+        question_elem.text = qa["question"]
+
+        answer_elem = ET.SubElement(pair_elem, "Answer")
+
+        # å›ç­”å†…å®¹ã‚’è§£æ
+        parsed_answer = _parse_answer_with_think(qa["answer"])
+
+        if parsed_answer["has_think"]:
+            # <think>ã‚’ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
+            think_elem = ET.SubElement(answer_elem, "think")
+            think_elem.text = parsed_answer["think_content"]
+            think_elem.tail = parsed_answer["answer_content"]
+        else:
+            # é€šå¸¸ã®å›ç­”
+            answer_elem.text = parsed_answer["answer_content"]
+
+    # æ•´å½¢ã—ã¦ä¿å­˜
+    rough_string = ET.tostring(root, 'utf-8')
+    reparsed = minidom.parseString(rough_string)
+    pretty_xml = reparsed.toprettyxml(indent="  ")
+
+    qa_file_path.write_text(pretty_xml, encoding='utf-8')
+    console.print(f"[green]âœ“[/green] QAãƒšã‚¢ã‚’ä¿å­˜: {qa_filename} ({len(qa_pairs)}ä»¶)")
+
+
+def _parse_answer_with_think(answer_text: str) -> Dict[str, str]:
+    """<think>ã‚¿ã‚°ã‚’å«ã‚€å›ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦åˆ†é›¢"""
+    import re
+
+    # <think>...</think>ã‚¿ã‚°ã‚’æ¤œç´¢
+    think_match = re.search(r'<think>(.*?)</think>', answer_text, re.DOTALL)
+
+    if think_match:
+        think_content = think_match.group(1).strip()
+        # <think>ã‚¿ã‚°ä»¥é™ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
+        answer_content = answer_text[think_match.end():].strip()
+        return {
+            "has_think": True,
+            "think_content": think_content,
+            "answer_content": answer_content
+        }
+    else:
+        return {
+            "has_think": False,
+            "think_content": "",
+            "answer_content": answer_text
+        }
+
+
+def _save_execution_summary(logs_dir: Path, timestamp: str, genre_safe: str, audience_safe: str, summary_data: Dict) -> None:
+    """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¨JSONã§ä¿å­˜"""
+    if not logs_dir:
+        return
+    
+    # JSONã‚µãƒãƒªãƒ¼
+    json_summary = {
+        "timestamp": timestamp,
+        "genre": genre_safe,
+        "audience": audience_safe,
+        "execution_summary": summary_data
+    }
+    
+    json_filename = f"summary_{genre_safe}_{audience_safe}_{timestamp}.json"
+    json_file_path = logs_dir / json_filename
+    with open(json_file_path, 'w', encoding='utf-8') as f:
+        json.dump(json_summary, f, ensure_ascii=False, indent=2)
+    
+    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚µãƒãƒªãƒ¼
+    md_filename = f"summary_{genre_safe}_{audience_safe}_{timestamp}.md"
+    md_file_path = logs_dir / md_filename
+    
+    status_emoji = "âœ…" if summary_data.get("success", False) else "âŒ"
+    
+    md_content = f"""# QAç”Ÿæˆå®Ÿè¡Œã‚µãƒãƒªãƒ¼ {status_emoji}
+
+**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** {timestamp}  
+**ã‚¸ãƒ£ãƒ³ãƒ«:** {genre_safe.replace('_', ' ')}  
+**ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:** {audience_safe.replace('_', ' ')}  
+**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {'æˆåŠŸ' if summary_data.get('success', False) else 'å¤±æ•—'}
+
+## ğŸ“Š å®Ÿè¡Œçµ±è¨ˆ
+
+| é …ç›® | å€¤ |
+|------|-----|
+| å‡¦ç†æ™‚é–“ | {summary_data.get('processing_time', 0):.2f}ç§’ |
+| ç”Ÿæˆã•ã‚ŒãŸQAæ•° | {summary_data.get('qa_count', 0)}ä»¶ |
+| ãƒãƒ£ãƒ³ã‚¯é•· | {summary_data.get('chunk_length', 0):,}æ–‡å­— |
+| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•· | {summary_data.get('prompt_length', 0):,}æ–‡å­— |
+| ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•· | {summary_data.get('response_length', 0):,}æ–‡å­— |
+
+## ğŸ“ é–¢é€£ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
+
+- `prompt_{genre_safe}_{audience_safe}_{timestamp}.md` - ä½¿ç”¨ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+- `request_{genre_safe}_{audience_safe}_{timestamp}.json` - ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°  
+- `response_{genre_safe}_{audience_safe}_{timestamp}.json` - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°
+- `qa_raw_{genre_safe}_{audience_safe}_{timestamp}.md` - ç”ŸRAWãƒ¬ã‚¹ãƒãƒ³ã‚¹
+"""
+
+    if summary_data.get("success", False):
+        md_content += f"- `qa_pairs_{genre_safe}_{audience_safe}_{timestamp}.xml` - ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢\n"
+    else:
+        md_content += f"""
+## âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°
+
+**ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:** {summary_data.get('error_type', 'Unknown')}  
+**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:** {summary_data.get('error_message', 'No message')}
+
+- `error_{genre_safe}_{audience_safe}_{timestamp}.json` - ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
+"""
+    
+    md_file_path.write_text(md_content, encoding='utf-8')
+    console.print(f"[dim]å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜: {md_filename}[/dim]")
diff --git a/easy_dataset_cli/generators/qa_generator_fulltext.py b/easy_dataset_cli/generators/qa_generator_fulltext.py
new file mode 100644
index 0000000..ebee785
--- /dev/null
+++ b/easy_dataset_cli/generators/qa_generator_fulltext.py
@@ -0,0 +1,437 @@
+#!/usr/bin/env python3
+"""
+å…¨æ–‡å¯¾å¿œQ&Aç”Ÿæˆæ©Ÿèƒ½
+"""
+
+import os
+import xml.etree.ElementTree as ET
+from xml.dom import minidom
+from pathlib import Path
+from typing import List, Dict
+from openai import OpenAI
+from rich.console import Console
+from dotenv import load_dotenv
+import traceback
+import json
+from datetime import datetime
+
+from ..prompts import get_qa_generation_with_fulltext_prompt
+from ..xml_utils import parse_qa_from_text_fallback
+
+# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
+load_dotenv()
+
+console = Console()
+
+
+def generate_qa_for_chunk_with_ga_and_fulltext(
+    chunk: str,
+    full_text: str,
+    model: str,
+    ga_pair: Dict[str, Dict[str, str]],
+    logs_dir: Path = None,
+    num_qa_pairs: int = None
+) -> List[Dict[str, str]]:
+    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨å…¨æ–‡ã€1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_with_fulltext_prompt()
+    prompt = prompt_template.format(
+        chunk=chunk,
+        full_text=full_text,
+        genre_title=ga_pair['genre']['title'],
+        genre_description=ga_pair['genre']['description'],
+        audience_title=ga_pair['audience']['title'],
+        audience_description=ga_pair['audience']['description'],
+        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
+    )
+
+    messages = [
+        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é€šå¸¸ã®XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€<Question>ã€<Answer>ã€<think>ã‚¿ã‚°ã¯ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
+        {"role": "user", "content": prompt}
+    ]
+
+    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
+    client = OpenAI(
+        base_url=os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
+        api_key=os.getenv("OPENAI_API_KEY"),
+    )
+
+    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+    audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
+
+    try:
+        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir:
+            request_log = {
+                "timestamp": timestamp,
+                "model": model,
+                "genre": ga_pair['genre']['title'],
+                "audience": ga_pair['audience']['title'],
+                "prompt_length": len(prompt),
+                "messages": messages
+            }
+            request_filename = f"request_{genre_safe}_{audience_safe}_{timestamp}.json"
+            request_file_path = logs_dir / request_filename
+            with open(request_file_path, 'w', encoding='utf-8') as f:
+                json.dump(request_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜: {request_filename}[/dim]")
+
+            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
+            prompt_filename = f"prompt_fulltext_{genre_safe}_{audience_safe}_{timestamp}.md"
+            prompt_file_path = logs_dir / prompt_filename
+            prompt_content = f"""# QAç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å…¨æ–‡ä»˜ã)
+
+**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** {timestamp}  
+**ãƒ¢ãƒ‡ãƒ«:** {model}  
+**ã‚¸ãƒ£ãƒ³ãƒ«:** {ga_pair['genre']['title']}  
+**ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:** {ga_pair['audience']['title']}  
+**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:** {len(prompt)} æ–‡å­—
+
+---
+
+## ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
+
+{messages[0]['content']}
+
+---
+
+## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+
+{prompt}
+"""
+            prompt_file_path.write_text(prompt_content, encoding='utf-8')
+            console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {prompt_filename}[/dim]")
+
+        response = client.chat.completions.create(
+            model=model,
+            messages=messages
+        )
+        xml_content = response.choices[0].message.content
+
+        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir:
+            response_log = {
+                "timestamp": timestamp,
+                "model": model,
+                "genre": ga_pair['genre']['title'],
+                "audience": ga_pair['audience']['title'],
+                "response_length": len(xml_content),
+                "response_content": xml_content
+            }
+            response_filename = f"response_{genre_safe}_{audience_safe}_{timestamp}.json"
+            response_file_path = logs_dir / response_filename
+            with open(response_file_path, 'w', encoding='utf-8') as f:
+                json.dump(response_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜: {response_filename}[/dim]")
+
+        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
+        if logs_dir:
+            raw_filename = f"qa_fulltext_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
+            raw_file_path = logs_dir / raw_filename
+            raw_file_path.write_text(xml_content, encoding="utf-8")
+
+        qa_pairs = _parse_qa_response(xml_content, logs_dir, genre_safe, audience_safe, timestamp)
+
+        # ç”Ÿæˆã—ãŸQAã‚’ä¿å­˜
+        if qa_pairs and logs_dir:
+            qa_filename = f"qa_pairs_{genre_safe}_{audience_safe}_{timestamp}.xml"
+            qa_file_path = logs_dir / qa_filename
+
+            _save_qa_pairs_to_xml(qa_pairs, logs_dir, qa_filename)
+
+        return qa_pairs
+
+    except Exception as general_error:
+        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
+        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
+        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(general_error).__name__}")
+        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(general_error)}")
+        console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
+        console.print(traceback.format_exc())
+        console.print(f"[dim]Genre: {ga_pair['genre']['title']}, Audience: {ga_pair['audience']['title']}[/dim]")
+
+        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir:
+            error_log = {
+                "timestamp": timestamp,
+                "model": model,
+                "genre": ga_pair['genre']['title'],
+                "audience": ga_pair['audience']['title'],
+                "error_type": type(general_error).__name__,
+                "error_message": str(general_error),
+                "traceback": traceback.format_exc()
+            }
+            error_filename = f"error_{genre_safe}_{audience_safe}_{timestamp}.json"
+            error_file_path = logs_dir / error_filename
+            with open(error_file_path, 'w', encoding='utf-8') as f:
+                json.dump(error_log, f, ensure_ascii=False, indent=2)
+            console.print(f"[dim]ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {error_filename}[/dim]")
+
+        return []
+
+
+def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str = None, audience_safe: str = None, timestamp: str = None) -> List[Dict[str, str]]:
+    """Q&Aç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®XMLã‚’è§£æã™ã‚‹ï¼ˆå…±é€šå‡¦ç†ï¼‰"""
+    qa_pairs = []
+
+    # LLMã‹ã‚‰ã®å‡ºåŠ›ã®å‰å‡¦ç†ï¼šä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
+    cleaned_content = _clean_llm_response(xml_content)
+
+    # XMLéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º - å„ªå…ˆçš„ã«<QAPairs>ã‚¿ã‚°ã‚’æ¢ã™
+    xml_start = cleaned_content.find("<QAPairs>")
+    xml_end = cleaned_content.rfind("</QAPairs>")
+
+    # <QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œ
+    if xml_start == -1 or xml_end == -1:
+        console.print("[yellow]<QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œã—ã¾ã™...[/yellow]")
+        xml_start = cleaned_content.find("<Pair>")
+        xml_end = cleaned_content.rfind("</Pair>")
+
+        # <Pair>ã‚¿ã‚°ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æŠ½å‡º
+        if xml_start != -1 and xml_end != -1:
+            # ã™ã¹ã¦ã®<Pair>...</Pair>ã‚’æŠ½å‡º
+            import re
+            pair_pattern = r'<Pair>.*?</Pair>'
+            pair_matches = re.findall(pair_pattern, cleaned_content, re.DOTALL)
+
+            for pair_match in pair_matches:
+                # å„Pairã‹ã‚‰Questionã¨Answerã‚’æŠ½å‡º
+                question_match = re.search(r'<Question>(.*?)</Question>', pair_match, re.DOTALL)
+                answer_match = re.search(r'<Answer>(.*?)</Answer>', pair_match, re.DOTALL)
+
+                if question_match and answer_match:
+                    qa_pairs.append({
+                        "question": _decode_xml_entities(question_match.group(1).strip()),
+                        "answer": _decode_xml_entities(answer_match.group(1).strip())
+                    })
+
+            if qa_pairs:
+                console.print(f"[green]âœ“[/green] <Pair>ã‚¿ã‚°ã‹ã‚‰{len(qa_pairs)}ä»¶ã®Q&Aã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
+                return qa_pairs
+
+    if xml_start != -1 and xml_end != -1:
+        clean_xml = cleaned_content[xml_start: xml_end + len("</QAPairs>")]
+
+        # XMLè§£æç”¨ã®ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir and genre_safe and audience_safe and timestamp:
+            xml_debug_log = {
+                "timestamp": timestamp,
+                "original_xml_content": xml_content[:500],
+                "cleaned_xml_content": cleaned_content[:500],
+                "final_xml_content": clean_xml,
+                "xml_length": len(clean_xml)
+            }
+            xml_debug_filename = f"xml_debug_{genre_safe}_{audience_safe}_{timestamp}.json"
+            xml_debug_file_path = logs_dir / xml_debug_filename
+            with open(xml_debug_file_path, 'w', encoding='utf-8') as f:
+                json.dump(xml_debug_log, f, ensure_ascii=False, indent=2)
+
+        try:
+            root = ET.fromstring(clean_xml)
+
+            for pair_node in root.findall('Pair'):
+                question_node = pair_node.find('Question')
+                answer_node = pair_node.find('Answer')
+
+                if question_node is not None and answer_node is not None:
+                    question_text = question_node.text or ""
+
+                    # <Answer>è¦ç´ å†…ã®å†…å®¹ã‚’é©åˆ‡ã«å–å¾—
+                    if len(answer_node) > 0:
+                        # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼ˆ<think>ã‚¿ã‚°ãªã©ï¼‰
+                        answer_parts = []
+
+                        # Answerè¦ç´ ã®ç›´æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ<think>ã‚ˆã‚Šå‰ï¼‰
+                        if answer_node.text:
+                            answer_parts.append(answer_node.text.strip())
+
+                        # å„ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®tailï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
+                        for child in answer_node:
+                            if child.tag == 'think':
+                                # <think>ã‚¿ã‚°ã®å†…å®¹ã‚’å–å¾—
+                                think_content = child.text or ""
+                                answer_parts.append(f"<think>{think_content}</think>")
+
+                            # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
+                            if child.tail:
+                                answer_parts.append(child.tail.strip())
+
+                        answer_text = "".join(answer_parts)
+                    else:
+                        # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
+                        answer_text = answer_node.text or ""
+
+                    # XMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰
+                    question_text = _decode_xml_entities(question_text)
+                    answer_text = _decode_xml_entities(answer_text)
+
+                    qa_pairs.append({
+                        "question": question_text,
+                        "answer": answer_text
+                    })
+
+        except ET.ParseError as parse_error:
+            # XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æ
+            console.print("[yellow]XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€è‡ªå‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
+            console.print(f"[dim]ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(parse_error)}[/dim]")
+
+            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
+            if logs_dir and genre_safe and audience_safe and timestamp:
+                parse_error_log = {
+                    "timestamp": timestamp,
+                    "error_type": "XML_ParseError",
+                    "error_message": str(parse_error),
+                    "xml_content": clean_xml
+                }
+                parse_error_filename = f"xml_parse_error_{genre_safe}_{audience_safe}_{timestamp}.json"
+                parse_error_file_path = logs_dir / parse_error_filename
+                with open(parse_error_file_path, 'w', encoding='utf-8') as f:
+                    json.dump(parse_error_log, f, ensure_ascii=False, indent=2)
+
+            # è‡ªå‹•è§£æã‚’è©¦è¡Œ
+            qa_pairs = parse_qa_from_text_fallback(clean_xml)
+
+            # è‡ªå‹•è§£æã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
+            if not qa_pairs:
+                console.print("[yellow]è‡ªå‹•è§£æã‚‚å¤±æ•—ã—ãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã—ã¾ã™...[/yellow]")
+                qa_pairs = _extract_qa_from_fallback_text(cleaned_content)
+
+    if not qa_pairs:
+        console.print(f"[bold red]LLMãŒç”Ÿæˆã—ãŸXMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
+        console.print(f"[dim]å—ä¿¡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ: {cleaned_content[:500]}...[/dim]")
+
+        # è§£æå¤±æ•—ã®ãƒ­ã‚°ã‚’ä¿å­˜
+        if logs_dir and genre_safe and audience_safe and timestamp:
+            failure_log = {
+                "timestamp": timestamp,
+                "failure_reason": "XMLè§£æå¤±æ•—",
+                "original_content": xml_content[:1000],
+                "cleaned_content": cleaned_content[:1000]
+            }
+            failure_filename = f"xml_parse_failure_{genre_safe}_{audience_safe}_{timestamp}.json"
+            failure_file_path = logs_dir / failure_filename
+            with open(failure_file_path, 'w', encoding='utf-8') as f:
+                json.dump(failure_log, f, ensure_ascii=False, indent=2)
+
+    return qa_pairs
+
+
+def _clean_llm_response(response: str) -> str:
+    """LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹"""
+    import re
+
+    # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
+    cleaned = response
+
+    # \```xml ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
+    cleaned = re.sub(r'\```xml\s*|\s*\```', '', cleaned, flags=re.IGNORECASE)
+
+    # \``` ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
+    cleaned = re.sub(r'\```\s*|\s*\```', '', cleaned)
+
+    # <xml> ... </xml> ã®ã‚ˆã†ãªã‚¿ã‚°ã‚’é™¤å»
+    cleaned = re.sub(r'<xml>\s*|\s*</xml>', '', cleaned, flags=re.IGNORECASE)
+
+    # ä¸è¦ãªç©ºç™½ã‚„æ”¹è¡Œã‚’æ•´ç†
+    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
+
+    return cleaned
+
+
+def _extract_qa_from_fallback_text(text: str) -> List[Dict[str, str]]:
+    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°"""
+    qa_pairs = []
+
+    try:
+        # <Question>ã¨<Answer>ã‚¿ã‚°ã§åˆ†å‰²
+        import re
+
+        # Questionã‚¿ã‚°ã‚’æ¤œç´¢
+        question_pattern = r'<Question>(.*?)</Question>'
+        answer_pattern = r'<Answer>(.*?)</Answer>'
+
+        questions = re.findall(question_pattern, text, re.DOTALL)
+        answers = re.findall(answer_pattern, text, re.DOTALL)
+
+        # åŒã˜æ•°ã®è³ªå•ã¨å›ç­”ãŒã‚ã‚‹å ´åˆã®ã¿ãƒšã‚¢ã‚’ä½œæˆ
+        min_count = min(len(questions), len(answers))
+        for i in range(min_count):
+            qa_pairs.append({
+                "question": _decode_xml_entities(questions[i].strip()),
+                "answer": _decode_xml_entities(answers[i].strip())
+            })
+
+    except Exception as e:
+        console.print(f"[red]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚‚å¤±æ•—:[/red] {e}")
+
+    return qa_pairs
+
+
+def _decode_xml_entities(text: str) -> str:
+    """XMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
+    import html
+    if text:
+        return html.unescape(text)
+    return text
+
+
+def _save_qa_pairs_to_xml(qa_pairs: List[Dict[str, str]], logs_dir: Path, qa_filename: str) -> None:
+    """Q&Aãƒšã‚¢ã‚’ãã‚Œã„ã«æ•´å½¢ã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ–¹å¼ï¼‰"""
+    if not qa_pairs or not logs_dir:
+        return
+
+    qa_file_path = logs_dir / qa_filename
+
+    # ElementTreeã§æ§‹é€ åŒ–ç”Ÿæˆ
+    root = ET.Element("QAPairs")
+    for qa in qa_pairs:
+        pair_elem = ET.SubElement(root, "Pair")
+        question_elem = ET.SubElement(pair_elem, "Question")
+        question_elem.text = qa["question"]
+
+        answer_elem = ET.SubElement(pair_elem, "Answer")
+
+        # å›ç­”å†…å®¹ã‚’è§£æ
+        parsed_answer = _parse_answer_with_think(qa["answer"])
+
+        if parsed_answer["has_think"]:
+            # <think>ã‚’ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
+            think_elem = ET.SubElement(answer_elem, "think")
+            think_elem.text = parsed_answer["think_content"]
+            think_elem.tail = parsed_answer["answer_content"]
+        else:
+            # é€šå¸¸ã®å›ç­”
+            answer_elem.text = parsed_answer["answer_content"]
+
+    # æ•´å½¢ã—ã¦ä¿å­˜
+    rough_string = ET.tostring(root, 'utf-8')
+    reparsed = minidom.parseString(rough_string)
+    pretty_xml = reparsed.toprettyxml(indent="  ")
+
+    qa_file_path.write_text(pretty_xml, encoding='utf-8')
+    console.print(f"[green]âœ“[/green] QAãƒšã‚¢ã‚’ä¿å­˜: {qa_filename} ({len(qa_pairs)}ä»¶)")
+
+
+def _parse_answer_with_think(answer_text: str) -> Dict[str, str]:
+    """<think>ã‚¿ã‚°ã‚’å«ã‚€å›ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦åˆ†é›¢"""
+    import re
+
+    # <think>...</think>ã‚¿ã‚°ã‚’æ¤œç´¢
+    think_match = re.search(r'<think>(.*?)</think>', answer_text, re.DOTALL)
+
+    if think_match:
+        think_content = think_match.group(1).strip()
+        # <think>ã‚¿ã‚°ä»¥é™ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
+        answer_content = answer_text[think_match.end():].strip()
+        return {
+            "has_think": True,
+            "think_content": think_content,
+            "answer_content": answer_content
+        }
+    else:
+        return {
+            "has_think": False,
+            "think_content": "",
+            "answer_content": answer_text
+        }
diff --git a/easy_dataset_cli/qa_generator.py b/easy_dataset_cli/generators/qa_generator_thinking.py
similarity index 68%
rename from easy_dataset_cli/qa_generator.py
rename to easy_dataset_cli/generators/qa_generator_thinking.py
index 545d830..114797d 100644
--- a/easy_dataset_cli/qa_generator.py
+++ b/easy_dataset_cli/generators/qa_generator_thinking.py
@@ -1,25 +1,25 @@
-# easy_dataset_cli/qa_generator.py
-"""Q&Aç”Ÿæˆé–¢é€£æ©Ÿèƒ½"""
+#!/usr/bin/env python3
+"""
+æ€è€ƒãƒ•ãƒ­ãƒ¼å¯¾å¿œQ&Aç”Ÿæˆæ©Ÿèƒ½
+"""
 
 import os
 import xml.etree.ElementTree as ET
 from xml.dom import minidom
 from pathlib import Path
 from typing import List, Dict
-from litellm import completion
+from openai import OpenAI
 from rich.console import Console
 from dotenv import load_dotenv
 import traceback
 import json
 from datetime import datetime
 
-from .prompts import (
-    get_qa_generation_prompt,
-    get_qa_generation_with_fulltext_prompt,
+from ..prompts import (
     get_qa_generation_with_thinking_prompt,
-    get_ga_definition_generation_prompt
+    get_qa_generation_with_surrounding_prompt
 )
-from .xml_utils import parse_qa_from_text_fallback
+from ..xml_utils import parse_qa_from_text_fallback
 
 # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
 load_dotenv()
@@ -27,7 +27,7 @@ load_dotenv()
 console = Console()
 
 
-def generate_qa_for_chunk_with_ga_and_fulltext(
+def generate_qa_for_chunk_with_ga_and_thinking(
     chunk: str,
     full_text: str,
     model: str,
@@ -35,8 +35,8 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
     logs_dir: Path = None,
     num_qa_pairs: int = None
 ) -> List[Dict[str, str]]:
-    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨å…¨æ–‡ã€1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
-    prompt_template = get_qa_generation_with_fulltext_prompt()
+    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨å…¨æ–‡ã€1ã¤ã®GAãƒšã‚¢ã‹ã‚‰æ€è€ƒãƒ•ãƒ­ãƒ¼ä»˜ãQ&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_with_thinking_prompt()
     prompt = prompt_template.format(
         chunk=chunk,
         full_text=full_text,
@@ -52,14 +52,17 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
         {"role": "user", "content": prompt}
     ]
 
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
+    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
+    client = OpenAI(
+        base_url=os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
+        api_key=os.getenv("OPENAI_API_KEY"),
+    )
 
     # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
     genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
     audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-    
+
     try:
         # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir:
@@ -71,13 +74,42 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
                 "prompt_length": len(prompt),
                 "messages": messages
             }
-            request_filename = f"request_{genre_safe}_{audience_safe}_{timestamp}.json"
+            request_filename = f"request_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
             request_file_path = logs_dir / request_filename
             with open(request_file_path, 'w', encoding='utf-8') as f:
                 json.dump(request_log, f, ensure_ascii=False, indent=2)
             console.print(f"[dim]ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜: {request_filename}[/dim]")
 
-        response = completion(model=model, messages=messages)
+            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
+            prompt_filename = f"prompt_thinking_{genre_safe}_{audience_safe}_{timestamp}.md"
+            prompt_file_path = logs_dir / prompt_filename
+            prompt_content = f"""# QAç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (æ€è€ƒãƒ•ãƒ­ãƒ¼ä»˜ã)
+
+**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** {timestamp}  
+**ãƒ¢ãƒ‡ãƒ«:** {model}  
+**ã‚¸ãƒ£ãƒ³ãƒ«:** {ga_pair['genre']['title']}  
+**ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:** {ga_pair['audience']['title']}  
+**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:** {len(prompt)} æ–‡å­—
+
+---
+
+## ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
+
+{messages[0]['content']}
+
+---
+
+## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+
+{prompt}
+"""
+            prompt_file_path.write_text(prompt_content, encoding='utf-8')
+            console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {prompt_filename}[/dim]")
+
+        response = client.chat.completions.create(
+            model=model,
+            messages=messages
+        )
         xml_content = response.choices[0].message.content
 
         # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜
@@ -90,7 +122,7 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
                 "response_length": len(xml_content),
                 "response_content": xml_content
             }
-            response_filename = f"response_{genre_safe}_{audience_safe}_{timestamp}.json"
+            response_filename = f"response_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
             response_file_path = logs_dir / response_filename
             with open(response_file_path, 'w', encoding='utf-8') as f:
                 json.dump(response_log, f, ensure_ascii=False, indent=2)
@@ -98,30 +130,28 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
 
         # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
         if logs_dir:
-            raw_filename = f"qa_fulltext_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
+            raw_filename = f"qa_thinking_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
             raw_file_path = logs_dir / raw_filename
             raw_file_path.write_text(xml_content, encoding="utf-8")
 
         qa_pairs = _parse_qa_response(xml_content, logs_dir, genre_safe, audience_safe, timestamp)
-        
+
         # ç”Ÿæˆã—ãŸQAã‚’ä¿å­˜
         if qa_pairs and logs_dir:
-            qa_filename = f"qa_pairs_{genre_safe}_{audience_safe}_{timestamp}.xml"
-            qa_file_path = logs_dir / qa_filename
-            
+            qa_filename = f"qa_pairs_thinking_{genre_safe}_{audience_safe}_{timestamp}.xml"
             _save_qa_pairs_to_xml(qa_pairs, logs_dir, qa_filename)
 
         return qa_pairs
 
     except Exception as general_error:
         # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
-        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
+        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®æ€è€ƒãƒ•ãƒ­ãƒ¼ä»˜ãQ&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
         console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(general_error).__name__}")
         console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(general_error)}")
         console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
         console.print(traceback.format_exc())
         console.print(f"[dim]Genre: {ga_pair['genre']['title']}, Audience: {ga_pair['audience']['title']}[/dim]")
-        
+
         # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir:
             error_log = {
@@ -133,26 +163,26 @@ def generate_qa_for_chunk_with_ga_and_fulltext(
                 "error_message": str(general_error),
                 "traceback": traceback.format_exc()
             }
-            error_filename = f"error_{genre_safe}_{audience_safe}_{timestamp}.json"
+            error_filename = f"error_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
             error_file_path = logs_dir / error_filename
             with open(error_file_path, 'w', encoding='utf-8') as f:
                 json.dump(error_log, f, ensure_ascii=False, indent=2)
             console.print(f"[dim]ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {error_filename}[/dim]")
-        
+
         return []
 
 
-def generate_qa_for_chunk_with_ga(
-    chunk: str,
+def generate_qa_for_chunk_with_surrounding_context(
+    content: str,
     model: str,
     ga_pair: Dict[str, Dict[str, str]],
     logs_dir: Path = None,
     num_qa_pairs: int = None
 ) -> List[Dict[str, str]]:
-    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨1ã¤ã®GAãƒšã‚¢ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
-    prompt_template = get_qa_generation_prompt()
+    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã„ã€å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
+    prompt_template = get_qa_generation_with_surrounding_prompt()
     prompt = prompt_template.format(
-        context=chunk,
+        content=content,
         genre_title=ga_pair['genre']['title'],
         genre_description=ga_pair['genre']['description'],
         audience_title=ga_pair['audience']['title'],
@@ -165,14 +195,17 @@ def generate_qa_for_chunk_with_ga(
         {"role": "user", "content": prompt}
     ]
 
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
+    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
+    client = OpenAI(
+        base_url=os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1"),
+        api_key=os.getenv("OPENAI_API_KEY"),
+    )
 
     # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
     genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
     audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-    
+
     try:
         # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir:
@@ -184,13 +217,42 @@ def generate_qa_for_chunk_with_ga(
                 "prompt_length": len(prompt),
                 "messages": messages
             }
-            request_filename = f"request_{genre_safe}_{audience_safe}_{timestamp}.json"
+            request_filename = f"request_surrounding_{genre_safe}_{audience_safe}_{timestamp}.json"
             request_file_path = logs_dir / request_filename
             with open(request_file_path, 'w', encoding='utf-8') as f:
                 json.dump(request_log, f, ensure_ascii=False, indent=2)
             console.print(f"[dim]ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜: {request_filename}[/dim]")
 
-        response = completion(model=model, messages=messages)
+            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
+            prompt_filename = f"prompt_surrounding_{genre_safe}_{audience_safe}_{timestamp}.md"
+            prompt_file_path = logs_dir / prompt_filename
+            prompt_content = f"""# QAç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ)
+
+**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** {timestamp}  
+**ãƒ¢ãƒ‡ãƒ«:** {model}  
+**ã‚¸ãƒ£ãƒ³ãƒ«:** {ga_pair['genre']['title']}  
+**ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:** {ga_pair['audience']['title']}  
+**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:** {len(prompt)} æ–‡å­—
+
+---
+
+## ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
+
+{messages[0]['content']}
+
+---
+
+## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
+
+{prompt}
+"""
+            prompt_file_path.write_text(prompt_content, encoding='utf-8')
+            console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {prompt_filename}[/dim]")
+
+        response = client.chat.completions.create(
+            model=model,
+            messages=messages
+        )
         xml_content = response.choices[0].message.content
 
         # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜
@@ -203,7 +265,7 @@ def generate_qa_for_chunk_with_ga(
                 "response_length": len(xml_content),
                 "response_content": xml_content
             }
-            response_filename = f"response_{genre_safe}_{audience_safe}_{timestamp}.json"
+            response_filename = f"response_surrounding_{genre_safe}_{audience_safe}_{timestamp}.json"
             response_file_path = logs_dir / response_filename
             with open(response_file_path, 'w', encoding='utf-8') as f:
                 json.dump(response_log, f, ensure_ascii=False, indent=2)
@@ -211,30 +273,28 @@ def generate_qa_for_chunk_with_ga(
 
         # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
         if logs_dir:
-            raw_filename = f"qa_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
+            raw_filename = f"qa_surrounding_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
             raw_file_path = logs_dir / raw_filename
             raw_file_path.write_text(xml_content, encoding="utf-8")
 
         qa_pairs = _parse_qa_response(xml_content, logs_dir, genre_safe, audience_safe, timestamp)
-        
+
         # ç”Ÿæˆã—ãŸQAã‚’ä¿å­˜
         if qa_pairs and logs_dir:
-            qa_filename = f"qa_pairs_{genre_safe}_{audience_safe}_{timestamp}.xml"
-            qa_file_path = logs_dir / qa_filename
-            
+            qa_filename = f"qa_pairs_surrounding_{genre_safe}_{audience_safe}_{timestamp}.xml"
             _save_qa_pairs_to_xml(qa_pairs, logs_dir, qa_filename)
 
         return qa_pairs
 
     except Exception as general_error:
         # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
-        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
+        console.print(f"[bold red]å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã®Q&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
         console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(general_error).__name__}")
         console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(general_error)}")
         console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
         console.print(traceback.format_exc())
         console.print(f"[dim]Genre: {ga_pair['genre']['title']}, Audience: {ga_pair['audience']['title']}[/dim]")
-        
+
         # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir:
             error_log = {
@@ -246,58 +306,13 @@ def generate_qa_for_chunk_with_ga(
                 "error_message": str(general_error),
                 "traceback": traceback.format_exc()
             }
-            error_filename = f"error_{genre_safe}_{audience_safe}_{timestamp}.json"
+            error_filename = f"error_surrounding_{genre_safe}_{audience_safe}_{timestamp}.json"
             error_file_path = logs_dir / error_filename
             with open(error_file_path, 'w', encoding='utf-8') as f:
                 json.dump(error_log, f, ensure_ascii=False, indent=2)
             console.print(f"[dim]ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {error_filename}[/dim]")
-        
-        return []
-
-
-def generate_ga_definitions(text_content: str, model: str, num_ga_pairs: int = None) -> str:
-    """litellmã‚’ä½¿ã„ã€å…ƒã®æ–‡ç« ã‹ã‚‰GAãƒšã‚¢å®šç¾©ã®XMLã‚’ç”Ÿæˆã™ã‚‹"""
-    # LLMã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã™ãã‚‹ã¨ã‚³ã‚¹ãƒˆã‚„æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ãŸã‚ã€å…ˆé ­éƒ¨åˆ†ã«é™å®šã™ã‚‹
-    context = text_content[:8000]
-    console.print(f"[dim]ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(context)} æ–‡å­—[/dim]")
-
-    prompt_template = get_ga_definition_generation_prompt()
-    prompt = prompt_template.format(
-        context=context,
-        num_ga_pairs=num_ga_pairs if num_ga_pairs is not None else "3-5å€‹ã®"
-    )
-
-    messages = [
-        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
-        {"role": "user", "content": prompt}
-    ]
-
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    api_key = os.getenv("OPENROUTER_API_KEY", "")
-    if not api_key:
-        console.print("[bold red]OPENROUTER_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
-        raise ValueError("OPENROUTER_API_KEYãŒå¿…è¦ã§ã™")
 
-    os.environ["OPENROUTER_API_KEY"] = api_key
-
-    # OpenRouterã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
-    if "openrouter" not in model and not model.startswith("openrouter/"):
-        if model.startswith("gpt-"):
-            model = f"openrouter/openai/{model}"
-        elif model.startswith("claude-"):
-            model = f"openrouter/anthropic/{model}"
-        else:
-            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§openrouterãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
-            model = f"openrouter/{model}"
-
-    try:
-        response = completion(model=model, messages=messages)
-        xml_content = response.choices[0].message.content
-        console.print(f"[dim]LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(xml_content)} æ–‡å­—[/dim]")
-        return xml_content
-    except Exception as error:
-        console.print(f"[bold red]GAå®šç¾©ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {error}")
-        raise
+        return []
 
 
 def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str = None, audience_safe: str = None, timestamp: str = None) -> List[Dict[str, str]]:
@@ -306,42 +321,42 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
 
     # LLMã‹ã‚‰ã®å‡ºåŠ›ã®å‰å‡¦ç†ï¼šä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
     cleaned_content = _clean_llm_response(xml_content)
-    
+
     # XMLéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º - å„ªå…ˆçš„ã«<QAPairs>ã‚¿ã‚°ã‚’æ¢ã™
     xml_start = cleaned_content.find("<QAPairs>")
     xml_end = cleaned_content.rfind("</QAPairs>")
-    
+
     # <QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œ
     if xml_start == -1 or xml_end == -1:
         console.print("[yellow]<QAPairs>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€<Pair>ã‚¿ã‚°ã§æŠ½å‡ºã‚’è©¦è¡Œã—ã¾ã™...[/yellow]")
         xml_start = cleaned_content.find("<Pair>")
         xml_end = cleaned_content.rfind("</Pair>")
-        
+
         # <Pair>ã‚¿ã‚°ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æŠ½å‡º
         if xml_start != -1 and xml_end != -1:
             # ã™ã¹ã¦ã®<Pair>...</Pair>ã‚’æŠ½å‡º
             import re
             pair_pattern = r'<Pair>.*?</Pair>'
             pair_matches = re.findall(pair_pattern, cleaned_content, re.DOTALL)
-            
+
             for pair_match in pair_matches:
                 # å„Pairã‹ã‚‰Questionã¨Answerã‚’æŠ½å‡º
                 question_match = re.search(r'<Question>(.*?)</Question>', pair_match, re.DOTALL)
                 answer_match = re.search(r'<Answer>(.*?)</Answer>', pair_match, re.DOTALL)
-                
+
                 if question_match and answer_match:
                     qa_pairs.append({
                         "question": _decode_xml_entities(question_match.group(1).strip()),
                         "answer": _decode_xml_entities(answer_match.group(1).strip())
                     })
-            
+
             if qa_pairs:
                 console.print(f"[green]âœ“[/green] <Pair>ã‚¿ã‚°ã‹ã‚‰{len(qa_pairs)}ä»¶ã®Q&Aã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                 return qa_pairs
 
     if xml_start != -1 and xml_end != -1:
         clean_xml = cleaned_content[xml_start: xml_end + len("</QAPairs>")]
-        
+
         # XMLè§£æç”¨ã®ãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir and genre_safe and audience_safe and timestamp:
             xml_debug_log = {
@@ -365,36 +380,36 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
 
                 if question_node is not None and answer_node is not None:
                     question_text = question_node.text or ""
-                    
+
                     # <Answer>è¦ç´ å†…ã®å†…å®¹ã‚’é©åˆ‡ã«å–å¾—
                     if len(answer_node) > 0:
                         # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼ˆ<think>ã‚¿ã‚°ãªã©ï¼‰
                         answer_parts = []
-                        
+
                         # Answerè¦ç´ ã®ç›´æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ<think>ã‚ˆã‚Šå‰ï¼‰
                         if answer_node.text:
                             answer_parts.append(answer_node.text.strip())
-                        
+
                         # å„ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®tailï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                         for child in answer_node:
                             if child.tag == 'think':
                                 # <think>ã‚¿ã‚°ã®å†…å®¹ã‚’å–å¾—
                                 think_content = child.text or ""
                                 answer_parts.append(f"<think>{think_content}</think>")
-                            
+
                             # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
                             if child.tail:
                                 answer_parts.append(child.tail.strip())
-                        
+
                         answer_text = "".join(answer_parts)
                     else:
                         # ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
                         answer_text = answer_node.text or ""
-                    
+
                     # XMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰
                     question_text = _decode_xml_entities(question_text)
                     answer_text = _decode_xml_entities(answer_text)
-                    
+
                     qa_pairs.append({
                         "question": question_text,
                         "answer": answer_text
@@ -404,7 +419,7 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
             # XMLãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€æ‰‹å‹•ã§ãƒ†ã‚­ã‚¹ãƒˆè§£æ
             console.print("[yellow]XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€è‡ªå‹•è§£æã‚’è©¦è¡Œä¸­...[/yellow]")
             console.print(f"[dim]ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(parse_error)}[/dim]")
-            
+
             # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
             if logs_dir and genre_safe and audience_safe and timestamp:
                 parse_error_log = {
@@ -417,10 +432,10 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
                 parse_error_file_path = logs_dir / parse_error_filename
                 with open(parse_error_file_path, 'w', encoding='utf-8') as f:
                     json.dump(parse_error_log, f, ensure_ascii=False, indent=2)
-            
+
             # è‡ªå‹•è§£æã‚’è©¦è¡Œ
             qa_pairs = parse_qa_from_text_fallback(clean_xml)
-            
+
             # è‡ªå‹•è§£æã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
             if not qa_pairs:
                 console.print("[yellow]è‡ªå‹•è§£æã‚‚å¤±æ•—ã—ãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã—ã¾ã™...[/yellow]")
@@ -429,7 +444,7 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
     if not qa_pairs:
         console.print(f"[bold red]LLMãŒç”Ÿæˆã—ãŸXMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
         console.print(f"[dim]å—ä¿¡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ: {cleaned_content[:500]}...[/dim]")
-        
+
         # è§£æå¤±æ•—ã®ãƒ­ã‚°ã‚’ä¿å­˜
         if logs_dir and genre_safe and audience_safe and timestamp:
             failure_log = {
@@ -449,40 +464,40 @@ def _parse_qa_response(xml_content: str, logs_dir: Path = None, genre_safe: str
 def _clean_llm_response(response: str) -> str:
     """LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹"""
     import re
-    
+
     # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
     cleaned = response
-    
+
     # \```xml ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
     cleaned = re.sub(r'\```xml\s*|\s*\```', '', cleaned, flags=re.IGNORECASE)
-    
+
     # \``` ... \``` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
     cleaned = re.sub(r'\```\s*|\s*\```', '', cleaned)
-    
+
     # <xml> ... </xml> ã®ã‚ˆã†ãªã‚¿ã‚°ã‚’é™¤å»
     cleaned = re.sub(r'<xml>\s*|\s*</xml>', '', cleaned, flags=re.IGNORECASE)
-    
+
     # ä¸è¦ãªç©ºç™½ã‚„æ”¹è¡Œã‚’æ•´ç†
     cleaned = re.sub(r'\s+', ' ', cleaned).strip()
-    
+
     return cleaned
 
 
 def _extract_qa_from_fallback_text(text: str) -> List[Dict[str, str]]:
     """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥Q&Aã‚’æŠ½å‡ºã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°"""
     qa_pairs = []
-    
+
     try:
         # <Question>ã¨<Answer>ã‚¿ã‚°ã§åˆ†å‰²
         import re
-        
+
         # Questionã‚¿ã‚°ã‚’æ¤œç´¢
         question_pattern = r'<Question>(.*?)</Question>'
         answer_pattern = r'<Answer>(.*?)</Answer>'
-        
+
         questions = re.findall(question_pattern, text, re.DOTALL)
         answers = re.findall(answer_pattern, text, re.DOTALL)
-        
+
         # åŒã˜æ•°ã®è³ªå•ã¨å›ç­”ãŒã‚ã‚‹å ´åˆã®ã¿ãƒšã‚¢ã‚’ä½œæˆ
         min_count = min(len(questions), len(answers))
         for i in range(min_count):
@@ -490,10 +505,10 @@ def _extract_qa_from_fallback_text(text: str) -> List[Dict[str, str]]:
                 "question": _decode_xml_entities(questions[i].strip()),
                 "answer": _decode_xml_entities(answers[i].strip())
             })
-            
+
     except Exception as e:
         console.print(f"[red]ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æã‚‚å¤±æ•—:[/red] {e}")
-    
+
     return qa_pairs
 
 
@@ -505,49 +520,25 @@ def _decode_xml_entities(text: str) -> str:
     return text
 
 
-def _parse_answer_with_think(answer_text: str) -> Dict[str, str]:
-    """<think>ã‚¿ã‚°ã‚’å«ã‚€å›ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦åˆ†é›¢"""
-    import re
-    
-    # <think>...</think>ã‚¿ã‚°ã‚’æ¤œç´¢
-    think_match = re.search(r'<think>(.*?)</think>', answer_text, re.DOTALL)
-    
-    if think_match:
-        think_content = think_match.group(1).strip()
-        # <think>ã‚¿ã‚°ä»¥é™ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
-        answer_content = answer_text[think_match.end():].strip()
-        return {
-            "has_think": True,
-            "think_content": think_content,
-            "answer_content": answer_content
-        }
-    else:
-        return {
-            "has_think": False,
-            "think_content": "",
-            "answer_content": answer_text
-        }
-
-
 def _save_qa_pairs_to_xml(qa_pairs: List[Dict[str, str]], logs_dir: Path, qa_filename: str) -> None:
     """Q&Aãƒšã‚¢ã‚’ãã‚Œã„ã«æ•´å½¢ã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ–¹å¼ï¼‰"""
     if not qa_pairs or not logs_dir:
         return
-        
+
     qa_file_path = logs_dir / qa_filename
-    
+
     # ElementTreeã§æ§‹é€ åŒ–ç”Ÿæˆ
     root = ET.Element("QAPairs")
     for qa in qa_pairs:
         pair_elem = ET.SubElement(root, "Pair")
         question_elem = ET.SubElement(pair_elem, "Question")
         question_elem.text = qa["question"]
-        
+
         answer_elem = ET.SubElement(pair_elem, "Answer")
-        
+
         # å›ç­”å†…å®¹ã‚’è§£æ
         parsed_answer = _parse_answer_with_think(qa["answer"])
-        
+
         if parsed_answer["has_think"]:
             # <think>ã‚’ã‚µãƒ–ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
             think_elem = ET.SubElement(answer_elem, "think")
@@ -556,126 +547,35 @@ def _save_qa_pairs_to_xml(qa_pairs: List[Dict[str, str]], logs_dir: Path, qa_fil
         else:
             # é€šå¸¸ã®å›ç­”
             answer_elem.text = parsed_answer["answer_content"]
-    
+
     # æ•´å½¢ã—ã¦ä¿å­˜
     rough_string = ET.tostring(root, 'utf-8')
     reparsed = minidom.parseString(rough_string)
     pretty_xml = reparsed.toprettyxml(indent="  ")
-    
+
     qa_file_path.write_text(pretty_xml, encoding='utf-8')
     console.print(f"[green]âœ“[/green] QAãƒšã‚¢ã‚’ä¿å­˜: {qa_filename} ({len(qa_pairs)}ä»¶)")
 
 
-def generate_qa_for_chunk_with_ga_and_thinking(
-    chunk: str,
-    full_text: str,
-    model: str,
-    ga_pair: Dict[str, Dict[str, str]],
-    logs_dir: Path = None,
-    num_qa_pairs: int = None
-) -> List[Dict[str, str]]:
-    """litellmã‚’ä½¿ã„ã€1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨å…¨æ–‡ã€1ã¤ã®GAãƒšã‚¢ã‹ã‚‰æ€è€ƒãƒ•ãƒ­ãƒ¼ä»˜ãQ&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
-    prompt_template = get_qa_generation_with_thinking_prompt()
-    prompt = prompt_template.format(
-        chunk=chunk,
-        full_text=full_text,
-        genre_title=ga_pair['genre']['title'],
-        genre_description=ga_pair['genre']['description'],
-        audience_title=ga_pair['audience']['title'],
-        audience_description=ga_pair['audience']['description'],
-        num_qa_pairs=num_qa_pairs if num_qa_pairs is not None else "è¤‡æ•°ã®"
-    )
-
-    messages = [
-        {"role": "system", "content": "ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é€šå¸¸ã®XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, \", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€<Question>ã€<Answer>ã€<think>ã‚¿ã‚°ã¯ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
-        {"role": "user", "content": prompt}
-    ]
-
-    # OpenRouterç”¨ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
-    os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY", "")
-
-    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
-    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-    genre_safe = "".join(c for c in ga_pair['genre']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-    audience_safe = "".join(c for c in ga_pair['audience']['title'] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
-    
-    try:
-        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜
-        if logs_dir:
-            request_log = {
-                "timestamp": timestamp,
-                "model": model,
-                "genre": ga_pair['genre']['title'],
-                "audience": ga_pair['audience']['title'],
-                "prompt_length": len(prompt),
-                "messages": messages
-            }
-            request_filename = f"request_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
-            request_file_path = logs_dir / request_filename
-            with open(request_file_path, 'w', encoding='utf-8') as f:
-                json.dump(request_log, f, ensure_ascii=False, indent=2)
-            console.print(f"[dim]ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’ä¿å­˜: {request_filename}[/dim]")
-
-        response = completion(model=model, messages=messages)
-        xml_content = response.choices[0].message.content
-
-        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜
-        if logs_dir:
-            response_log = {
-                "timestamp": timestamp,
-                "model": model,
-                "genre": ga_pair['genre']['title'],
-                "audience": ga_pair['audience']['title'],
-                "response_length": len(xml_content),
-                "response_content": xml_content
-            }
-            response_filename = f"response_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
-            response_file_path = logs_dir / response_filename
-            with open(response_file_path, 'w', encoding='utf-8') as f:
-                json.dump(response_log, f, ensure_ascii=False, indent=2)
-            console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿å­˜: {response_filename}[/dim]")
-
-        # rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
-        if logs_dir:
-            raw_filename = f"qa_thinking_raw_{genre_safe}_{audience_safe}_{timestamp}.md"
-            raw_file_path = logs_dir / raw_filename
-            raw_file_path.write_text(xml_content, encoding="utf-8")
-
-        qa_pairs = _parse_qa_response(xml_content, logs_dir, genre_safe, audience_safe, timestamp)
-
-        # ç”Ÿæˆã—ãŸQAã‚’ä¿å­˜
-        if qa_pairs and logs_dir:
-            qa_filename = f"qa_pairs_thinking_{genre_safe}_{audience_safe}_{timestamp}.xml"
-            _save_qa_pairs_to_xml(qa_pairs, logs_dir, qa_filename)
-
-        return qa_pairs
-
-    except Exception as general_error:
-        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
-        console.print(f"[bold red]ãƒãƒ£ãƒ³ã‚¯ã¨GAãƒšã‚¢ã‹ã‚‰ã®æ€è€ƒãƒ•ãƒ­ãƒ¼ä»˜ãQ&Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(general_error).__name__}")
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(general_error)}")
-        console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
-        console.print(traceback.format_exc())
-        console.print(f"[dim]Genre: {ga_pair['genre']['title']}, Audience: {ga_pair['audience']['title']}[/dim]")
-        
-        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
-        if logs_dir:
-            error_log = {
-                "timestamp": timestamp,
-                "model": model,
-                "genre": ga_pair['genre']['title'],
-                "audience": ga_pair['audience']['title'],
-                "error_type": type(general_error).__name__,
-                "error_message": str(general_error),
-                "traceback": traceback.format_exc()
-            }
-            error_filename = f"error_thinking_{genre_safe}_{audience_safe}_{timestamp}.json"
-            error_file_path = logs_dir / error_filename
-            with open(error_file_path, 'w', encoding='utf-8') as f:
-                json.dump(error_log, f, ensure_ascii=False, indent=2)
-            console.print(f"[dim]ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {error_filename}[/dim]")
-        
-        return []
+def _parse_answer_with_think(answer_text: str) -> Dict[str, str]:
+    """<think>ã‚¿ã‚°ã‚’å«ã‚€å›ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦åˆ†é›¢"""
+    import re
 
+    # <think>...</think>ã‚¿ã‚°ã‚’æ¤œç´¢
+    think_match = re.search(r'<think>(.*?)</think>', answer_text, re.DOTALL)
 
+    if think_match:
+        think_content = think_match.group(1).strip()
+        # <think>ã‚¿ã‚°ä»¥é™ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
+        answer_content = answer_text[think_match.end():].strip()
+        return {
+            "has_think": True,
+            "think_content": think_content,
+            "answer_content": answer_content
+        }
+    else:
+        return {
+            "has_think": False,
+            "think_content": "",
+            "answer_content": answer_text
+        }
diff --git a/easy_dataset_cli/main.py b/easy_dataset_cli/main.py
index 9285adf..0f42e47 100644
--- a/easy_dataset_cli/main.py
+++ b/easy_dataset_cli/main.py
@@ -1,428 +1,20 @@
-# easy_dataset_cli/main.py
-"""CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
+#!/usr/bin/env python3
+"""
+Easy Dataset CLI - ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
+"""
 
-from pathlib import Path
-from typing_extensions import Annotated
-import typer
-from rich.console import Console
-from rich.progress import Progress
 from dotenv import load_dotenv
 
-from .core import (
-    split_text,
-    parse_ga_file,
-    generate_qa_for_chunk_with_ga,
-    generate_qa_for_chunk_with_ga_and_fulltext,
-    generate_qa_for_chunk_with_ga_and_thinking,
-    convert_to_xml_by_genre,
-    generate_ga_definitions,
-    parse_ga_definitions_from_xml,
-    save_ga_definitions_by_genre,
-    create_output_directories,
-    sanitize_filename,
-    convert_all_xml_to_alpaca,
-    upload_to_huggingface,
-    create_dataset_card
-)
-
 # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
 load_dotenv()
 
-app = typer.Typer(
-    help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªCLIãƒ„ãƒ¼ãƒ«ã€‚",
-    context_settings={"help_option_names": ["-h", "--help"]}
-)
-console = Console()
-
-
-@app.command()
-def create_ga(
-    file_path: Annotated[Path, typer.Argument(
-        exists=True, dir_okay=False, readable=True,
-        help="GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€‚"
-    )],
-    output_dir: Annotated[Path, typer.Option(
-        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
-        help="ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚"
-    )],
-    model: Annotated[str, typer.Option(
-        "--model", "-m",
-        help="GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
-    )] = "openrouter/openai/gpt-oss-120b",
-    num_ga_pairs: Annotated[int, typer.Option(
-        "--num-ga-pairs", "-g",
-        help="ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
-    )] = 5,
-):
-    """å…ƒã®æ–‡ç« ã‚’åˆ†æã—ã€GAãƒšã‚¢å®šç¾©ã‚’XMLå½¢å¼ã§ç”Ÿæˆã—ã€Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
-    console.print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: [cyan]{file_path}[/cyan]")
-
-    try:
-        text = file_path.read_text(encoding="utf-8")
-        console.print(f"[dim]èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text)} æ–‡å­—[/dim]")
-
-        console.print("[bold green]LLMã«æœ€é©ãªGAãƒšã‚¢ã‚’ææ¡ˆã•ã›ã¦ã„ã¾ã™...[/bold green]")
-        xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs)
-
-        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
-        dirs = create_output_directories(output_dir)
-        console.print(f"[dim]å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: ga/, logs/, qa/[/dim]")
-        
-        # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
-        raw_file_path = dirs["logs"] / "raw.md"
-        raw_file_path.write_text(xml_content, encoding="utf-8")
-        console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: [cyan]{raw_file_path}[/cyan]")
-
-        console.print("[bold green]XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æã—ã¦ã„ã¾ã™...[/bold green]")
-        # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æ
-        ga_pairs = parse_ga_definitions_from_xml(xml_content)
-        
-        if not ga_pairs:
-            console.print("[bold red]æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚[/bold red]")
-            console.print("[yellow]ç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:[/yellow]")
-            console.print(xml_content)
-            raise typer.Exit(code=1)
-
-        # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
-        xml_file_path = dirs["ga"] / "ga_definitions.xml"
-        # XMLã‚¿ã‚°éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
-        xml_start = xml_content.find("<GADefinitions>")
-        xml_end = xml_content.rfind("</GADefinitions>")
-        if xml_start != -1 and xml_end != -1:
-            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
-            xml_file_path.write_text(clean_xml, encoding="utf-8")
-            console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: [cyan]{xml_file_path}[/cyan]")
-
-        # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
-        save_ga_definitions_by_genre(ga_pairs, dirs["ga"])
-
-        console.print(
-            f"\n[bold green]âœ“[/bold green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ "
-            f"[cyan]{dirs['ga']}[/cyan] ã«ä¿å­˜ã—ã¾ã—ãŸã€‚"
-        )
-        console.print(
-            "[yellow]ãƒ’ãƒ³ãƒˆ: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€å¿…è¦ã«å¿œã˜ã¦ç·¨é›†ã—ã¦ã‹ã‚‰ "
-            "`generate` ã‚³ãƒãƒ³ãƒ‰ã§ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚[/yellow]"
-        )
-
-    except Exception as e:
-        console.print(
-            f"[bold red]GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {e}"
-        )
-        raise typer.Exit(code=1)
-
-
-@app.command()
-def generate(
-    file_path: Annotated[Path, typer.Argument(
-        exists=True, dir_okay=False, readable=True,
-        help="å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚"
-    )],
-    ga_file: Annotated[Path, typer.Option(
-        "--ga-file", "-g", exists=True, dir_okay=False, readable=True,
-        help="Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLã¾ãŸã¯Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ga_definitions.xmlã‚’æ¨å¥¨ã€‚"
-    )],
-    output_dir: Annotated[Path, typer.Option(
-        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
-        help="ç”Ÿæˆã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚"
-    )] = None,
-    model: Annotated[str, typer.Option(
-        "--model", "-m",
-        help="Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
-    )] = "openrouter/openai/gpt-oss-120b",
-    chunk_size: Annotated[int, typer.Option(
-        help="ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚ºã€‚"
-    )] = 2000,
-    chunk_overlap: Annotated[int, typer.Option(
-        help="ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚"
-    )] = 200,
-    num_qa_pairs: Annotated[int, typer.Option(
-        "--num-qa-pairs", "-q",
-        help="å„ãƒãƒ£ãƒ³ã‚¯ãƒ»GAãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
-    )] = 10,
-    use_fulltext: Annotated[bool, typer.Option(
-        "--use-fulltext", "-f",
-        help="å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
-    )] = False,
-    use_thinking: Annotated[bool, typer.Option(
-        "--use-thinking", "-T",
-        help="å„Q&Aãƒšã‚¢ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã—ã¾ã™ã€‚ã‚ˆã‚Šæ·±ã„ç†è§£ã¨èª¬æ˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
-    )] = False,
-    append_mode: Annotated[bool, typer.Option(
-        "--append", "-A",
-        help="æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„Q&Aã‚’è¿½åŠ ã—ã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä¸Šæ›¸ãã—ã¾ã™ã€‚"
-    )] = False,
-    export_alpaca: Annotated[bool, typer.Option(
-        "--export-alpaca", "-a",
-        help="ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"
-    )] = False,
-    upload_hf: Annotated[bool, typer.Option(
-        "--upload-hf", "-u",
-        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
-    )] = False,
-    hf_repo_name: Annotated[str, typer.Option(
-        "--hf-repo-name", "-r",
-        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
-    )] = "",
-    hf_token: Annotated[str, typer.Option(
-        "--hf-token", "-t",
-        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
-    )] = "",
-    hf_private: Annotated[bool, typer.Option(
-        "--hf-private",
-        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
-    )] = False,
-):
-    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
-    
-    --use-fulltextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†æ™‚ã«å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
-    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ãŸã ã—ã€å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
-    """
-    try:
-        console.print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: [cyan]{file_path}[/cyan]")
-        text = file_path.read_text(encoding="utf-8")
-
-        console.print(f"GAãƒšã‚¢ã‚’è§£æã—ã¦ã„ã¾ã™: [cyan]{ga_file}[/cyan]")
-        ga_pairs = parse_ga_file(ga_file)
-
-        if not ga_pairs:
-            console.print("[bold red]æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚[/bold red]")
-            raise typer.Exit(code=1)
-
-        console.print(f"[green]{len(ga_pairs)}[/green] å€‹ã®GAãƒšã‚¢ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚")
-
-        console.print("ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã„ã¾ã™...")
-        chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
-        console.print(f"[green]{len(chunks)}[/green] å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
-
-        all_qa_pairs_with_ga = []
-        total_tasks = len(chunks) * len(ga_pairs)
-        
-        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯æ§‹é€ ã‚’ä½œæˆ
-        dirs = None
-        if output_dir:
-            dirs = create_output_directories(output_dir)
-            console.print(f"[dim]å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: ga/, logs/, qa/[/dim]")
-
-        # å…¨æ–‡ä½¿ç”¨ã®å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤º
-        if use_fulltext:
-            console.print("[yellow]âš  å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚[/yellow]")
-            console.print(f"[dim]å…¨æ–‡é•·: {len(text)} æ–‡å­—[/dim]")
-
-        # æ€è€ƒãƒ•ãƒ­ãƒ¼ä½¿ç”¨ã®å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤º
-        if use_thinking:
-            console.print("[yellow]âš  æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚å„Q&Aã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚[/yellow]")
-
-        with Progress(console=console) as progress:
-            task = progress.add_task("[green]Q&Aãƒšã‚¢ã‚’ç”Ÿæˆä¸­...", total=total_tasks)
-
-            for chunk in chunks:
-                for ga_pair in ga_pairs:
-                    if use_thinking:
-                        qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
-                            chunk=chunk,
-                            full_text=text if use_fulltext else "",
-                            model=model,
-                            ga_pair=ga_pair,
-                            logs_dir=dirs["logs"] if dirs else None,
-                            num_qa_pairs=num_qa_pairs
-                        )
-                    elif use_fulltext:
-                        qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
-                            chunk=chunk,
-                            full_text=text,
-                            model=model,
-                            ga_pair=ga_pair,
-                            logs_dir=dirs["logs"] if dirs else None,
-                            num_qa_pairs=num_qa_pairs
-                        )
-                    else:
-                        qa_pairs = generate_qa_for_chunk_with_ga(
-                            chunk, model=model, ga_pair=ga_pair,
-                            logs_dir=dirs["logs"] if dirs else None,
-                            num_qa_pairs=num_qa_pairs
-                        )
-
-                    for pair in qa_pairs:
-                        qa_entry = {
-                            "genre": ga_pair['genre']['title'],
-                            "audience": ga_pair['audience']['title'],
-                            "question": pair['question'],
-                            "answer": pair['answer'],  # <think>...</think>å›ç­”...å½¢å¼ãŒãã®ã¾ã¾å…¥ã‚‹
-                        }
-                        all_qa_pairs_with_ga.append(qa_entry)
-
-                    progress.update(
-                        task, advance=1,
-                        description=f"Genre: {ga_pair['genre']['title']}"
-                    )
-
-        console.print(
-            f"\nåˆè¨ˆ [bold green]{len(all_qa_pairs_with_ga)}[/bold green] "
-            "å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
-        )
-
-        xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"] if dirs else None, append_mode)
-
-        if dirs:
-            console.print(f"XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ [cyan]{dirs['qa']}[/cyan] ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
-
-            for genre, xml_content in xml_outputs_by_genre.items():
-                safe_genre_name = sanitize_filename(genre)
-                output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
-                output_file_path.write_text(xml_content, encoding="utf-8")
-                console.print(f"  - [green]âœ“[/green] {output_file_path.name}")
-
-            console.print("\n[bold green]ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚[/bold green]")
-            
-            # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
-            if export_alpaca:
-                console.print("\n[bold blue]Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...[/bold blue]")
-                alpaca_file = dirs["base"] / "dataset_alpaca.json"
-                alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
-                
-                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
-                readme_file = dirs["base"] / "README.md"
-                create_dataset_card(alpaca_data, readme_file, "Generated QA Dataset")
-                
-                # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
-                if upload_hf:
-                    if not hf_repo_name:
-                        console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
-                        console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
-                    else:
-                        console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
-                        success = upload_to_huggingface(
-                            dataset_data=alpaca_data,
-                            repo_name=hf_repo_name,
-                            hf_token=hf_token if hf_token else None,
-                            private=hf_private,
-                            commit_message=f"Upload QA dataset with {len(alpaca_data)} entries",
-                            readme_file=readme_file
-                        )
-                        if not success:
-                            console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
-        else:
-            console.print("\n--- ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ (Genreåˆ¥XML) ---")
-            for genre, xml_content in xml_outputs_by_genre.items():
-                console.print(f"\n[bold yellow]## Genre: {genre} ##[/bold yellow]")
-                console.print(xml_content, overflow="fold")
-    
-    except Exception as e:
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red]")
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:[/bold red] {type(e).__name__}")
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:[/bold red] {str(e)}")
-        console.print(f"[bold red]ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:[/bold red]")
-        import traceback
-        console.print(traceback.format_exc())
-        raise typer.Exit(code=1)
-
-
-@app.command()
-def convert_to_alpaca(
-    qa_dir: Annotated[Path, typer.Argument(
-        exists=True, dir_okay=True, readable=True,
-        help="XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹qaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
-    )],
-    output_file: Annotated[Path, typer.Option(
-        "--output-file", "-o", file_okay=True, dir_okay=False,
-        help="å‡ºåŠ›ã™ã‚‹Alpacaå½¢å¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚"
-    )] = None,
-    upload_hf: Annotated[bool, typer.Option(
-        "--upload-hf", "-u",
-        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
-    )] = False,
-    hf_repo_name: Annotated[str, typer.Option(
-        "--hf-repo-name", "-r",
-        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
-    )] = "",
-    hf_token: Annotated[str, typer.Option(
-        "--hf-token", "-t",
-        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
-    )] = "",
-    hf_private: Annotated[bool, typer.Option(
-        "--hf-private",
-        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
-    )] = False,
-):
-    """æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã®JSONã«å¤‰æ›ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
-    
-    try:
-        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
-        if output_file is None:
-            output_file = qa_dir.parent / "dataset_alpaca.json"
-        
-        console.print(f"XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›ä¸­: [cyan]{qa_dir}[/cyan]")
-        
-        # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã«å¤‰æ›
-        alpaca_data = convert_all_xml_to_alpaca(qa_dir, output_file)
-        
-        if not alpaca_data:
-            console.print("[bold red]å¤‰æ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚[/bold red]")
-            raise typer.Exit(code=1)
-        
-        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
-        readme_file = output_file.parent / "README.md"
-        create_dataset_card(alpaca_data, readme_file, "Converted QA Dataset")
-        
-        # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
-        if upload_hf:
-            if not hf_repo_name:
-                console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
-                console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
-                raise typer.Exit(code=1)
-            
-            console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
-            success = upload_to_huggingface(
-                dataset_data=alpaca_data,
-                repo_name=hf_repo_name,
-                hf_token=hf_token if hf_token else None,
-                private=hf_private,
-                commit_message=f"Upload converted QA dataset with {len(alpaca_data)} entries",
-                readme_file=readme_file
-            )
-            
-            if not success:
-                console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
-                raise typer.Exit(code=1)
-        
-        console.print(f"\n[bold green]âœ“[/bold green] å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
-        
-    except Exception as e:
-        console.print(f"[bold red]å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {e}")
-        raise typer.Exit(code=1)
-
-
-@app.command()
-def aggregate_logs(
-    output_dir: Annotated[Path, typer.Argument(
-        exists=True, dir_okay=True, readable=True,
-        help="logsãƒ•ã‚©ãƒ«ãƒ€ãŒå«ã¾ã‚Œã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
-    )]
-):
-    """logsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ã—ã¦qaãƒ•ã‚©ãƒ«ãƒ€ã®XMLã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
-    
-    try:
-        logs_dir = output_dir / "logs"
-        qa_dir = output_dir / "qa"
-        
-        if not logs_dir.exists():
-            console.print(f"[bold red]logsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {logs_dir}[/bold red]")
-            raise typer.Exit(code=1)
-        
-        console.print(f"logsãƒ•ã‚©ãƒ«ãƒ€: [cyan]{logs_dir}[/cyan]")
-        console.print(f"å‡ºåŠ›å…ˆqaãƒ•ã‚©ãƒ«ãƒ€: [cyan]{qa_dir}[/cyan]")
-        
-        # XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ã—ã¦qaãƒ•ã‚©ãƒ«ãƒ€ã«ç”Ÿæˆ
-        from easy_dataset_cli.core import aggregate_logs_xml_to_qa
-        aggregate_logs_xml_to_qa(logs_dir, qa_dir)
-        
-        console.print(f"\n[bold green]âœ“[/bold green] é›†ç´„ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
-        
-    except Exception as e:
-        console.print(f"[bold red]ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:[/bold red] {e}")
-        raise typer.Exit(code=1)
+# commandsã‹ã‚‰ã‚¢ãƒ—ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
+from .commands import app, print_logo
 
+def main():
+    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆé–¢æ•°"""
+    print_logo()
+    app()
 
 if __name__ == "__main__":
-    app()
+    main()
diff --git a/easy_dataset_cli/prompts.py b/easy_dataset_cli/prompts.py
index c2b00b8..0237879 100644
--- a/easy_dataset_cli/prompts.py
+++ b/easy_dataset_cli/prompts.py
@@ -7,7 +7,15 @@ from pathlib import Path
 def load_prompt_template(template_name: str) -> str:
     """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€"""
     prompt_dir = Path(__file__).parent / "prompts"
-    template_path = prompt_dir / f"{template_name}.md"
+    
+    # GAç³»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹QAç³»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚’åˆ¤å®š
+    if template_name.startswith("ga_"):
+        template_path = prompt_dir / "ga" / f"{template_name}.md"
+    elif template_name.startswith("qa_"):
+        template_path = prompt_dir / "qa" / f"{template_name}.md"
+    else:
+        # å¾“æ¥ã®å½¢å¼ã‚‚ä¿æŒ
+        template_path = prompt_dir / f"{template_name}.md"
     
     if not template_path.exists():
         raise FileNotFoundError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {template_path}")
@@ -33,3 +41,8 @@ def get_ga_definition_generation_prompt() -> str:
 def get_qa_generation_with_thinking_prompt() -> str:
     """æ€è€ƒãƒ•ãƒ­ãƒ¼å¯¾å¿œQ&Aç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
     return load_prompt_template("qa_generation_with_thinking")
+
+
+def get_qa_generation_with_surrounding_prompt() -> str:
+    """å‘¨è¾ºãƒãƒ£ãƒ³ã‚¯ä»˜Q&Aç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
+    return load_prompt_template("qa_generation_with_surrounding")
diff --git a/easy_dataset_cli/prompts/ga/ga_definition_generation.md b/easy_dataset_cli/prompts/ga/ga_definition_generation.md
new file mode 100644
index 0000000..43779f1
--- /dev/null
+++ b/easy_dataset_cli/prompts/ga/ga_definition_generation.md
@@ -0,0 +1,51 @@
+# å½¹å‰²: GAï¼ˆGenre-Audienceï¼‰ãƒšã‚¢å®šç¾©ã®å°‚é–€å®¶
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã®å†…å®¹ã‚’åˆ†æã—ã€æœ€é©ãªGenreï¼ˆä½“è£ï¼‰ã¨Audienceï¼ˆèª­è€…ï¼‰ã®ãƒšã‚¢ã‚’ææ¡ˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
+
+ã“ã®ãƒ„ãƒ¼ãƒ«ã¯**easy-dataset-cli**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚
+- GitHub: https://github.com/Sunwood-ai-labsII/easy-dataset-cli.git
+- ç”¨é€”: é«˜å“è³ªãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
+- ä¸»è¦æ©Ÿèƒ½: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è‡ªå‹•Q&Aãƒšã‚¢ç”Ÿæˆã€ä½“è£ãƒ»èª­è€…ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
+
+## æŒ‡ç¤º:
+1. ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã®å†…å®¹ã€ãƒˆãƒ”ãƒƒã‚¯ã€å°‚é–€æ€§ãƒ¬ãƒ™ãƒ«ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
+2. ã“ã®æ–‡ç« ã‹ã‚‰è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹éš›ã«æœ€é©ã¨ãªã‚‹{num_ga_pairs}å€‹ã®Genre-Audienceãƒšã‚¢ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
+3. å„Genreã¯ç•°ãªã‚‹æ–‡ä½“ãƒ»å½¢å¼ï¼ˆå­¦è¡“è«–æ–‡ã€æŠ€è¡“ãƒ–ãƒ­ã‚°ã€æ•™ç§‘æ›¸ã€FAQã€å¯¾è©±å½¢å¼ãªã©ï¼‰ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
+4. å„Audienceã¯ç•°ãªã‚‹çŸ¥è­˜ãƒ¬ãƒ™ãƒ«ãƒ»ç«‹å ´ï¼ˆåˆå¿ƒè€…ã€å­¦ç”Ÿã€å°‚é–€å®¶ã€å®Ÿå‹™è€…ãªã©ï¼‰ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
+5. æ–‡ç« ã®å†…å®¹ã«é©ã—ãŸãƒšã‚¢ã‚’é¸æŠã—ã€å¤šæ§˜æ€§ãƒ»å¤šè§’æ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚
+
+## æ–‡ç« :
+---
+{context}
+---
+
+## å‡ºåŠ›å½¢å¼:
+**çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ï¼š**
+- å‡ºåŠ›ã¯ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<GADefinitions>` ã§å§‹ã¾ã‚‹ç´”ç²‹ãªXMLã®ã¿ã«ã—ã¦ãã ã•ã„
+- XMLå®£è¨€ `<?xml version="1.0" encoding="utf-8"?>` ã‚’å…ˆé ­ã«è¿½åŠ ã—ã¦ãã ã•ã„
+- ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚¿ã‚°ã‚’å«ã‚ã€é–‰ã˜ã‚¿ã‚°ã¾ã§å®Œå…¨ã«å«ã‚ã¦ãã ã•ã„
+- ç©ºç™½ã€æ”¹è¡Œã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯è¨˜å·ï¼ˆ\```ã‚„\```xmlï¼‰ã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„
+- èª¬æ˜æ–‡ã€ã‚³ãƒ¡ãƒ³ãƒˆã€è¿½åŠ ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„
+
+**XMLæ§‹é€ ï¼š**
+å„GAãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Genre>` ã¨ `<Audience>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+- `<Genre>` ã‚¿ã‚°å†…ã«ã¯ `<Title>` ã¨ `<Description>` ã‚’å«ã‚€
+- `<Audience>` ã‚¿ã‚°å†…ã«ã¯ `<Title>` ã¨ `<Description>` ã‚’å«ã‚€
+
+**å®Ÿæ–½ä¾‹ï¼ˆã“ã®æ§‹é€ ã‚’å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ï¼‰ï¼š**
+<GADefinitions>
+<Pair>
+<Genre>
+<Title>FAQ</Title>
+<Description>ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ†ã‚¹ãƒˆã«é–¢ã™ã‚‹ç‰¹å®šã®è³ªå•ã«ç´ æ—©ãã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ãªå½¢å¼ã§ã€ã‚ˆãã‚ã‚‹è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ãŸã‚‚ã®ã€‚</Description>
+</Genre>
+<Audience>
+<Title>åˆå¿ƒè€…</Title>
+<Description>ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚’åˆã‚ã¦æ‰±ã†äººã€…ã€‚åŸºæœ¬çš„ãªæ¦‚å¿µã‚„æ‰‹é †ã‚’æ‰‹è»½ã«å­¦ã³ãŸã„äººãŸã¡ã€‚</Description>
+</Audience>
+</Pair>
+</GADefinitions>
+
+ãã‚Œã§ã¯ã€æœ€é©ãªGAå®šç¾©ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
diff --git a/easy_dataset_cli/prompts/ga_definition_generation.md b/easy_dataset_cli/prompts/ga_definition_generation.md
deleted file mode 100644
index 17c05a4..0000000
--- a/easy_dataset_cli/prompts/ga_definition_generation.md
+++ /dev/null
@@ -1,47 +0,0 @@
-# å½¹å‰²: GAï¼ˆGenre-Audienceï¼‰ãƒšã‚¢å®šç¾©ã®å°‚é–€å®¶
-
-ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã®å†…å®¹ã‚’åˆ†æã—ã€æœ€é©ãªGenreï¼ˆä½“è£ï¼‰ã¨Audienceï¼ˆèª­è€…ï¼‰ã®ãƒšã‚¢ã‚’ææ¡ˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
-
-## æŒ‡ç¤º:
-1. ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã®å†…å®¹ã€ãƒˆãƒ”ãƒƒã‚¯ã€å°‚é–€æ€§ãƒ¬ãƒ™ãƒ«ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
-2. ã“ã®æ–‡ç« ã‹ã‚‰è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹éš›ã«æœ€é©ã¨ãªã‚‹{num_ga_pairs}å€‹ã®Genre-Audienceãƒšã‚¢ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
-3. å„Genreã¯ç•°ãªã‚‹æ–‡ä½“ãƒ»å½¢å¼ï¼ˆå­¦è¡“è«–æ–‡ã€æŠ€è¡“ãƒ–ãƒ­ã‚°ã€æ•™ç§‘æ›¸ã€FAQã€å¯¾è©±å½¢å¼ãªã©ï¼‰ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
-4. å„Audienceã¯ç•°ãªã‚‹çŸ¥è­˜ãƒ¬ãƒ™ãƒ«ãƒ»ç«‹å ´ï¼ˆåˆå¿ƒè€…ã€å­¦ç”Ÿã€å°‚é–€å®¶ã€å®Ÿå‹™è€…ãªã©ï¼‰ã‚’è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚
-5. æ–‡ç« ã®å†…å®¹ã«é©ã—ãŸãƒšã‚¢ã‚’é¸æŠã—ã€å¤šæ§˜æ€§ãƒ»å¤šè§’æ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚
-
-## æ–‡ç« :
----
-{context}
----
-
-## å‡ºåŠ›å½¢å¼:
-**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<GADefinitions>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
-å„GAãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Genre>` ã¨ `<Audience>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
-
-## å‡ºåŠ›ä¾‹:
-\```xml
-<GADefinitions>
-<Pair>
-<Genre>
-<Title>FAQ</Title>
-<Description>ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚²ãƒ¼ãƒ ã«é–¢ã™ã‚‹ç‰¹å®šã®è³ªå•ã«ç´ æ—©ãã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ãªå½¢å¼ã§ã€ã‚ˆãã‚ã‚‹è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã€‚</Description>
-</Genre>
-<Audience>
-<Title>åˆå¿ƒè€…ã‚²ãƒ¼ãƒãƒ¼</Title>
-<Description>æ±æ–¹Projectã‚„å¼¾å¹•ç³»ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚²ãƒ¼ãƒ ã‚’åˆã‚ã¦ãƒ—ãƒ¬ã‚¤ã™ã‚‹äººã€…ã€‚ã‚²ãƒ¼ãƒ ã®åŸºæœ¬çš„ãªæƒ…å ±ã‚„æ”»ç•¥ã®ãƒ’ãƒ³ãƒˆãŒæ¬²ã—ã„ã€‚</Description>
-</Audience>
-</Pair>
-<Pair>
-<Genre>
-<Title>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¬ã‚¤ãƒ‰</Title>
-<Description>ã‚²ãƒ¼ãƒ ã‚·ã‚¹ãƒ†ãƒ ã€å¿…è¦ç’°å¢ƒã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ãªã©ã®æŠ€è¡“çš„ãªè©³ç´°ã‚’èª¬æ˜ã™ã‚‹å½¢å¼ã€‚ç‰¹ã«æŠ€è¡“çš„ãªè©³ç´°ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã€‚</Description>
-</Genre>
-<Audience>
-<Title>PCã‚²ãƒ¼ãƒŸãƒ³ã‚°æ„›å¥½è€…</Title>
-<Description>PCã§ã®ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤ã«æ…£ã‚Œã¦ã„ã‚‹ãŒã€ç‰¹ã«æ±æ–¹ã‚·ãƒªãƒ¼ã‚ºã«é–¢ã™ã‚‹æŠ€è¡“çš„ãªè©³ç´°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹æ„›å¥½è€…ã€‚</Description>
-</Audience>
-</Pair>
-</GADefinitions>
-\```
-
-ãã‚Œã§ã¯ã€æœ€é©ãªGAå®šç¾©ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
diff --git a/easy_dataset_cli/prompts/qa/qa_generation.md b/easy_dataset_cli/prompts/qa/qa_generation.md
new file mode 100644
index 0000000..a74fc0f
--- /dev/null
+++ b/easy_dataset_cli/prompts/qa/qa_generation.md
@@ -0,0 +1,60 @@
+# å½¹å‰²: Q&Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ï¼ˆåŸºæœ¬ç‰ˆï¼‰
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰é«˜å“è³ªãªè³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹ã«ã€æŒ‡å®šã•ã‚ŒãŸã€Œä½“è£ã€ã¨ã€Œèª­è€…ã€ã«åˆã‚ã›ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã«é•·ã‘ã¦ã„ã¾ã™ã€‚
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
+
+ã“ã®ãƒ„ãƒ¼ãƒ«ã¯**easy-dataset-cli**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚
+- GitHub: https://github.com/Sunwood-ai-labsII/easy-dataset-cli.git
+- ç”¨é€”: é«˜å“è³ªãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
+- ä¸»è¦æ©Ÿèƒ½: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è‡ªå‹•Q&Aãƒšã‚¢ç”Ÿæˆã€ä½“è£ãƒ»èª­è€…ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
+
+## æŒ‡ç¤º:
+1. ä¸ãˆã‚‰ã‚ŒãŸã€Œæ–‡ç« ã€ã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ãã ã•ã„ã€‚
+2. æŒ‡å®šã•ã‚ŒãŸã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã¨ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®å½¹å‰²ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
+3. æ–‡ç« ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹æƒ…å ±**ã®ã¿**ã«åŸºã¥ã„ã¦ã€{num_qa_pairs}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§æ´å¯Ÿã«å¯Œã‚“ã Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+4. è³ªå•ã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨è¤‡é›‘ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®è¦–ç‚¹ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+5. å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ¼ãƒ³ã€è©³ç´°ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+
+## QAãƒšã‚¢ä½œæˆã«ãŠã‘ã‚‹é‡è¦ãªãƒ«ãƒ¼ãƒ«:
+- **ä»£åè©ã®ä½¿ç”¨ç¦æ­¢**: è³ªå•ã¨å›ç­”ã§ã¯ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œãã‚Œã€ã€Œã“ã‚Œã€ã€Œãã®äººã€ã€Œãã®ä¼šç¤¾ã€ãªã©ã®ä»£åè©ã‚’ä½¿ç”¨ã›ãšã€å…·ä½“çš„ãªåè©ã‚„å›ºæœ‰åè©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
+- **æ–‡è„ˆã®è‡ªå·±å®Œçµæ€§**: å„QAãƒšã‚¢ã¯ã€ãã®ãƒšã‚¢å˜ä½“ã§æ„å‘³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
+- **å…·ä½“çš„ãªè¨€åŠ**: äººç‰©ã€ç‰©äº‹ã€æ¦‚å¿µã«ã¤ã„ã¦è¨€åŠã™ã‚‹éš›ã¯ã€å¿…ãšå…·ä½“çš„ãªåå‰ã‚„èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„
+- **çœç•¥ã®å›é¿**: ã€Œå‰è¿°ã®ã€ã€Œä¸Šè¨˜ã®ã€ã€Œå…ˆã»ã©ã®ã€ã€Œè©²å½“ã™ã‚‹ã€ãªã©ã®ä»–ã®ç®‡æ‰€ã‚’å‚ç…§ã™ã‚‹è¡¨ç¾ã¯é¿ã‘ã¦ãã ã•ã„
+- **æ–‡ç« å†…æƒ…å ±ã®å³å®ˆ**: ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã€æ¨æ¸¬ã‚„å¤–éƒ¨çŸ¥è­˜ã‚’åŠ ãˆãªã„ã§ãã ã•ã„
+
+## ç›®æ¨™ã¨ã™ã‚‹ä½“è£:
+{genre_title}
+{genre_description}
+
+## ç›®æ¨™ã¨ã™ã‚‹èª­è€…:
+{audience_title}
+{audience_description}
+
+## æ–‡ç« :
+---
+{context}
+---
+
+## å‡ºåŠ›å½¢å¼:
+**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<QAPairs>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
+å„Q&Aãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Question>` ã¨ `<Answer>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+
+**é‡è¦**: XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, ", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š& â†’ &amp;, < â†’ &lt;ï¼‰ã€‚
+å›ç­”æ–‡ã«æ”¹è¡Œã‚’å«ã‚ãšã€ä¸€è¡Œã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## å‡ºåŠ›ä¾‹:
+\```xml
+<QAPairs>
+<Pair>
+<Question>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ã€ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é€šè²¨ã§ã‚ã‚‹ã‚¢ãƒ‡ãƒã‚·ãƒ³ä¸‰ãƒªãƒ³é…¸ï¼ˆATPï¼‰ã®å¤§éƒ¨åˆ†ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚</Answer>
+</Pair>
+<Pair>
+<Question>ç”°ä¸­æ•™æˆãŒé–‹ç™ºã—ãŸæ–°ã—ã„æ²»ç™‚æ³•ã¯ã©ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ã‹ï¼Ÿ</Question>
+<Answer>ç”°ä¸­æ•™æˆãŒé–‹ç™ºã—ãŸæ–°ã—ã„æ²»ç™‚æ³•ã¯ã€å¾“æ¥ã®åŒ–å­¦ç™‚æ³•ã¨æ¯”è¼ƒã—ã¦å‰¯ä½œç”¨ãŒå°‘ãªãã€æ²»ç™‚åŠ¹æœãŒ30%å‘ä¸Šã™ã‚‹ã¨ã„ã†ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</Answer>
+</Pair>
+</QAPairs>
+\```
+
+ãã‚Œã§ã¯ã€Q&Aãƒšã‚¢ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
\ No newline at end of file
diff --git a/easy_dataset_cli/prompts/qa/qa_generation_with_fulltext.md b/easy_dataset_cli/prompts/qa/qa_generation_with_fulltext.md
new file mode 100644
index 0000000..6f57590
--- /dev/null
+++ b/easy_dataset_cli/prompts/qa/qa_generation_with_fulltext.md
@@ -0,0 +1,65 @@
+# å½¹å‰²: Q&Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ï¼ˆå…¨æ–‡+ãƒãƒ£ãƒ³ã‚¯å¯¾å¿œç‰ˆï¼‰
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰é«˜å“è³ªãªè³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹ã«ã€æŒ‡å®šã•ã‚ŒãŸã€Œä½“è£ã€ã¨ã€Œèª­è€…ã€ã«åˆã‚ã›ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã«é•·ã‘ã¦ã„ã¾ã™ã€‚
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
+
+ã“ã®ãƒ„ãƒ¼ãƒ«ã¯**easy-dataset-cli**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚
+- GitHub: https://github.com/Sunwood-ai-labsII/easy-dataset-cli.git
+- ç”¨é€”: é«˜å“è³ªãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
+- ä¸»è¦æ©Ÿèƒ½: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è‡ªå‹•Q&Aãƒšã‚¢ç”Ÿæˆã€ä½“è£ãƒ»èª­è€…ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
+
+## æŒ‡ç¤º:
+1. ä¸ãˆã‚‰ã‚ŒãŸã€Œå…¨æ–‡ã€ã¨ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ãã ã•ã„ã€‚
+2. æŒ‡å®šã•ã‚ŒãŸã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã¨ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®å½¹å‰²ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
+3. **ãƒãƒ£ãƒ³ã‚¯**ã®å†…å®¹ã‚’ä¸­å¿ƒã¨ã—ã¤ã¤ã€**å…¨æ–‡**ã®æ–‡è„ˆã‚’ç†è§£ã—ãŸä¸Šã§ã€{num_qa_pairs}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§æ´å¯Ÿã«å¯Œã‚“ã Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+4. è³ªå•ã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨è¤‡é›‘ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®è¦–ç‚¹ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+5. å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ¼ãƒ³ã€è©³ç´°ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+6. **é‡è¦**: è³ªå•ã¨å›ç­”ã¯ä¸»ã«ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã®å†…å®¹ã«åŸºã¥ã„ã¦ä½œæˆã—ã€ã€Œå…¨æ–‡ã€ã¯æ–‡è„ˆç†è§£ã®ãŸã‚ã®è£œåŠ©æƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
+
+## QAãƒšã‚¢ä½œæˆã«ãŠã‘ã‚‹é‡è¦ãªãƒ«ãƒ¼ãƒ«:
+- **ä»£åè©ã®ä½¿ç”¨ç¦æ­¢**: è³ªå•ã¨å›ç­”ã§ã¯ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œãã‚Œã€ã€Œã“ã‚Œã€ã€Œãã®äººã€ãªã©ã®ä»£åè©ã‚’ä½¿ç”¨ã›ãšã€å…·ä½“çš„ãªåè©ã‚„å›ºæœ‰åè©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
+- **æ–‡è„ˆã®è‡ªå·±å®Œçµæ€§**: å„QAãƒšã‚¢ã¯ã€ãã®ãƒšã‚¢å˜ä½“ã§æ„å‘³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
+- **å…·ä½“çš„ãªè¨€åŠ**: äººç‰©ã€ç‰©äº‹ã€æ¦‚å¿µã«ã¤ã„ã¦è¨€åŠã™ã‚‹éš›ã¯ã€å¿…ãšå…·ä½“çš„ãªåå‰ã‚„èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„
+- **çœç•¥ã®å›é¿**: ã€Œå‰è¿°ã®ã€ã€Œä¸Šè¨˜ã®ã€ã€Œå…ˆã»ã©ã®ã€ãªã©ã®ä»–ã®ç®‡æ‰€ã‚’å‚ç…§ã™ã‚‹è¡¨ç¾ã¯é¿ã‘ã¦ãã ã•ã„
+
+## ç›®æ¨™ã¨ã™ã‚‹ä½“è£:
+{genre_title}
+{genre_description}
+
+## ç›®æ¨™ã¨ã™ã‚‹èª­è€…:
+{audience_title}
+{audience_description}
+
+## å…¨æ–‡ï¼ˆæ–‡è„ˆç†è§£ç”¨ï¼‰:
+---
+{full_text}
+---
+
+## ãƒãƒ£ãƒ³ã‚¯ï¼ˆQAç”Ÿæˆå¯¾è±¡ï¼‰:
+---
+{chunk}
+---
+
+## å‡ºåŠ›å½¢å¼:
+**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<QAPairs>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
+å„Q&Aãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Question>` ã¨ `<Answer>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+
+**é‡è¦**: XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, ", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š& â†’ &amp;, < â†’ &lt;ï¼‰ã€‚
+å›ç­”æ–‡ã«æ”¹è¡Œã‚’å«ã‚ãšã€ä¸€è¡Œã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## å‡ºåŠ›ä¾‹:
+\```xml
+<QAPairs>
+<Pair>
+<Question>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ã€ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é€šè²¨ã§ã‚ã‚‹ã‚¢ãƒ‡ãƒã‚·ãƒ³ä¸‰ãƒªãƒ³é…¸ï¼ˆATPï¼‰ã®å¤§éƒ¨åˆ†ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚</Answer>
+</Pair>
+<Pair>
+<Question>ç”°ä¸­åšå£«ãŒç™ºè¦‹ã—ãŸæ–°ã—ã„é…µç´ ã®åå‰ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer>ç”°ä¸­åšå£«ãŒç™ºè¦‹ã—ãŸæ–°ã—ã„é…µç´ ã®åå‰ã¯ãƒ—ãƒ­ãƒ†ã‚¢ãƒ¼ã‚¼Xã§ã™ã€‚</Answer>
+</Pair>
+</QAPairs>
+\```
+
+ãã‚Œã§ã¯ã€Q&Aãƒšã‚¢ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
\ No newline at end of file
diff --git a/easy_dataset_cli/prompts/qa/qa_generation_with_surrounding.md b/easy_dataset_cli/prompts/qa/qa_generation_with_surrounding.md
new file mode 100644
index 0000000..d67c7a3
--- /dev/null
+++ b/easy_dataset_cli/prompts/qa/qa_generation_with_surrounding.md
@@ -0,0 +1,60 @@
+# å½¹å‰²: Q&Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ï¼ˆå‘¨è¾ºæ–‡è„ˆå¯¾å¿œç‰ˆï¼‰
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰é«˜å“è³ªãªè³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹ã«ã€æŒ‡å®šã•ã‚ŒãŸã€Œä½“è£ã€ã¨ã€Œèª­è€…ã€ã«åˆã‚ã›ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã«é•·ã‘ã¦ã„ã¾ã™ã€‚æä¾›ã•ã‚ŒãŸã€Œãƒ¡ã‚¤ãƒ³æœ¬æ–‡ã€ã¨ã€Œå‘¨è¾ºæ–‡è„ˆã€ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã—ã€æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè‡ªç„¶ãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
+
+ã“ã®ãƒ„ãƒ¼ãƒ«ã¯**easy-dataset-cli**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚
+- GitHub: https://github.com/Sunwood-ai-labsII/easy-dataset-cli.git
+- ç”¨é€”: é«˜å“è³ªãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
+- ä¸»è¦æ©Ÿèƒ½: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è‡ªå‹•Q&Aãƒšã‚¢ç”Ÿæˆã€ä½“è£ãƒ»èª­è€…ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
+
+## æŒ‡ç¤º:
+1. **ãƒ¡ã‚¤ãƒ³æœ¬æ–‡**ã¨**å‘¨è¾ºæ–‡è„ˆ**ã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ãã ã•ã„ã€‚
+2. æŒ‡å®šã•ã‚ŒãŸã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã¨ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®å½¹å‰²ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
+3. **ãƒ¡ã‚¤ãƒ³æœ¬æ–‡**ã‚’ä¸­å¿ƒã«ã—ã¤ã¤ã€**å‘¨è¾ºæ–‡è„ˆ**ã‚’å‚è€ƒã«ã—ã¦æ–‡è„ˆã‚’ç†è§£ã—ã€{num_qa_pairs}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§æ´å¯Ÿã«å¯Œã‚“ã Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+4. è³ªå•ã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨è¤‡é›‘ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®è¦–ç‚¹ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+5. å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ¼ãƒ³ã€è©³ç´°ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+6. **é‡è¦**: ãƒ¡ã‚¤ãƒ³æœ¬æ–‡ã®å†…å®¹ã‚’ä¸­å¿ƒã«ã—ã¤ã¤ã€å‘¨è¾ºæ–‡è„ˆã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ–‡è„ˆæƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€ã‚ˆã‚Šè‡ªç„¶ã§æ„å‘³ã®ã‚ã‚‹Q&Aã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+
+## QAãƒšã‚¢ä½œæˆã«ãŠã‘ã‚‹é‡è¦ãªãƒ«ãƒ¼ãƒ«:
+- **ä»£åè©ã®ä½¿ç”¨ç¦æ­¢**: è³ªå•ã¨å›ç­”ã§ã¯ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œãã‚Œã€ã€Œã“ã‚Œã€ã€Œãã®äººã€ã€Œãã®ä¼šç¤¾ã€ãªã©ã®ä»£åè©ã‚’ä½¿ç”¨ã›ãšã€å…·ä½“çš„ãªåè©ã‚„å›ºæœ‰åè©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
+- **æ–‡è„ˆã®è‡ªå·±å®Œçµæ€§**: å„QAãƒšã‚¢ã¯ã€ãã®ãƒšã‚¢å˜ä½“ã§æ„å‘³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
+- **å…·ä½“çš„ãªè¨€åŠ**: äººç‰©ã€ç‰©äº‹ã€æ¦‚å¿µã«ã¤ã„ã¦è¨€åŠã™ã‚‹éš›ã¯ã€å¿…ãšå…·ä½“çš„ãªåå‰ã‚„èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„
+- **çœç•¥ã®å›é¿**: ã€Œå‰è¿°ã®ã€ã€Œä¸Šè¨˜ã®ã€ã€Œå…ˆã»ã©ã®ã€ã€Œè©²å½“ã™ã‚‹ã€ãªã©ã®ä»–ã®ç®‡æ‰€ã‚’å‚ç…§ã™ã‚‹è¡¨ç¾ã¯é¿ã‘ã¦ãã ã•ã„
+- **æ–‡è„ˆæ´»ç”¨**: å‘¨è¾ºæ–‡è„ˆã®æƒ…å ±ã¯ç©æ¥µçš„ã«æ´»ç”¨ã—ã€ã‚ˆã‚Šè±Šã‹ãªè³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
+- **ãƒ¡ã‚¤ãƒ³æœ¬æ–‡å„ªå…ˆ**: Q&Aãƒšã‚¢ã¯ä¸»ã«ãƒ¡ã‚¤ãƒ³æœ¬æ–‡ã®å†…å®¹ã«åŸºã¥ã„ã¦ä½œæˆã—ã€å‘¨è¾ºæ–‡è„ˆã¯è£œå®Œæƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„
+
+## ç›®æ¨™ã¨ã™ã‚‹ä½“è£:
+{genre_title}
+{genre_description}
+
+## ç›®æ¨™ã¨ã™ã‚‹èª­è€…:
+{audience_title}
+{audience_description}
+
+## ã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
+{content}
+
+## å‡ºåŠ›å½¢å¼:
+**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<QAPairs>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
+å„Q&Aãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Question>` ã¨ `<Answer>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+
+**é‡è¦**: XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, ", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š& â†’ &, < â†’ <ï¼‰ã€‚
+å›ç­”æ–‡ã«æ”¹è¡Œã‚’å«ã‚ãšã€ä¸€è¡Œã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## å‡ºåŠ›ä¾‹:
+\```xml
+<QAPairs>
+<Pair>
+<Question>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ã€ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é€šè²¨ã§ã‚ã‚‹ã‚¢ãƒ‡ãƒã‚·ãƒ³ä¸‰ãƒªãƒ³é…¸ï¼ˆATPï¼‰ã®å¤§éƒ¨åˆ†ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚</Answer>
+</Pair>
+<Pair>
+<Question>ç”°ä¸­æ•™æˆãŒé–‹ç™ºã—ãŸæ–°ã—ã„æ²»ç™‚æ³•ã¯ã©ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ã‹ï¼Ÿ</Question>
+<Answer>ç”°ä¸­æ•™æˆãŒé–‹ç™ºã—ãŸæ–°ã—ã„æ²»ç™‚æ³•ã¯ã€å¾“æ¥ã®åŒ–å­¦ç™‚æ³•ã¨æ¯”è¼ƒã—ã¦å‰¯ä½œç”¨ãŒå°‘ãªãã€æ²»ç™‚åŠ¹æœãŒ30%å‘ä¸Šã™ã‚‹ã¨ã„ã†ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</Answer>
+</Pair>
+</QAPairs>
+\```
+
+ãã‚Œã§ã¯ã€å‘¨å›²ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸQ&Aãƒšã‚¢ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
diff --git a/easy_dataset_cli/prompts/qa/qa_generation_with_thinking.md b/easy_dataset_cli/prompts/qa/qa_generation_with_thinking.md
new file mode 100644
index 0000000..b55e9eb
--- /dev/null
+++ b/easy_dataset_cli/prompts/qa/qa_generation_with_thinking.md
@@ -0,0 +1,69 @@
+# å½¹å‰²: Q&Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ï¼ˆæ€è€ƒãƒ•ãƒ­ãƒ¼å¯¾å¿œç‰ˆï¼‰
+
+ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰é«˜å“è³ªãªè³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹ã«ã€æŒ‡å®šã•ã‚ŒãŸã€Œä½“è£ã€ã¨ã€Œèª­è€…ã€ã«åˆã‚ã›ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã«é•·ã‘ã¦ã„ã¾ã™ã€‚ã¾ãŸã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ˜ç¤ºçš„ã«è¨˜è¿°ã™ã‚‹èƒ½åŠ›ã«ã‚‚å„ªã‚Œã¦ã„ã¾ã™ã€‚
+
+## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
+
+ã“ã®ãƒ„ãƒ¼ãƒ«ã¯**easy-dataset-cli**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã§ã™ã€‚
+- GitHub: https://github.com/Sunwood-ai-labsII/easy-dataset-cli.git
+- ç”¨é€”: é«˜å“è³ªãªQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
+- ä¸»è¦æ©Ÿèƒ½: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è‡ªå‹•Q&Aãƒšã‚¢ç”Ÿæˆã€ä½“è£ãƒ»èª­è€…ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
+
+## æŒ‡ç¤º:
+1. ä¸ãˆã‚‰ã‚ŒãŸã€Œå…¨æ–‡ã€ã¨ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ãã ã•ã„ã€‚
+2. æŒ‡å®šã•ã‚ŒãŸã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã¨ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®å½¹å‰²ã«ãªã‚Šãã£ã¦ãã ã•ã„ã€‚
+3. **ãƒãƒ£ãƒ³ã‚¯**ã®å†…å®¹ã‚’ä¸­å¿ƒã¨ã—ã¤ã¤ã€**å…¨æ–‡**ã®æ–‡è„ˆã‚’ç†è§£ã—ãŸä¸Šã§ã€{num_qa_pairs}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§æ´å¯Ÿã«å¯Œã‚“ã Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
+4. è³ªå•ã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨è¤‡é›‘ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹èª­è€…ã€ã®è¦–ç‚¹ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+5. å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ¼ãƒ³ã€è©³ç´°ã•ã¯ã€ã€Œç›®æ¨™ã¨ã™ã‚‹ä½“è£ã€ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
+6. **é‡è¦**: è³ªå•ã¨å›ç­”ã¯ä¸»ã«ã€Œãƒãƒ£ãƒ³ã‚¯ã€ã®å†…å®¹ã«åŸºã¥ã„ã¦ä½œæˆã—ã€ã€Œå…¨æ–‡ã€ã¯æ–‡è„ˆç†è§£ã®ãŸã‚ã®è£œåŠ©æƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
+7. å„Q&Aãƒšã‚¢ã«ã¯ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ˜ç¤ºçš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## QAãƒšã‚¢ä½œæˆã«ãŠã‘ã‚‹é‡è¦ãªãƒ«ãƒ¼ãƒ«:
+- **ä»£åè©ã®ä½¿ç”¨ç¦æ­¢**: è³ªå•ã¨å›ç­”ã§ã¯ã€ã€Œå½¼ã€ã€Œå½¼å¥³ã€ã€Œãã‚Œã€ã€Œã“ã‚Œã€ã€Œãã®äººã€ãªã©ã®ä»£åè©ã‚’ä½¿ç”¨ã›ãšã€å…·ä½“çš„ãªåè©ã‚„å›ºæœ‰åè©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
+- **æ–‡è„ˆã®è‡ªå·±å®Œçµæ€§**: å„QAãƒšã‚¢ã¯ã€ãã®ãƒšã‚¢å˜ä½“ã§æ„å‘³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
+- **å…·ä½“çš„ãªè¨€åŠ**: äººç‰©ã€ç‰©äº‹ã€æ¦‚å¿µã«ã¤ã„ã¦è¨€åŠã™ã‚‹éš›ã¯ã€å¿…ãšå…·ä½“çš„ãªåå‰ã‚„èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„
+- **çœç•¥ã®å›é¿**: ã€Œå‰è¿°ã®ã€ã€Œä¸Šè¨˜ã®ã€ã€Œå…ˆã»ã©ã®ã€ãªã©ã®ä»–ã®ç®‡æ‰€ã‚’å‚ç…§ã™ã‚‹è¡¨ç¾ã¯é¿ã‘ã¦ãã ã•ã„
+- **æ€è€ƒãƒ•ãƒ­ãƒ¼å†…ã§ã‚‚è‡ªå·±å®Œçµ**: `<think>`ã‚¿ã‚°å†…ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚ã€ä»£åè©ã‚„çœç•¥è¡¨ç¾ã‚’ä½¿ã‚ãšã€å…·ä½“çš„ãªåè©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
+
+## ç›®æ¨™ã¨ã™ã‚‹ä½“è£:
+{genre_title}
+{genre_description}
+
+## ç›®æ¨™ã¨ã™ã‚‹èª­è€…:
+{audience_title}
+{audience_description}
+
+## å…¨æ–‡ï¼ˆæ–‡è„ˆç†è§£ç”¨ï¼‰:
+---
+{full_text}
+---
+
+## ãƒãƒ£ãƒ³ã‚¯ï¼ˆQAç”Ÿæˆå¯¾è±¡ï¼‰:
+---
+{chunk}
+---
+
+## å‡ºåŠ›å½¢å¼:
+**å¿…ãš**ã€ãƒ«ãƒ¼ãƒˆè¦ç´ ãŒ `<QAPairs>` ã§ã‚ã‚‹å˜ä¸€ã®æœ‰åŠ¹ãªXMLã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚XMLä»¥å¤–ã®èª¬æ˜æ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
+å„Q&Aãƒšã‚¢ã¯ `<Pair>` ã‚¿ã‚°ã§å›²ã¿ã€ãã®ä¸­ã« `<Question>` ã¨ `<Answer>` ã‚¿ã‚°ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
+
+**é‡è¦**: 
+- å›ç­”ã¯å¿…ãš`<Answer><think>æ€è€ƒãƒ•ãƒ­ãƒ¼...</think>å›ç­”...</Answer>`ã®ã‚ˆã†ã«ã€æ€è€ƒãƒ•ãƒ­ãƒ¼ã‚’`<think>`ã‚¿ã‚°ã§å›²ã¿ã€å›ç­”æœ¬æ–‡ã®ç›´å‰ã«å«ã‚ã¦ãã ã•ã„ã€‚
+- XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, ", 'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š& â†’ &amp;, < â†’ &lt;ï¼‰ã€‚
+- å›ç­”æ–‡ã«æ”¹è¡Œã‚’å«ã‚ãšã€ä¸€è¡Œã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
+
+## å‡ºåŠ›ä¾‹:
+\```xml
+<QAPairs>
+<Pair>
+<Question>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ä½•ã§ã™ã‹ï¼Ÿ</Question>
+<Answer><think>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã¯ç´°èƒã®ã€Œç™ºé›»æ‰€ã€ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ãŠã‚Šã€ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã¯ç´°èƒå‘¼å¸ã‚’é€šã˜ã¦æ „é¤Šç´ ã‹ã‚‰ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ç”£ç”Ÿã™ã‚‹ã€‚ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªç”£ç‰©ãŒATPã§ã‚ã‚Šã€ATPã¯ç´°èƒã®ã‚ã‚‰ã‚†ã‚‹ç”Ÿå‘½æ´»å‹•ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã¨ãªã‚‹ã€‚</think>ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®ä¸»ãªæ©Ÿèƒ½ã¯ã€ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é€šè²¨ã§ã‚ã‚‹ã‚¢ãƒ‡ãƒã‚·ãƒ³ä¸‰ãƒªãƒ³é…¸ï¼ˆATPï¼‰ã®å¤§éƒ¨åˆ†ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚</Answer>
+</Pair>
+<Pair>
+<Question>ç”°ä¸­åšå£«ãŒç™ºè¦‹ã—ãŸæ–°ã—ã„é…µç´ ã¯ã©ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ã‹ï¼Ÿ</Question>
+<Answer><think>ç”°ä¸­åšå£«ãŒç™ºè¦‹ã—ãŸãƒ—ãƒ­ãƒ†ã‚¢ãƒ¼ã‚¼Xã¨ã„ã†é…µç´ ã¯ã€å¾“æ¥ã®ãƒ—ãƒ­ãƒ†ã‚¢ãƒ¼ã‚¼ã¨ã¯ç•°ãªã‚‹åŸºè³ªç‰¹ç•°æ€§ã‚’ç¤ºã™ã€‚ãƒ—ãƒ­ãƒ†ã‚¢ãƒ¼ã‚¼Xã¯é«˜æ¸©ç’°å¢ƒã§ã‚‚å®‰å®šã—ã¦æ©Ÿèƒ½ã—ã€ç”£æ¥­å¿œç”¨ã®å¯èƒ½æ€§ãŒé«˜ã„é…µç´ ã§ã‚ã‚‹ã€‚</think>ç”°ä¸­åšå£«ãŒç™ºè¦‹ã—ãŸãƒ—ãƒ­ãƒ†ã‚¢ãƒ¼ã‚¼Xã¯ã€é«˜æ¸©ç’°å¢ƒã§ã‚‚å®‰å®šã—ã¦æ©Ÿèƒ½ã—ã€å¾“æ¥ã®é…µç´ ã«ã¯ãªã„ç‹¬ç‰¹ãªåŸºè³ªç‰¹ç•°æ€§ã‚’æŒã¤ç”»æœŸçš„ãªé…µç´ ã§ã™ã€‚</Answer>
+</Pair>
+</QAPairs>
+\```
+
+ãã‚Œã§ã¯ã€æ€è€ƒãƒ•ãƒ­ãƒ¼ã‚’å«ã‚€Q&Aãƒšã‚¢ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
\ No newline at end of file
diff --git a/easy_dataset_cli/text_splitter.py b/easy_dataset_cli/text_splitter.py
index 2f72b69..547343a 100644
--- a/easy_dataset_cli/text_splitter.py
+++ b/easy_dataset_cli/text_splitter.py
@@ -1,7 +1,7 @@
 # easy_dataset_cli/text_splitter.py
 """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²é–¢é€£æ©Ÿèƒ½"""
 
-from typing import List
+from typing import List, Tuple
 from langchain_text_splitters import RecursiveCharacterTextSplitter
 
 
@@ -15,3 +15,92 @@ def split_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
     )
     docs = text_splitter.create_documents([text])
     return [doc.page_content for doc in docs]
+
+
+def get_chunk_with_surrounding_context(
+    chunks: List[str],
+    target_index: int,
+    context_before: int = 1,
+    context_after: int = 1
+) -> Tuple[str, List[str]]:
+    """
+    æŒ‡å®šã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒ£ãƒ³ã‚¯ã¨å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
+
+    Args:
+        chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
+        target_index: å¯¾è±¡ã¨ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
+        context_before: å‰æ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
+        context_after: å¾Œæ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
+
+    Returns:
+        Tuple[str, List[str]]: (å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ)
+    """
+    # å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
+    target_chunk = chunks[target_index]
+
+    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
+    context_chunks = []
+
+    # å‰æ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
+    start_idx = max(0, target_index - context_before)
+    for i in range(start_idx, target_index):
+        if i >= 0 and i < len(chunks):
+            context_chunks.append(f"[å‰æ–‡è„ˆ{i+1}]: {chunks[i]}")
+
+    # å¾Œæ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
+    end_idx = min(len(chunks), target_index + context_after + 1)
+    for i in range(target_index + 1, end_idx):
+        if i >= 0 and i < len(chunks):
+            context_chunks.append(f"[å¾Œæ–‡è„ˆ{i+1}]: {chunks[i]}")
+
+    return target_chunk, context_chunks
+
+
+def create_augmented_chunks(
+    chunks: List[str],
+    context_before: int = 1,
+    context_after: int = 1,
+    max_context_length: int = 4000
+) -> List[Tuple[str, str, List[str]]]:
+    """
+    å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä»˜ä¸ã—ãŸæ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹
+
+    Args:
+        chunks: å…ƒã®ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
+        context_before: å‰æ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
+        context_after: å¾Œæ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
+        max_context_length: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§æ–‡å­—æ•°ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ã‚µã‚¤ã‚ºåˆ¶é™å¯¾ç­–ï¼‰
+
+    Returns:
+        List[Tuple[str, str, List[str]]]: [(å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯, æ–‡è„ˆä»˜ããƒãƒ£ãƒ³ã‚¯, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ãƒªã‚¹ãƒˆ), ...]
+    """
+    augmented_chunks = []
+
+    for i, chunk in enumerate(chunks):
+        target_chunk, context_chunks = get_chunk_with_surrounding_context(
+            chunks, i, context_before, context_after
+        )
+
+        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦ä¸€ã¤ã®æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹
+        context_text = "\n\n".join(context_chunks)
+
+        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚µã‚¤ã‚ºåˆ¶é™å¯¾ç­–ï¼šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯å¾Œã‚ã‹ã‚‰åˆ‡ã‚Šè©°ã‚ã‚‹
+        if len(context_text) > max_context_length:
+            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾Œã‚ã‹ã‚‰é †ã«åˆ‡ã‚Šè©°ã‚
+            current_context = ""
+            reversed_context = context_chunks[::-1]  # å¾Œã‚ã‹ã‚‰é †ã«
+
+            for ctx in reversed_context:
+                if len(current_context) + len(ctx) + 2 <= max_context_length:
+                    current_context = ctx + "\n\n" + current_context
+                else:
+                    break
+
+            context_text = current_context.rstrip()
+
+        # å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿åˆã‚ã›
+        augmented_content = f"### ã€ãƒ¡ã‚¤ãƒ³æœ¬æ–‡ã€‘: ------------- \n\```\n{target_chunk}\n\```\n\n ### ã€å‘¨è¾ºæ–‡è„ˆã€‘: -------------\n\```\n{context_text}\n\```"
+
+        augmented_chunks.append((target_chunk, augmented_content, context_chunks))
+
+    return augmented_chunks
diff --git a/example/input/documents/Touhou_Chireiden.md b/example/input/documents/Touhou_Chireiden.md
deleted file mode 100644
index 80f2304..0000000
--- a/example/input/documents/Touhou_Chireiden.md
+++ /dev/null
@@ -1,203 +0,0 @@
-# æ±æ–¹åœ°éœŠæ®¿ ã€œ Subterranean Animism.
-
-æ±æ–¹åœ°éœŠæ®¿ ã€œ Subterranean Animism.ã‚¸ãƒ£ãƒ³ãƒ«|  [å¼¾å¹•ç³»ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](/wiki/%E5%BC%BE%E5%B9%95%E7%B3%BB%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0 "å¼¾å¹•ç³»ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°")ã‚²ãƒ¼ãƒ   
----|---  
-å¯¾å¿œæ©Ÿç¨®|  [Windows](/wiki/Microsoft_Windows "Microsoft Windows") [2000](/wiki/Microsoft_Windows_2000 "Microsoft Windows 2000")/[XP](/wiki/Microsoft_Windows_XP "Microsoft Windows XP")  
-é–‹ç™ºå…ƒ|  [ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£](/wiki/%E4%B8%8A%E6%B5%B7%E3%82%A2%E3%83%AA%E3%82%B9%E5%B9%BB%E6%A8%82%E5%9B%A3 "ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£")  
-ç™ºå£²å…ƒ|  [ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£](/wiki/%E4%B8%8A%E6%B5%B7%E3%82%A2%E3%83%AA%E3%82%B9%E5%B9%BB%E6%A8%82%E5%9B%A3 "ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£")  
-ã‚·ãƒªãƒ¼ã‚º|  [æ±æ–¹Project](/wiki/%E6%9D%B1%E6%96%B9Project "æ±æ–¹Project")  
-ãƒãƒ¼ã‚¸ãƒ§ãƒ³|  1.00aï¼ˆ2008å¹´8æœˆ16æ—¥ï¼‰  
-äººæ•°|  1äºº  
-ãƒ¡ãƒ‡ã‚£ã‚¢|  [CD-ROM](/wiki/CD-ROM "CD-ROM")  
-ç™ºå£²æ—¥|  2008å¹´8æœˆ16æ—¥  
-2020å¹´6æœˆ6æ—¥ï¼ˆ[Steam](/wiki/Steam "Steam")ç‰ˆï¼‰  
-å¿…è¦ç’°å¢ƒ|  CPU: [Pentium](/wiki/Intel_Pentium_\(1993%E5%B9%B4\) "Intel Pentium \(1993å¹´\)")ä»¥é™ 1GHzä»¥ä¸Š æ¨å¥¨  
-[DirectX](/wiki/Microsoft_DirectX "Microsoft DirectX"): 9.0ä»¥ä¸Š  
-[HDDç©ºãå®¹é‡](/wiki/%E3%83%8F%E3%83%BC%E3%83%89%E3%83%87%E3%82%A3%E3%82%B9%E3%82%AF%E3%83%89%E3%83%A9%E3%82%A4%E3%83%96 "ãƒãƒ¼ãƒ‰ãƒ‡ã‚£ã‚¹ã‚¯ãƒ‰ãƒ©ã‚¤ãƒ–"): 600MB ä»¥ä¸Š  
-[ãƒ¡ãƒ¢ãƒª](/wiki/%E4%B8%BB%E8%A8%98%E6%86%B6%E8%A3%85%E7%BD%AE "ä¸»è¨˜æ†¶è£…ç½®"): 256MB ä»¥ä¸Š  
-ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”|  4:3  
-è§£åƒåº¦|  640Ã—480ï¼ˆæ¨™æº–ï¼‰  
-ãã®ä»–|  [åŒäººã‚²ãƒ¼ãƒ ](/wiki/%E5%90%8C%E4%BA%BA%E3%82%B2%E3%83%BC%E3%83%A0 "åŒäººã‚²ãƒ¼ãƒ ")ï¼ˆ[ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼ã‚ºã‚²ãƒ¼ãƒ ](/wiki/%E3%82%A4%E3%83%B3%E3%83%87%E3%82%A3%E3%83%BC%E3%82%BA%E3%82%B2%E3%83%BC%E3%83%A0 "ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼ã‚ºã‚²ãƒ¼ãƒ ")ï¼‰  
-[ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¡¨ç¤º](/wiki/Template:%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%82%B2%E3%83%BC%E3%83%A0 "Template:ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚²ãƒ¼ãƒ ")  
-  
-ã€**æ±æ–¹åœ°éœŠæ®¿ ã€œ Subterranean Animism.** ã€ï¼ˆã¨ã†ã»ã†ã¡ã‚Œã„ã§ã‚“ ã‚µãƒ–ã‚¿ãƒ¬ã‚¤ãƒ‹ã‚¢ãƒ³ãƒ»ã‚¢ãƒ‹ãƒŸã‚ºãƒ ï¼‰ã¨ã¯ã€[åŒäººã‚µãƒ¼ã‚¯ãƒ«](/wiki/%E5%90%8C%E4%BA%BA%E3%82%B5%E3%83%BC%E3%82%AF%E3%83%AB "åŒäººã‚µãƒ¼ã‚¯ãƒ«")ã€Œ[ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£](/wiki/%E4%B8%8A%E6%B5%B7%E3%82%A2%E3%83%AA%E3%82%B9%E5%B9%BB%E6%A8%82%E5%9B%A3 "ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£")ã€åˆ¶ä½œã®[å¼¾å¹•ç³»ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](/wiki/%E5%BC%BE%E5%B9%95%E7%B3%BB%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0 "å¼¾å¹•ç³»ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°")ã‚²ãƒ¼ãƒ ã§ã‚ã‚Šã€[æ±æ–¹Project](/wiki/%E6%9D%B1%E6%96%B9Project "æ±æ–¹Project")ã®ç¬¬11å¼¾ã«ã‚ãŸã‚‹ä½œå“ã§ã‚ã‚‹ã€‚ 
-
-æœ¬ä½œã¯ã€2008å¹´5æœˆ25æ—¥é–‹å‚¬ã®[åŒäººã‚¤ãƒ™ãƒ³ãƒˆ](/wiki/%E5%90%8C%E4%BA%BA%E8%AA%8C%E5%8D%B3%E5%A3%B2%E4%BC%9A "åŒäººèªŒå³å£²ä¼š")ã€Œ[åšéº—ç¥ç¤¾ä¾‹å¤§ç¥­](/wiki/%E5%8D%9A%E9%BA%97%E7%A5%9E%E7%A4%BE%E4%BE%8B%E5%A4%A7%E7%A5%AD "åšéº—ç¥ç¤¾ä¾‹å¤§ç¥­")5ã€ã«ã¦ä½“é¨“ç‰ˆCD-ROMãŒè²©å£²ã•ã‚Œã€6æœˆ29æ—¥ã«ä¸Šæµ·ã‚¢ãƒªã‚¹å¹»æ¨‚å›£ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã§Webä½“é¨“ç‰ˆãŒå…¬é–‹ã•ã‚Œã€8æœˆ16æ—¥é–‹å‚¬ã®åŒäººã‚¤ãƒ™ãƒ³ãƒˆã€Œ[ã‚³ãƒŸãƒƒã‚¯ãƒãƒ¼ã‚±ãƒƒãƒˆ](/wiki/%E3%82%B3%E3%83%9F%E3%83%83%E3%82%AF%E3%83%9E%E3%83%BC%E3%82%B1%E3%83%83%E3%83%88 "ã‚³ãƒŸãƒƒã‚¯ãƒãƒ¼ã‚±ãƒƒãƒˆ")74ã€ã«ã¦å®Œæˆç‰ˆãŒè²©å£²ã•ã‚ŒãŸã€‚å¾Œã«[åŒäººã‚·ãƒ§ãƒƒãƒ—](/wiki/%E5%90%8C%E4%BA%BA%E3%82%B7%E3%83%A7%E3%83%83%E3%83%97 "åŒäººã‚·ãƒ§ãƒƒãƒ—")ã§ã®[å§”è¨—è²©å£²](/wiki/%E5%A7%94%E8%A8%97%E8%B2%A9%E5%A3%B2 "å§”è¨—è²©å£²")ã‚‚è¡Œãªã‚ã‚Œã¦ã„ã‚‹ã€‚é›‘èªŒã€[ã‚­ãƒ£ãƒ©â˜†ãƒ¡ãƒ«](/wiki/%E3%82%AD%E3%83%A3%E3%83%A9%E2%98%86%E3%83%A1%E3%83%AB "ã‚­ãƒ£ãƒ©â˜†ãƒ¡ãƒ«")ã€Vol.5ï¼ˆ2008å¹´6æœˆ25æ—¥ç™ºå£²ï¼‰ã®ä»˜å±CD-ROMã«ã‚‚ä½“é¨“ç‰ˆãŒåéŒ²ã•ã‚Œã¦ã„ã‚‹ã€‚ 
-
-ã¾ãŸã€æœ¬ä½œã¯2020å¹´6æœˆ6æ—¥ã«Steamã«ã¦é…ä¿¡ã•ã‚ŒãŸ[1]ã€‚ 
-
-æœ¬ä½œã®ã‚ã‚‰ã™ã˜ã«ãŠã‘ã‚‹å†¬å­£ã®è©±ã¨ã—ã¦ã¯éå»ã‚ˆã‚Šã€æ—§ä½œã«ãŠã„ã¦ã¯å†¬ã‚³ãƒŸ(C53Â·C55)ã«ã¦ç™ºè¡¨ã•ã‚ŒãŸäº‹ãŒã—ã°ã—ã°ã‚ã£ãŸã‚‚ã®ã®ã€äºˆå®šé€šã‚Šã«å¤å­£ã«é ’å¸ƒã•ã‚ŒãŸä½œå“ã§ã‚‚ã‚ã‚‹ï½¡ 
-
-æœ¬é …ã§ã¯ã€ä»¥é™ã¯ã€åœ°éœŠæ®¿ã€ã¨ç§°ã™ã‚‹ã“ã¨ã¨ã™ã‚‹ã€‚ãã®ä»–ã®æœ¬é …ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ±æ–¹Projecté–¢é€£ã®ç•¥ç§°ã«ã¤ã„ã¦ã¯ã€[æ±æ–¹Project#å‡¡ä¾‹](/wiki/%E6%9D%B1%E6%96%B9Project#å‡¡ä¾‹ "æ±æ–¹Project")ã‚’å‚ç…§ã€‚ 
-
-## ã‚·ã‚¹ãƒ†ãƒ 
-
-
-â†’ã€Œ[æ±æ–¹Project Â§ åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ ](/wiki/%E6%9D%B1%E6%96%B9Project#åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ  "æ±æ–¹Project")ã€ã‚‚å‚ç…§
-
-æ©Ÿä½“æ€§èƒ½ã®ç•°ãªã‚‹ã€Œ[åšéº—éœŠå¤¢](/wiki/%E5%8D%9A%E9%BA%97%E9%9C%8A%E5%A4%A2 "åšéº—éœŠå¤¢")ã€ã€Œ[éœ§é›¨é­”ç†æ²™](/wiki/%E9%9C%A7%E9%9B%A8%E9%AD%94%E7%90%86%E6%B2%99 "éœ§é›¨é­”ç†æ²™")ã€ã®2ç¨®é¡ã®è‡ªæ©Ÿã‹ã‚‰1ã¤é¸æŠã—ã€ãã®å¾Œãã‚Œãã‚Œ3ç¨®é¡ã‚ã‚‹æ­¦å™¨ã‚¿ã‚¤ãƒ—ï¼ˆè£…å‚™ï¼‰ã‹ã‚‰ã„ãšã‚Œã‹ã‚’é¸æŠã™ã‚‹ã€‚æœ¬ä½œã§ã¯ã€Œè‡ªæ©ŸãŒå¦–æ€ªã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã®ã²ã¨ã‚Šã¨çµ„ã¿ã€åœ°ä¸Šã«æ®‹ã‚‹ãã®å¦–æ€ªã®åŠ›ã‚’å€Ÿã‚ŠãªãŒã‚‰åœ°ä¸‹ã«æ½œã‚‹ã€ã¨ã„ã†è¨­å®šã«ãªã£ã¦ãŠã‚Šã€æ­¦å™¨ã‚¿ã‚¤ãƒ—ã®é¸æŠã¯ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã®é¸æŠã¨åŒç¾©ã§ã‚ã‚‹ã€‚ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ãªã‚‹å¦–æ€ªã¯ã€éœŠå¤¢ãŒã€Œ[å…«é›²ç´«](/wiki/%E5%85%AB%E9%9B%B2%E7%B4%AB "å…«é›²ç´«")ã€ã€Œ[ä¼Šå¹èƒé¦™](/wiki/%E4%BC%8A%E5%90%B9%E8%90%83%E9%A6%99 "ä¼Šå¹èƒé¦™")ã€ã€Œ[å°„å‘½ä¸¸æ–‡](/wiki/%E5%B0%84%E5%91%BD%E4%B8%B8%E6%96%87 "å°„å‘½ä¸¸æ–‡")ã€ã€é­”ç†æ²™ãŒã€Œ[ã‚¢ãƒªã‚¹ãƒ»ãƒãƒ¼ã‚¬ãƒˆãƒ­ã‚¤ãƒ‰](/wiki/%E3%82%A2%E3%83%AA%E3%82%B9%E3%83%BB%E3%83%9E%E3%83%BC%E3%82%AC%E3%83%88%E3%83%AD%E3%82%A4%E3%83%89 "ã‚¢ãƒªã‚¹ãƒ»ãƒãƒ¼ã‚¬ãƒˆãƒ­ã‚¤ãƒ‰")ã€ã€Œ[ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸](/wiki/%E3%83%91%E3%83%81%E3%83%A5%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8E%E3%83%BC%E3%83%AC%E3%83%83%E3%82%B8 "ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸")ã€ã€Œ[æ²³åŸã«ã¨ã‚Š](/wiki/%E6%B2%B3%E5%9F%8E%E3%81%AB%E3%81%A8%E3%82%8A "æ²³åŸã«ã¨ã‚Š")ã€ã®å„3å[1]ã€‚æ•µã‚„æ•µå¼¾ã«å½“ãŸã‚‹ã¨ãƒŸã‚¹ã¨ãªã‚Šæ®‹æ©ŸãŒ1ã¤æ¸›ã£ãŸä¸Šã§ãã®å ´ã§å¾©æ´»ã™ã‚‹ã€‚å…¨ã¦ã®æ®‹æ©Ÿã‚’å¤±ã†ã¨[ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼](/wiki/%E3%82%B2%E3%83%BC%E3%83%A0%E3%82%AA%E3%83%BC%E3%83%90%E3%83%BC "ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼")ã¨ãªã‚‹ãŒã€[ã‚³ãƒ³ãƒ†ã‚£ãƒ‹ãƒ¥ãƒ¼](/wiki/%E3%82%B2%E3%83%BC%E3%83%A0%E3%82%AA%E3%83%BC%E3%83%90%E3%83%BC "ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼")ã™ã‚Œã°ãã®ã‚¹ãƒ†ãƒ¼ã‚¸ã®æœ€åˆã‹ã‚‰å¾©æ´»ã—ã‚²ãƒ¼ãƒ ã‚’ç¶šè¡Œå¯èƒ½ã€‚ã‚³ãƒ³ãƒ†ãƒ‹ãƒ¥ãƒ¼ã—ãªã„ã§6[é¢](/wiki/%E3%82%B9%E3%83%86%E3%83%BC%E3%82%B8_\(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%82%B2%E3%83%BC%E3%83%A0\) "ã‚¹ãƒ†ãƒ¼ã‚¸ \(ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚²ãƒ¼ãƒ \)")ï¼ˆæœ€çµ‚é¢ï¼‰ã®[ãƒœã‚¹](/wiki/%E3%83%9C%E3%82%B9%E3%82%AD%E3%83%A3%E3%83%A9%E3%82%AF%E3%82%BF%E3%83%BC "ãƒœã‚¹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼")ã‚’å€’ã™ã¨ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«ãªã‚‹ã€‚é›£æ˜“åº¦Normalä»¥ä¸Šã§ã‚³ãƒ³ãƒ†ã‚£ãƒ‹ãƒ¥ãƒ¼ã›ãšã«ã‚¯ãƒªã‚¢ã™ã‚Œã°ã€å…¨1é¢ã®Extraã‚¹ãƒ†ãƒ¼ã‚¸ãŒè¿½åŠ ã•ã‚Œã‚‹ã€‚ 
-
-æœ¬ä½œã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã€Œæ®‹æ©Ÿã®æ¬ ç‰‡ã€ã‚’ä¸€å®šæ•°é›†ã‚ã‚‹ã“ã¨ã§ã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ãƒ‰ã™ã‚‹[2][1]ã€‚ã€Œæ®‹æ©Ÿã®æ¬ ç‰‡ã€ã¯ã€ãƒœã‚¹æˆ¦ã«ã¦ãƒŸã‚¹ã‚’ã›ãšï¼ˆãƒœãƒ ã¯ä½¿ç”¨å¯ï¼‰ã«æ—¢å®šã®æ•µãƒ©ã‚¤ãƒ•ã‚’å‰Šã‚‹ã¨å‡ºç¾ã™ã‚‹ã€‚ 
-
-#### äº¤ä¿¡å¼·åº¦
-
-    [![](//upload.wikimedia.org/wikipedia/commons/9/96/Mobile_phone_signal.png)](/wiki/%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Mobile_phone_signal.png)ä¸€èˆ¬çš„ãªæºå¸¯é›»è©±ã®é€šä¿¡å¼·åº¦ã‚¢ã‚¤ã‚³ãƒ³ã®ä¾‹ã€‚ã€åœ°éœŠæ®¿ã€ã§ã®ãã‚Œã¨ã¯è‹¥å¹²ãƒ‡ã‚¶ã‚¤ãƒ³ãŒç•°ãªã‚‹ã€‚
-    æœ¬ä½œã«ã¯**äº¤ä¿¡å¼·åº¦** ã¨ã„ã†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹[2]ã€‚ç‚¹ã‚¢ã‚¤ãƒ†ãƒ å…¥æ‰‹æ™‚ã®å¾—ç‚¹ã¯äº¤ä¿¡å¼·åº¦ã«ã‚ˆã£ã¦è£œæ­£ã•ã‚Œã‚‹ãŸã‚ã€ã‚¹ã‚³ã‚¢ç¨¼ãã®éš›ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãªã£ã¦ã„ã‚‹ã€‚
-    äº¤ä¿¡å¼·åº¦ã¯ã€ç”»é¢å·¦ä¸‹ã«[æºå¸¯é›»è©±](/wiki/%E6%90%BA%E5%B8%AF%E9%9B%BB%E8%A9%B1 "æºå¸¯é›»è©±")ã®å—ä¿¡å¼·åº¦ã®ã‚¢ã‚¤ã‚³ãƒ³ã®ã‚ˆã†ãªå½¢ã§è¡¨ç¤ºã•ã‚Œã‚‹[2]ã€‚äº¤ä¿¡å¼·åº¦ã¯[ã‚¢ã‚¤ãƒ†ãƒ è‡ªå‹•è’é›†](/wiki/%E6%9D%B1%E6%96%B9Project#ã‚¢ã‚¤ãƒ†ãƒ  "æ±æ–¹Project")ãŒç™ºå‹•ã™ã‚‹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚ä¸Šã¾ã§ç§»å‹•ã™ã‚‹ã‹ã€[æ•µå¼¾ã«ã‹ã™ã‚‹](/wiki/%E6%9D%B1%E6%96%B9Project#å½“ãŸã‚Šåˆ¤å®š "æ±æ–¹Project")ã“ã¨ã§å¢—åŠ ã—ã€å‰è€…ã®å ´åˆã¯ä¸€ç¬ã§æœ€å¤§å€¤ã¾ã§å¢—åŠ ã™ã‚‹ã€‚äº¤ä¿¡å¼·åº¦ã¯å¢—åŠ ã™ã‚‹è¡Œå‹•ã‚’ã¨ã‚‰ãªã„ã¨æ¸›å°‘ãŒå§‹ã¾ã‚‹ãŒã€æœ€å¤§ã«ãªã£ã¦ã„ã‚‹ã¨ãã¯ä¸€å®šæ™‚é–“ã¯æ¸›å°‘ã—ãªã„ã€‚
-    äº¤ä¿¡å¼·åº¦ãŒæœ€å¤§ã®æ™‚ã«ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ è‡ªå‹•è’é›†ãŒç™ºç”Ÿã™ã‚‹ã€‚ã™ãªã‚ã¡ã€æ•µå¼¾ã«ã‹ã™ã‚‹ã“ã¨ã§é€šä¿¡å¼·åº¦ã‚’æœ€å¤§ã«å‡ºæ¥ã‚Œã°ã€ç”»é¢ä¸Šéƒ¨ã«è¡Œã‹ãªãã¦ã‚‚ã‚¢ã‚¤ãƒ†ãƒ è‡ªå‹•è’é›†ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
-    äº¤ä¿¡å¼·åº¦ã®æœ€å¤§å€¤ã¯åˆæœŸå€¤ã¯1.00ã ãŒã€ã‹ã™ã‚Šå›æ•°ãŒ100ã®å€æ•°ã«é”ã™ã‚‹ã”ã¨ã«0.01ãšã¤ä¸Šæ˜‡ã™ã‚‹ã€‚
-    ã‚²ãƒ¼ãƒ ä¸­ã§ã®è¨­å®šã§ã¯ã€åœ°ä¸‹ã«æ½œã‚‹éœŠå¤¢ã‚„é­”ç†æ²™ã¨ã€åœ°ä¸Šã«ã„ã‚‹ã‚µãƒãƒ¼ãƒˆå¦–æ€ªã¨ã®äº¤ä¿¡å¼·åº¦ã‚’ç¤ºã™ã‚‚ã®ã¨ã•ã‚Œã¦ã„ã‚‹ã€‚
-
-#### å¾—ç‚¹æœ€å¤§å€¤
-
-    æœ¬ä½œã§ã¯ç‚¹ã‚¢ã‚¤ãƒ†ãƒ å…¥æ‰‹æ™‚ã®æœ€å¤§å€¤ãŒã€äº¤ä¿¡å¼·åº¦ã®è¿‘ãã€ç”»é¢å·¦ä¸‹ã«æ•°å­—ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚åŸºæœ¬çš„ã«ã¯æœ€å¤§å¾—ç‚¹å€¤ã¨äº¤ä¿¡å¼·åº¦ã®å€¤ã®[ç©](/wiki/%E7%A9%8D "ç©")ãŒã€ç‚¹ã‚¢ã‚¤ãƒ†ãƒ å–å¾—æ™‚ã®å®Ÿéš›ã®ç‚¹æ•°ã¨ãªã‚‹ã€‚
-    ã“ã®å€¤ã¯ã€æ•µã‚’å€’ã—ãŸã¨ãã«æ”¾å‡ºã•ã‚Œã‚‹ã€Œå¾—ç‚¹æœ€å¤§å€¤å¢—åŠ ã‚¢ã‚¤ãƒ†ãƒ ã€ã‚’å…¥æ‰‹ã™ã‚‹ã“ã¨ã§å¢—åŠ ã™ã‚‹ã¨ã•ã‚Œã¦ã„ã‚‹ã€‚
-
-## ã‚ã‚‰ã™ã˜
-
-
-é›ªã®é™ã‚‹å†¬ã®ã‚ã‚‹æ—¥ã€åšéº—ç¥ç¤¾ã®è¿‘ãã«çªå¦‚é«˜æ¸©ã®[é–“æ¬ æ³‰](/wiki/%E9%96%93%E6%AC%A0%E6%B3%89 "é–“æ¬ æ³‰")ãŒå™´å‡ºã—ãŸ[1]ã€‚åšéº—ç¥ç¤¾ã®å·«å¥³ã§ã‚ã‚‹[åšéº—éœŠå¤¢](/wiki/%E5%8D%9A%E9%BA%97%E9%9C%8A%E5%A4%A2 "åšéº—éœŠå¤¢")ã¯ã€[æ¸©æ³‰](/wiki/%E6%B8%A9%E6%B3%89 "æ¸©æ³‰")ã«ãªã‚Œã°å‚æ‹å®¢ãŒå¢—ãˆã‚‹ã¯ãšã ã¨å–œã‚“ã§ã„ãŸãŒã€æ¸©æ³‰æ°´ã¨ã¨ã‚‚ã«åœ°åº•ã®æ‚ªéœŠã¾ã§æ¹§ãå‡ºã‚‹[1]ã€‚éœŠå¤¢ã¯æ…Œã¦ãŸã‚‚ã®ã®ã€åœ°éœŠã¯å¤§äººã—ã‹ã£ãŸãŸã‚ã€æ¸©æ³‰ã‚’å–ã‚Šç•°å¤‰ã‚’è§£æ±ºã—ã‚ˆã†ã¨ã¯æ€ã‚ãªã‹ã£ãŸ[2]ã€‚ã—ã‹ã—ã€é­”å¥³ã®ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸ã¯åœ°ä¸‹ã®å¦–æ€ªã‚„åœ°éœŠãŒè¡¨ã«å‡ºã‚‹ã“ã¨ã«å±é™ºã‚’æ„Ÿã˜ã€å¤ã„å¦–æ€ªã§ã‚ã‚‹å…«é›²ç´«ã«ç›¸è«‡ã™ã‚‹ã€‚åœ°åº•ã«ã¯åœ°åº•ã®ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚Šã€åœ°ä¸Šã®å¦–æ€ªãŒåœ°åº•ã«å¹²æ¸‰ã™ã‚‹ã“ã¨ã¯å¥½ã¾ã—ããªã„ã¨ã®åˆ¤æ–­ã‹ã‚‰ã€ç´«ã¯ãƒ‘ãƒãƒ¥ãƒªãƒ¼ã«éœŠå¤¢ãŸã¡äººé–“ã‚’åœ°åº•ã¸é€ã‚Šå‡ºã™ã“ã¨ã‚’ç´„æŸã—ã€å¦–æ€ªãŸã¡ã¯ãã‚Œã‚’é éš”ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã¨ãªã£ãŸ[1]ã€‚ 
-
-åœ°åº•ã«æ½œã£ãŸéœŠå¤¢ãŸã¡ã¯ã€é–“æ¬ æ³‰ã®åŸå› ãŒå¦–æ€ªã®éœŠçƒè·¯ç©ºã®ä»•æ¥­ã§ã‚ã‚‹ã“ã¨ã‚’çªãæ­¢ã‚ã‚‹ã€‚åœ°ä¸Šã®ç¥ã€…ã‹ã‚‰å¼·åŠ›ãª[æ ¸èåˆ](/wiki/%E6%A0%B8%E8%9E%8D%E5%90%88 "æ ¸èåˆ")ã®åŠ›ï¼ˆé–“æ¬ æ³‰ã¯ãã®ç†±ã«ã‚ˆã‚‹å‰¯æ¬¡çš„ãªã‚‚ã®ï¼‰ã‚’æ‰‹ã«å…¥ã‚ŒãŸç©ºã¯åœ°ä¸Šã®ä¾µç•¥ã‚’ä¼ã‚“ã§ã„ãŸãŒã€éœŠå¤¢ãŸã¡ã«æ‡²ã‚‰ã—ã‚ã‚‰ã‚Œã€æ”¹å¿ƒã—ãŸã€‚ã—ã‹ã—ç©ºã®æ ¸èåˆã®åŠ›ã¯ãã®ã¾ã¾ã ã£ãŸãŸã‚ã€åœ°éœŠã¯æ­¢ã¾ã£ãŸãŒé–“æ¬ æ³‰ãŒæ­¢ã‚€ã“ã¨ã¯ãªã‹ã£ãŸã€‚ 
-
-å¾Œã«éœŠå¤¢ãŸã¡ã¯ã€ç©ºã®è©±ã‹ã‚‰ã€ç©ºã«åŠ›ã‚’æˆã‘ãŸã®ã¯å®ˆçŸ¢ç¥ç¤¾ã®ç¥ã€…ã§ã¯ãªã„ã‹ã¨ç–‘ã„ã€çœŸç›¸ã‚’ç¢ºã‹ã‚ã‚‹ãŸã‚ã«å®ˆçŸ¢ç¥ç¤¾ã¸å‘ã‹ã†ã€‚ãã“ã§ã€å®ˆçŸ¢ã®1æŸ±ã§ã‚ã‚‹æ´©çŸ¢è«è¨ªå­ã‹ã‚‰äº‹ã®é¡›æœ«ã‚’èãã“ã¨ã«ãªã‚‹ã€‚ 
-
-## ç™»å ´äººç‰©
-
-
-â†’ã€Œ[æ±æ–¹Projectã®ç™»å ´äººç‰©](/wiki/%E6%9D%B1%E6%96%B9Project%E3%81%AE%E7%99%BB%E5%A0%B4%E4%BA%BA%E7%89%A9 "æ±æ–¹Projectã®ç™»å ´äººç‰©")ã€ãŠã‚ˆã³ã€Œ[å¹»æƒ³éƒ·](/wiki/%E5%B9%BB%E6%83%B3%E9%83%B7 "å¹»æƒ³éƒ·")ã€ã‚‚å‚ç…§
-
-### æ–°è¦ã®ç™»å ´äººç‰©
-
-
-ã“ã“ã§ã¯ã€ã€åœ°éœŠæ®¿ã€ãŒåˆå‡ºã®ç™»å ´äººç‰©ã‚’è§£èª¬ã™ã‚‹ã€‚ 
-
-#### ã‚­ã‚¹ãƒ¡
-
-    [é‡£ç“¶è½ã¨ã—](/wiki/%E9%87%A3%E7%93%B6%E8%90%BD%E3%81%A8%E3%81%97 "é‡£ç“¶è½ã¨ã—")ã€‚ç‹­ã„æ‰€ãŒå¥½ããªå¦–æ€ªã§ã€é‡£ç“¶ã®ä¸­ã«å…¥ã£ãŸçŠ¶æ…‹ã§ç™»å ´ã™ã‚‹ã€‚å¤–è¦‹ã«åã—ã¦å‡¶æš´ãªå¦–æ€ªã§ã‚ã‚Šã€è¿‘ã¥ãäººé–“ã®é¦–ã‚’å•ç­”ç„¡ç”¨ã§åˆˆã‚Šå–ã‚Šã€ãã®ã¾ã¾æ¡¶ã«å…¥ã‚Œã¦æŒã¡å»ã£ã¦ã—ã¾ã†ã¨ã•ã‚Œã‚‹[3]ã€‚ã•ã‚‰ã«ã€Œæ–‡ã€…ã€‚æ–°èã€ã«ã‚‚ã€å½¼å¥³ã¨æ€ã‚ã‚Œã‚‹é‡£ç“¶è½ã¨ã—ãŒèµ·ã“ã—ãŸæ€ªäº‹ä»¶ãŒæ²è¼‰ã•ã‚Œã¦ã„ã‚‹[4]ã€‚
-
-#### é»’è°· ãƒ¤ãƒãƒ¡ï¼ˆãã‚ã ã« ã‚„ã¾ã‚ï¼‰
-
-    [åœŸèœ˜è››](/wiki/%E5%9C%9F%E8%9C%98%E8%9B%9B "åœŸèœ˜è››")ã€‚èœ˜è››ã®å§¿ã‚’ã—ãŸå¦–æ€ªã€‚**ç—…æ°—ã‚’æ“ã‚‹ç¨‹åº¦ã®èƒ½åŠ›** ã‚’æŒã£ã¦ã„ã‚‹ã€‚å½¼å¥³ã«é­é‡ã—ãŸäººé–“ã¯é«˜ã„é »åº¦ã§é‡åº¦ã®ç†±ç—…ã‚’ç™ºç—‡ã™ã‚‹ã¨ã„ã†[5]ã€‚
-    å¦–æ€ªã®å±±ã®éº“ã«ã‚ã‚‹åœ°åº•ã¸ã¨ç¶šãé¢¨ç©´ã‚„æ—§éƒ½ã®å‘¨è¾ºã«ä½ã‚“ã§ã„ã‚‹ã€‚äººé–“ã«ã¨ã£ã¦ã¯å±é™ºãªå¦–æ€ªã ãŒã€æ€§æ ¼ã¯æ˜ã‚‹ãè¦ªå¯†ã«ãªã‚Œã°æ¥½ã—ã„ç›¸æ‰‹ã§åœ°ä¸‹ã®å¦–æ€ªãŸã¡ã®äººæ°—è€…ã§ã‚ã‚Šã€ã€åœ°éœŠæ®¿ã€ä½œä¸­ã§ã¯æ´çªŸã«ä¹—ã‚Šè¾¼ã‚“ã éœŠå¤¢ãŸã¡ã«ã‚‚æ°—ã•ããªå£èª¿ã§è©±ã—ã‹ã‘ã¦ã„ã‚‹ã€‚ã¾ãŸã€æˆ¦ã†ã“ã¨ã‚’å­ã‚ãšå¥½æˆ¦çš„ã ãŒã€å¤§å‹¢ã®äººé–“ã‚’ç›¸æ‰‹ã«ã™ã‚Œã°å‹ã¡ç›®ãŒãªã„ã“ã¨ã‚‚ç†è§£ã—ã¦ã„ã‚‹ã€‚
-    å»ºç¯‰ãŒå¾—æ„ã¨ã•ã‚Œã€åœ°ä¸Šã®å¦–æ€ªã‹ã‚‰ã®ä¾é ¼ã‚’å—ã‘ã¦å¤œã®é–“ã«åœ°ä¸Šã«ç¾ã‚Œã€ä¸€æ™©ã®ã†ã¡ã«å»ºç¯‰ä½œæ¥­ã‚’è¡Œã„ã€å†ã³åœ°åº•ã¸ã¨æˆ»ã£ã¦ã„ãã¨ã„ã†[5]ã€‚æ²³ç«¥ã®æ²³åŸã«ã¨ã‚Šã‹ã‚‰ã¯ã€Œæ²³ã‚’æ±šã™ã€ã¨ã„ã†ç†ç”±ã§å«Œã‚ã‚Œã¦ã„ã‚‹[6]ã€‚
-
-#### æ°´æ©‹ ãƒ‘ãƒ«ã‚¹ã‚£ï¼ˆã¿ãšã¯ã— ãƒ‘ãƒ«ã‚¹ã‚£ï¼‰
-
-    [æ©‹å§«](/wiki/%E6%A9%8B%E5%A7%AB "æ©‹å§«")ã€‚åœ°ä¸Šã¨åœ°ä¸‹ã‚’çµã¶ç¸¦ç©´ã®ç•ªäººã§ã€ç©´ã‚’é€šéã™ã‚‹è€…ã‚’è¦‹å®ˆã‚‹å½¹å‰²ã‚’æŒã¤ã€‚éå¸¸ã«å«‰å¦¬æ·±ã„æ€§æ ¼ã§ã€ã€åœ°éœŠæ®¿ã€ä½œä¸­ã§ã¯åœ°ä¸Šã®æ”¯æ´å¦–æ€ªã‹ã‚‰ã€Œå«‰å¦¬ã®å¦–æ€ªã€ã‚„ã€Œä¸‹è³¤ãªå¦–æ€ªã€ã¨å‘¼ã°ã‚Œã‚‹ã€‚éœŠå¤¢ã‚„é­”ç†æ²™ã«å¯¾ã—ã¦ã€ä¸€æ–¹çš„ã«ã€Œå¦¬ã¾ã—ã„ã€ã¨è¨€ã„ãªãŒã‚‰æ”»æ’ƒã‚’ä»•æ›ã‘ã¦ãã‚‹ã€‚
-    ã€æ±‚èå£æˆã€ã«ã‚ˆã‚Œã°ã€å½¼å¥³ã®æœ¬è³ªã¯ã€Œå«‰å¦¬ã®æ„Ÿæƒ…ã€ãã®ã‚‚ã®ã§ã‚ã‚Šã€ä»–äººã®å«‰å¦¬å¿ƒã‚’ç…½ã‚‹ã“ã¨ã§ãã®ç”Ÿæ´»ãŒå´©å£Šã™ã‚‹æ§˜ã‚’è¦‹ã‚‹ã®ã‚’å–œã³ã¨ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€ä»–è€…ã‹ã‚‰å«‰å¦¬ã‚’å—ã‘ãŸã‚Šã€ã‚ã‚‹ã„ã¯å½¼å¥³è‡ªèº«ãŒä»–è€…ã«å«‰å¦¬ã™ã‚‹å ´åˆã«ã‚‚åŠ›ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚æ„åœ°ã®æ‚ªã„æ€§æ ¼ã§ã‚ã‚Šã€ç›´æ¥ã€ç›¸å¯¾ã—ã¦ã„ã‚‹éš›ã«ã¯æ™®é€šã«æ˜ã‚‹ãä¼šè©±ã‚’ã™ã‚‹ãŒã€è£ã§ã¯ãã®ç›¸æ‰‹ã®é™°å£ã‚’å©ã„ãŸã‚Šé€†æ¨ã¿ã—ãŸã‚Šã™ã‚‹ãŸã‚ã€å«Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚ãŸã ã—ã€æ—§åœ°ç„ã«ã¯å«Œã‚ã‚Œè€…åŒå£«ã®ä»²é–“ã‚‚å¤šã„ã¨ã„ã†[7]ã€‚
-
-#### æ˜Ÿç†Š å‹‡å„€ï¼ˆã»ã—ãã¾ ã‚†ã†ãï¼‰
-
-    æ—§éƒ½ã«ä½ã‚€[é¬¼](/wiki/%E9%AC%BC "é¬¼")ã€‚é¡ã«ä¸€æœ¬ã®èµ¤ã„è§’ãŒç”Ÿãˆã¦ãŠã‚Šã€è§’ã®ä¸Šé¢ã«ã¯é»„è‰²ã„æ˜Ÿã®ãƒãƒ¼ã‚¯ãŒã¤ã„ã¦ã„ã‚‹ã€‚ã€åœ°éœŠæ®¿ã€ã§ã¯ä½“æ“æœã®ä¸Šéƒ¨åˆ†ã«åŠé€æ˜ã®ã‚¹ã‚«ãƒ¼ãƒˆã‚’ç€ç”¨ã—ã¦ã„ã‚‹ãŒã€é»„æ˜ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ä½œå“ã§ã¯è‚©ã¨èƒ¸ã‚’ã¯ã ã‘ã•ã›ãŸç€ç‰©å§¿ã§ç™»å ´ã—ã¦ã„ã‚‹ã€‚
-    ã€åœ°éœŠæ®¿ã€ã§ã¯ã€åœ°åº•ã«ç¾ã‚ŒãŸåšéº—éœŠå¤¢ã‚„éœ§é›¨é­”ç†æ²™ã«èˆˆå‘³ã‚’æŒã¡ã€åŠ›è©¦ã—ã¨ç§°ã—ã¦å¯¾æˆ¦ã™ã‚‹ã€‚é…’ã‚’ä¸€æ»´ã‚‚ã“ã¼ã•ãšã«æˆ¦ã†ãƒ«ãƒ¼ãƒ«ã‚’è‡ªåˆ†ã«èª²ã—ã¦ã€éŠã³ãªãŒã‚‰æˆ¦ã£ã¦ã„ã‚‹[8]ã€‚å¯¾æˆ¦å¾Œã¯ç•°å¤‰ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æä¾›ã—ã€åœ°éœŠæ®¿ã¸æ¡ˆå†…ã™ã‚‹ã€‚
-    ã‹ã¤ã¦ã¯å¦–æ€ªã®å±±ã«ä½ã‚“ã§ãŠã‚Šã€ä¼Šå¹èƒé¦™ã‚‰ã¨ã¨ã‚‚ã«ã€Œå±±ã®å››å¤©ç‹ã€ã¨å‘¼ã°ã‚Œã€å¤©ç‹—ã‚„æ²³ç«¥ã‚’å¾“ãˆä¸€å¤§ç¤¾ä¼šã‚’ç¯‰ã„ã¦ã„ãŸã€‚ã—ã‹ã—ã€äººé–“ã¨ã®é–¢ä¿‚ã®æ‚ªåŒ–ã‚’æ†‚ã„ã€åŒã˜ãåœ°ä¸Šã«å«Œæ°—ã®å·®ã—ãŸä»–ã®é¬¼ãŸã¡ã¨å…±ã«ã€çªç„¶å§¿ã‚’æ¶ˆã™ã€‚ãã®å¾Œã€åœ°ç„ã®ã€ŒçµŒå–¶ã®ã‚¹ãƒªãƒ åŒ–ã€ã®ä¸€ç’°ã¨ã—ã¦åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚Œã€å»ƒå¢Ÿã¨ãªã£ãŸæ—§åœ°ç„è·¡ã«ç§»ã‚Šä½ã¿ã€åŒã˜ããã®èƒ½åŠ›ã®å±é™ºæ€§ãªã©ã‹ã‚‰å¿Œã¿å«Œã‚ã‚ŒãŸä»–ã®å¦–æ€ªãŸã¡ã¨å…±ã«åœ°ä¸‹éƒ½å¸‚ã‚’å†å»ºã™ã‚‹ã€‚å¾Œã«ã€åœ°ä¸Šã®è³¢è€…ã¨ã®é–“ã«ã€Œåœ°ä¸Šã¨åœ°åº•ã®å¦–æ€ªåŒå£«ã®ç›¸äº’ä¸å¯ä¾µã€ã€Œæ—§åœ°ç„ã®æ€¨éœŠã®ç®¡ç†ã€ãªã©ã®ç´„æŸã‚’çµã³è‡ªæ²»ã‚’èªã‚ã‚‰ã‚Œã€ç¾åœ¨ã«è‡³ã‚‹ã€‚
-    ã€æ±‚èå£æˆã€ã«ã‚ˆã‚Œã°ã€è±ªå¿«ã‹ã¤ç«¹ã‚’å‰²ã£ãŸã‚ˆã†ãªæ€§æ ¼ã§ã€ã€ŒåŠ›å¼·ã„è€…ã€ã€Œå‹‡æ°—ã‚ã‚‹è€…ã€ã‚’å¥½ã¿ã€ã€Œè»Ÿå¼±ãªè€…ã€ã€Œå‘æ€¯ãªè€…ã€ã‚’å«Œã†ã€‚å¹»æƒ³éƒ·æœ€å¼·ã®ç¨®æ—ã¨ã•ã‚Œã‚‹é¬¼ã®ä¸­ã§ã‚‚å±ˆæŒ‡ã®æ€ªåŠ›ã®æŒã¡ä¸»ã§ã€å‹äººã§ã‚ã‚‹èƒé¦™ã¯ã€Œè‚‰ä½“ã‚’ä½¿ã£ãŸåŠ›ã¯è‡ªåˆ†ã‚ˆã‚Šå¼·ã„ã‹ã‚‚ã€ã¨è¿°ã¹ã¦ã„ã‚‹ã€‚ã¾ãŸã€ã€Œèªã‚‰ã‚Œã‚‹æ€ªåŠ›ä¹±ç¥ã€ã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã®ãŸã‚ã€åŠ›ã«ã‚ˆã‚‹æ”¯é…ãŒãƒ«ãƒ¼ãƒ«ã¨ã•ã‚Œã¦ã„ã‚‹æ—§åœ°ç„ã«ã¯æ•µãŒå­˜åœ¨ã›ãšã€ã€Œåœ°åº•ä¸–ç•Œã¯ç§é”ã®æ¥½åœ’ã€ã¨èªã£ã¦ã„ã‚‹[9]ã€‚æ‰‹åŠ æ¸›ã—ãŸã¨ã¯ã„ãˆã€äººé–“ã§ã‚ã‚ŠãªãŒã‚‰è‡ªåˆ†ã«å‹åˆ©ã—ãŸéœŠå¤¢ãŸã¡ã‚’æ°—ã«å…¥ã£ãŸã‚‰ã—ãã€é­”ç†æ²™ã‚’åœ°åº•ã®å®´ä¼šã«ã‚ˆãèª˜ã£ã¦ã„ã‚‹[10]ã€‚
-    æ˜Ÿã®ãƒãƒ¼ã‚¯ãŒå…¥ã£ãŸå·¨å¤§ãªèµ¤ã„ç›ƒã‚’æŒã¡æ­©ã„ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€Œæ˜Ÿç†Šç›ƒã€ã¨å‘¼ã°ã‚Œã‚‹é¬¼ã®åå“ã®ä¸€ã¤ã§ã‚ã‚Šã€æ³¨ãŒã‚ŒãŸé…’ã‚’ä¸€ç¬ã«ã—ã¦æœ€é«˜ãƒ©ãƒ³ã‚¯ã®ç´”ç±³å¤§åŸé†¸ã«å¤‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹[9]ã€‚
-
-#### å¤æ˜åœ° ã•ã¨ã‚Šï¼ˆã“ã‚ã„ã˜ ã•ã¨ã‚Šï¼‰
-
-    [ã•ã¨ã‚Š](/wiki/%E8%A6%9A "è¦š")ã€‚æ—§ç¼ç†±ç„è·¡ã®ä¸Šã«å»ºã¦ã‚‰ã‚ŒãŸã€Œåœ°éœŠæ®¿ã€ã®ä¸»ã§ã‚ã‚Šã€å¤æ˜åœ°ã“ã„ã—ã®å§‰ã€‚
-    ã€Œ**å¿ƒã‚’èª­ã‚€ç¨‹åº¦ã®èƒ½åŠ›** ã€ã‚’æŒã¡ã€å·¦èƒ¸éƒ¨ã®ã€Œã‚µãƒ¼ãƒ‰ãƒ»ã‚¢ã‚¤ã€ã§ç›¸æ‰‹ã®å¿ƒã‚’èª­ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚ä»–äººã®å¿ƒã‚’è¦‹é€ã‹ã™èƒ½åŠ›ã‚†ãˆã«ã€å«Œã‚ã‚Œè€…ã¨ã—ã¦åœ°åº•ã«å°ã˜ã‚‰ã‚ŒãŸå¦–æ€ªã®ä¸­ã§ã‚‚ç¾¤ã‚’æŠœã„ã¦æã‚Œã‚‰ã‚Œã¦ã„ã‚‹å­˜åœ¨ã§ã‚ã‚Šã€æ—§åœ°ç„ã«ãŠã„ã¦å±ˆæŒ‡ã®å®ŸåŠ›ã‚’æŒã¤å¤§ç‰©ã§ã‚‚ã‚ã‚‹ã€‚ãŸã ã—ã€æˆ¦é—˜ã¯ä½™ã‚Šå¾—æ„ã§ã¯ãªã„ã‚‰ã—ã„[11]ã€‚å¦¹ã®ã“ã„ã—æ›°ãã€ŒãŠå§‰ã¡ã‚ƒã‚“ã®çŸ¥ã‚Šåˆã„ã ã¨è¨€ãˆã°ã€åœ°åº•ã§ã¯èª°ã‚‚é€†ã‚‰ã‚ãªã„ã€ã€‚ã¾ãŸã€è¨€è‘‰ã‚’æŒãŸãªã„å¹½éœŠã‚„æ€¨éœŠã‹ã‚‰ã‚‚è‹¦æ‰‹ã¨ã•ã‚Œã¦ãŠã‚Šã€ã“ã®èƒ½åŠ›ã‚’æ´»ã‹ã—ã¦é–»é­”ã‹ã‚‰ç¼ç†±åœ°ç„è·¡ã®æ€¨éœŠã®ç®¡ç†ã‚’ä»»ã•ã‚Œã¦ã„ã‚‹ã€‚ã€æ±‚èå£æˆã€ã§ã¯ã€ç¥ã§ã‚ã‚‹å…«å‚ç¥å¥ˆå­ãŒéœŠçƒè·¯ç©ºã«åŠ›ã‚’æˆã‘ã‚‹éš›ã«ã‚‚ã€å½¼å¥³ã¨ã®æ¥è§¦ã‚’é¿ã‘ã‚‹ã¹ãæ³¨æ„ã‚’æ‰•ã£ã¦ã„ãŸã¨ã•ã‚Œã¦ã„ã‚‹[12]ã€‚
-    å½¼å¥³è‡ªèº«ã‚‚è‡ªåˆ†ãŒå¿Œã¿å«Œã‚ã‚Œã‚‹å­˜åœ¨ã§ã‚ã‚‹ã“ã¨ã‚’ç†è§£ã—ã¦ã„ã‚‹ãŸã‚ã€ä»–è€…ã¨ã®æ¥è§¦ã‚„ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ‹’çµ¶ã—ã€ä½å±…ã§ã‚ã‚‹åœ°éœŠæ®¿ã«å¼•ãç± ã‚‚ã£ã¦ã„ã‚‹ã€‚ãã®ä»£ã‚ã‚Šã€è¨€è‘‰ã‚’è©±ã›ãªã„å‹•ç‰©ã‹ã‚‰ã¯å¥½ã‹ã‚Œã¦ã„ã‚‹ã‚‰ã—ãã€åœ°éœŠæ®¿ã«ã¯å½¼å¥³ã‚’æ…•ã†ãƒšãƒƒãƒˆãŸã¡ãŒæ•°å¤šãä½ã‚“ã§ã„ã‚‹ã€‚ãã®ä¸­ã«ã¯æ€¨éœŠã‚„[é­‘é­…é­é­](/wiki/%E9%AD%91%E9%AD%85%E9%AD%8D%E9%AD%8E "é­‘é­…é­é­")ã‚’å–°ã‚‰ã†ã“ã¨ã§åŠ›ã‚’ã¤ã‘ã€å¦–æ€ªåŒ–ã—ãŸè€…ãŸã¡ã‚‚ã„ã‚‹ã€‚ãã®ãŸã‚ã€æ™®æ®µã¯å±‹æ•·ã‚„æ—§åœ°ç„ã®ç®¡ç†ã€å¦¹ã‚„ä»–ã®ãƒšãƒƒãƒˆã®ä¸–è©±ãªã©ã‚’å½¼å¥³ã‚‰ã«ä»»ã›ã€è‡ªåˆ†ã¯èª­æ›¸ã‚’ã—ãŸã‚Šå°èª¬ã‚’æ›¸ã„ãŸã‚Šã—ã¦æš®ã‚‰ã—ã¦ã„ã‚‹[11]ã€‚
-    ã€åœ°éœŠæ®¿ã€ã§ã¯ã€åœ°éœŠæ®¿ã‚’è¨ªã‚ŒãŸåšéº—éœŠå¤¢ã‚„éœ§é›¨é­”ç†æ²™ã®å¿ƒã‚’èª­ã‚“ã§åœ°åº•ã‚’è¨ªã‚ŒãŸç›®çš„ã‚’æ¢ã‚ã†ã¨ã™ã‚‹ãŒã€ç•°å¤‰ã®è§£æ±ºã«æ¶ˆæ¥µçš„ã ã£ãŸ2äººã‹ã‚‰ã¯ç›®çš„ã‚’ä¸Šæ‰‹ãæ¢ã‚‹ã“ã¨ãŒã§ããªã‹ã£ãŸã€‚è¨€å‹•ã¨è€ƒãˆã®ä¸€è‡´ã—ãªã„2äººã‚’ä¸å¯©ã«æ€ã„ã€å¯¾æˆ¦ã™ã‚‹ã€‚ãã®å¾Œã¯ãƒšãƒƒãƒˆã®ã„ã‚‹ç¼ç†±åœ°ç„è·¡ã¸æ¡ˆå†…ã™ã‚‹ã€‚å¯¾æˆ¦æ™‚ã¯ã€è‡ªèº«ã®èƒ½åŠ›ã§éœŠå¤¢ã‚„é­”ç†æ²™ã®è¨˜æ†¶ã®ä¸­ã«ã‚ã‚‹ã€Œ[ãƒˆãƒ©ã‚¦ãƒ](/wiki/%E3%83%88%E3%83%A9%E3%82%A6%E3%83%9E "ãƒˆãƒ©ã‚¦ãƒ")ã€ã‚’èª­ã¿å–ã‚Šã€ãã‚Œã‚’å†ç¾ã—ãŸæ”»æ’ƒã‚’è¡Œã†ã€‚å…·ä½“çš„ãªä½œä¸­æå†™ã¨ã—ã¦ã¯ã€ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã—ã¦é¸æŠã—ãŸå¦–æ€ªãŒéå»ã®ä½œå“ã§ä½¿ç”¨ã—ãŸã‚¹ãƒšãƒ«ã‚«ãƒ¼ãƒ‰ã‚’çœŸä¼¼ãŸã‚‚ã®ã‚’ã€ã•ã¨ã‚Šè‡ªèº«ã®ã‚¹ãƒšãƒ«ã‚«ãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
-
-#### ç«ç„”çŒ« ç‡ï¼ˆã‹ãˆã‚“ã³ã‚‡ã† ã‚Šã‚“ï¼‰
-
-    [ç«è»Š](/wiki/%E7%81%AB%E8%BB%8A_\(%E5%A6%96%E6%80%AA\) "ç«è»Š \(å¦–æ€ª\)")ã€‚å¤æ˜åœ°ã•ã¨ã‚Šã®ãƒšãƒƒãƒˆã®1äººã§ã‚ã‚Šã€ç¼ç†±åœ°ç„è·¡ã§æ€¨éœŠã®ç®¡ç†ã‚„æ­»ä½“é‹ã³ã‚’ä»»ã•ã‚Œã¦ã„ã‚‹ã€‚ç¼ç†±åœ°ç„è·¡ãŒæœ¬å½“ã®åœ°ç„ã ã£ãŸé ƒã‹ã‚‰ç”Ÿãã¦ãŠã‚Šã€åŠªåŠ›ã®æœ«ã«æ­»ä½“ã‚„æ€¨éœŠã‚’æ“ã‚‹èƒ½åŠ›ã‚’ä¼šå¾—ã—[13]ã€å½¼ã‚‰ã¨ä¼šè©±ãƒ»æ„æ€ç–é€šãŒã§ãã‚‹ã€‚ã•ã¨ã‚Šã®ãƒšãƒƒãƒˆã¨ãªã£ãŸæ™‚æœŸã¯åœ°åº•ç•ŒãŒåœ°ç„ã‹ã‚‰åˆ‡ã‚Šé›¢ã•ã‚ŒãŸé ƒã§ã€åŒã˜ããƒšãƒƒãƒˆã®éœŠçƒè·¯ç©ºã¨ã¯ãã®é ƒã‹ã‚‰ã®å¤ã„å‹äººã€‚
-    åŠ›ã‚’æ‰‹ã«å…¥ã‚Œã¦èª¿å­ã«ä¹—ã‚‹ç©ºã«å‘†ã‚ŒãªãŒã‚‰ã‚‚ã€æš´èµ°ãŒä¸»äººã®ã•ã¨ã‚Šã‚„æ—§åœ°ç„ã®ä½äººã«çŸ¥ã‚‰ã‚Œã¦å½¼å¥³ãŒå‡¦ç½°ã•ã‚Œã‚‹ã“ã¨ã‚’æã‚Œã€åœ°ä¸Šã«æ€¨éœŠã‚’é€ã‚Šè¾¼ã‚€ã“ã¨ã§åœ°ä¸Šã®å¦–æ€ªã«ç•°å¤‰ã‚’çŸ¥ã‚‰ã›ã€ç©ºã‚’æ­¢ã‚ã•ã›ã‚ˆã†ã¨è©¦ã¿ãŸã€‚ã—ã‹ã—ã€æ„ã«åã—ã¦ç¾ã‚ŒãŸã®ã¯äººé–“ã ã£ãŸãŸã‚ã€ãã®å®ŸåŠ›ã‚’è©¦ã™ãŸã‚ã«å¯¾æˆ¦ã™ã‚‹ã€‚
-    ä½œä¸­ã§ã¯çŒ«ã®é³´ãå£°ã®ã‚ˆã†ãªåŠ¹æœéŸ³ã¨å…±ã«é»’çŒ«ã®å§¿ã§ä½•åº¦ã‚‚ç™»å ´ã—ã€ä¸­ãƒœã‚¹ã¨ã—ã¦å¯¾æˆ¦ã™ã‚‹ã€‚5é¢ãƒœã‚¹æˆ¦å‰ã®ä¼šè©±ã‚¤ãƒ™ãƒ³ãƒˆã§ã€ŒçŒ«ã®å§¿ã§ã¯ä¼šè©±ãŒã§ããªã„ã€ã¨ã—ã¦äººå‹ã«å¤‰èº«ã™ã‚‹ã€‚å¤‰èº«å¾Œã®å§¿ã§ã¯çŒ«è€³ã‚’æŒã¤ãŒã€å´é ­éƒ¨ã«äººã®è€³ã‚‚ä»˜ã„ã¦ã„ã‚‹ã€‚
-    æ­»ä½“ã‚’å¥½ã‚€å¦–æ€ªã§ã‚ã‚‹ç«è»Šã§ã‚ã‚Šã€ã€åœ°éœŠæ®¿ã€ã§ã¯éœŠå¤¢ã‚„é­”ç†æ²™ã®æ­»ä½“ã‚’å¾—ãŸã„ã¨å½“äººã«è©±ã—ã¦ã„ã‚‹ã€‚ã€åœ°éœŠæ®¿ã€ã‚„ã€æ±‚èå£æˆã€ã§ã¯ã€åœ°ä¸Šã§ã¨ãã©ãç™ºç”Ÿã™ã‚‹äººé–“ã®æ­»ä½“ãŒç›—ã¾ã‚Œã‚‹äº‹ä»¶ã®çŠ¯äººãŒãŠç‡ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹[14]ã€‚èƒé¦™ã®è¨€ã«ã‚ˆã‚Œã°ã€å½¼å¥³ã«æ­»ä½“ã‚’å¥ªã‚ã‚ŒãŸæ­»è€…ã¯ãã®ã¾ã¾æ€¨éœŠã¨åŒ–ã—ã€ã‚ã®ä¸–ã«è¡Œãã“ã¨ã‚‚ã§ããªããªã£ã¦ã—ã¾ã†ã¨ã„ã†ã€‚æ­»ä½“ã¯æœ€çµ‚çš„ã«ã¯ã€Œç‡ƒæ–™ã€ã¨ã—ã¦ç¼ç†±åœ°ç„ã®ç‚ã®ä¸­ã¸æ”¾ã‚Šè¾¼ã‚“ã§ã—ã¾ã†[_[è¦å‡ºå…¸](/wiki/Wikipedia:%E3%80%8C%E8%A6%81%E5%87%BA%E5%85%B8%E3%80%8D%E3%82%92%E3%82%AF%E3%83%AA%E3%83%83%E3%82%AF%E3%81%95%E3%82%8C%E3%81%9F%E6%96%B9%E3%81%B8 "Wikipedia:ã€Œè¦å‡ºå…¸ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸæ–¹ã¸")_]ã€‚
-    è‡ªåˆ†ã®æœ¬åãŒé•·ã„ã“ã¨ã‚’å«Œã£ã¦ãŠã‚Šã€çš†ã«ã€ŒãŠç‡ã€ã¨å‘¼ã°ã›ã¦ã„ã‚‹ã€‚ä¼šè©±ã‚¤ãƒ™ãƒ³ãƒˆã§ã‚‚é€šç§°ã§ã‚ã‚‹ã€ŒãŠç‡ã€ã¨ã„ã†åå‰ãŒè¡¨ç¤ºã•ã‚Œã€ä½œä¸­ã§ã¯æœ¬åãŒè¡¨ç¤ºã•ã‚Œãªã„ã€‚æœ¬åã¯ã€åœ°éœŠæ®¿ã€ä»˜å±ã®ã€Œã‚­ãƒ£ãƒ©è¨­å®š.txtã€ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚ã€ãƒ€ãƒ–ãƒ«ã‚¹ãƒã‚¤ãƒ©ãƒ¼ã€ã§ã‚‚ã€ŒãŠç‡ã€åç¾©ã§ç™»å ´ã™ã‚‹ã€‚
-
-#### éœŠçƒè·¯ ç©ºï¼ˆã‚Œã„ã†ã˜ ã†ã¤ã»ï¼‰
-
-    åœ°ç„ã®å¦–æ€ª[å›ºæœ‰ç¨®](/wiki/%E5%9B%BA%E6%9C%89%E7%A8%AE "å›ºæœ‰ç¨®")ã§ã‚ã‚‹ã€Œåœ°ç„çƒã€[15]ã€‚å¤æ˜åœ°ã•ã¨ã‚Šã®ãƒšãƒƒãƒˆã§ã€ãŠç‡ã¨å…±ã«ç¼ç†±åœ°ç„è·¡ã®ç®¡ç†ã‚’ä»»ã•ã‚Œã¦ãŠã‚Šã€ç©ºã¯ç«åŠ›èª¿ç¯€ã‚’æ‹…å½“ã™ã‚‹ã€‚
-    ã‚ã‚‹æ—¥ã€å¹»æƒ³éƒ·ã®ç”£æ¥­é©å‘½è¨ˆç”»ã‚’è¨ˆã£ãŸå…«å‚ç¥å¥ˆå­ã¨æ´©çŸ¢è«è¨ªå­ã®2æŸ±ã«ã‚ˆã£ã¦å¤ªé™½ã®åŒ–èº«ã§ã‚ã‚‹ç¥éœŠã€Œ[å…«å’«çƒ](/wiki/%E5%85%AB%E5%92%AB%E7%83%8F "å…«å’«çƒ")ã€ã®åŠ›ã‚’ä¸ãˆã‚‰ã‚Œã€ã€Œ**æ ¸èåˆã‚’æ“ã‚‹ç¨‹åº¦ã®èƒ½åŠ›** ã€ã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚ã—ã‹ã—ã€å½¼å¥³ã¯ãã®å¼·å¤§ãªåŠ›ã«æººã‚Œã¦èƒ½åŠ›ã‚’æ¿«ç”¨ã—ã€é‚ã«ã¯é–“æ¬ æ³‰ã‚’å™´å‡ºã•ã›ã‚‹ç•°å¤‰ã‚’å¼•ãèµ·ã“ã™ã€‚ãã®å¾Œã€ç•°å¤‰è§£æ±ºã®ãŸã‚ã«ç¼ç†±åœ°ç„è·¡ã‚’è¨ªã‚ŒãŸéœŠå¤¢ã‚„é­”ç†æ²™ã«å¯¾ã—ã¦ã€Œåœ°ä¸Šã¸é€²å‡ºã—ã¦ä¸–ç•Œã‚’ç¼ç†±åœ°ç„ã«å¤‰ãˆã‚‹ã€ã¨ã„ã†é‡æœ›ã‚’æ˜ã‹ã—å‹è² ã‚’æŒ‘ã‚€ãŒã€æ’ƒé€€ã•ã‚Œæ”¹å¿ƒã™ã‚‹ã€‚å¾Œã«ãŠç‡ã¨å…±ã«åšéº—ç¥ç¤¾ã‚’è¨ªã‚Œã€ã€ŒäºŒäººçµ„ã®ç¥æ§˜ã€ã‹ã‚‰èƒ½åŠ›ã‚’ã‚‚ã‚‰ã£ãŸã“ã¨ã‚’éœŠå¤¢ã¨é­”ç†æ²™ã«å‘Šã’ã‚‹ã€‚ãŠç‡ã‹ã‚‰ã€Œ[é³¥é ­](/wiki/%E9%B3%A5%E9%A0%AD "é³¥é ­")ã€ã¨æ¶æ„ã•ã‚Œã‚‹ã»ã©è¨˜æ†¶åŠ›ã«æ¬ ã‘ã€ç¥ã‹ã‚‰åŠ›ã‚’ä¸ãˆã‚‰ã‚ŒãŸç†ç”±ãªã©ã¯å®Œå…¨ã«è¨˜æ†¶ã‹ã‚‰æŠœã‘è½ã¡ã¦ã„ãŸãŸã‚ã€ã•ã¨ã‚Šã®èƒ½åŠ›ã‚’ä½¿ç”¨ã—ã¦ã‚‚èª­ã¿å–ã‚‹ã“ã¨ãŒã§ããªã‹ã£ãŸã€‚
-    ãã®å¾Œã¯ã€ç¥å¥ˆå­ã®æŒ‡ç¤ºã§åœ°åº•ã«å»ºé€ ã•ã‚ŒãŸæ ¸èåˆç ”ç©¶æ–½è¨­ã§ã‚ã‚‹ã€Œé–“æ¬ æ³‰åœ°ä¸‹ã‚»ãƒ³ã‚¿ãƒ¼ã€ã§ã€ä½•ã‚‰ã‹ã®ä»•äº‹ã‚’ã—ã¦ã„ã‚‹ã€‚ã€éæƒ³å¤©å‰‡ã€ã§ã¯ã€ã‚»ãƒ³ã‚¿ãƒ¼ã«å…¥ã‚Šè¾¼ã‚“ã æ±é¢¨è°·æ—©è‹—ã‚„ãƒãƒ«ãƒã‚’ã€Œç•°ç‰©ã€ã¨ã—ã¦æ’é™¤ã™ã‚‹ãŸã‚ã«ç¾ã‚ŒãŸã€‚
-    å…«å’«çƒã®åŠ›ã‚’å–ã‚Šè¾¼ã‚“ã å½¼å¥³ã¯ã€ãã®å½±éŸ¿ã«ã‚ˆã‚Šå…ƒã®å§¿ã‹ã‚‰å¤§ããå¤‰åŒ–ã—ãŸã¨ã•ã‚Œã‚‹ã€‚å·¦è¶³ã¯é›»å­ã®ã‚ˆã†ãªã‚‚ã®ãŒå‘¨å›²ã‚’æ¼‚ã†ã€Œåˆ†è§£ã®è¶³ã€ã€å³è¶³ã¯é‡‘å±ã®å¡Šã®ã‚ˆã†ãªã€Œèåˆã®è¶³ã€ã€å³æ‰‹ã¯å¤šè§’æŸ±ã®åˆ¶å¾¡æ£’ã§ã‚ã‚‹ã€Œç¬¬ä¸‰ã®è¶³ã€ã¨ãªã‚Šã€ã“ã‚Œã‚‰ã®ã€Œä¸‰æœ¬ã®è¶³ã€ã§æ ¸èåˆåå¿œã‚’æ“ä½œã™ã‚‹ã€‚ã¾ãŸã€èƒ¸ã«ã¯å·¨å¤§ãªé³¥ã®ç›®ã®ã‚ˆã†ãªã€Œèµ¤ã®ç›®ã€ãŒå­˜åœ¨ã™ã‚‹[15]ã€‚å¯¾æˆ¦æ™‚ã«ã¯æ ¸ã®åŠ›ã‚’ä½¿ã£ãŸã‚¹ãƒšãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã€ã‚¹ãƒšãƒ«ã‚«ãƒ¼ãƒ‰ç™ºå‹•æ™‚ã«ã¯ãƒ¡ãƒ«ãƒˆãƒ€ã‚¦ãƒ³ã®ã‚ˆã†ãªã‚¢ãƒ©ãƒ¼ãƒˆã¨å…±ã«ã€Œ[â˜¢](/wiki/%E6%94%BE%E5%B0%84%E7%B7%9A#æ¦‚è¦ "æ”¾å°„ç·š") CAUTION!!ã€ã®æ–‡å­—ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€‚
-    çš†ã‹ã‚‰ã€ŒãŠãã†ã€ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹ã¨ã•ã‚Œ[16]ã€å®Ÿéš›ã«ãŠç‡ã‚„å¤æ˜åœ°ã“ã„ã—ã¯ä½œä¸­ã§ã€ŒãŠãã†ã€ã¨å‘¼ã‚“ã§ã„ã‚‹ãŒã€ãŠç‡ã¨ã¯ç•°ãªã‚Šã€ä¼šè©±ã‚¤ãƒ™ãƒ³ãƒˆã§ã¯æœ¬åãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€‚ã€ãƒ€ãƒ–ãƒ«ã‚¹ãƒã‚¤ãƒ©ãƒ¼ã€ã§ã¯å°„å‘½ä¸¸æ–‡ã‹ã‚‰ã€ŒãŠç©ºã•ã‚“ã€ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹ã€‚
-
-#### å¤æ˜åœ°ã“ã„ã—ï¼ˆã“ã‚ã„ã˜ ã“ã„ã—ï¼‰
-
-    å¤æ˜åœ°ã•ã¨ã‚Šã®å¦¹ã€‚å§‰ã¨åŒã˜ãç¨®æ—ã¯ã€Œã•ã¨ã‚Šã€ã§ã€å…ƒã€…ã¯ã“ã„ã—ã‚‚å¿ƒã‚’èª­ã‚€èƒ½åŠ›ã‚’æŒã£ã¦ã„ãŸãŒã€èƒ½åŠ›ã®ã›ã„ã§çš†ã«å«Œã‚ã‚Œã‚‹ã“ã¨ã‚’çŸ¥ã£ãŸãŸã‚ã€ã‚µãƒ¼ãƒ‰ãƒ»ã‚¢ã‚¤ã‚’é–‰ã˜ã¦èƒ½åŠ›ã‚’å°å°ã—ã€å¿ƒã‚’é–‰ã–ã—ã¦ã—ã¾ã†ã€‚ãã®çµæœã€æœ¬æ¥ã®å¿ƒã‚’èª­ã‚€èƒ½åŠ›ã«ä»£ã‚ã‚Šã€æ–°ãŸã«ã€Œ**ç„¡æ„è­˜ã‚’æ“ã‚‹ç¨‹åº¦ã®èƒ½åŠ›** ã€ã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã£ã¦èª°ã‹ã‚‰ã‚‚æ°—ã¥ã‹ã‚Œãšã«ãƒ•ãƒ©ãƒ•ãƒ©ã¨å‡ºã‹ã‘ã¦ã¯å¸°ã£ã¦ãã‚‹ã¨ã„ã†å¦–æ€ªã¨ãªã£ã¦ã„ã‚‹ã€‚ã•ã‚‰ã«ã€å§‰ã®ã•ã¨ã‚Šã‚‚ã€é–‰ã–ã•ã‚ŒãŸã“ã„ã—ã®å¿ƒã ã‘ã¯èª­ã‚€ã“ã¨ãŒã§ããªããªã£ãŸã€‚ã“ã„ã—ã‚’ä¸æ†«ã«æ€ã£ãŸã•ã¨ã‚Šã‹ã‚‰ã€ã“ã„ã—ã¨éŠã¶ãŸã‚ã®å°‚å±ã®ãƒšãƒƒãƒˆã‚’ä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã€‚ãã®ãŠã‹ã’ã‹ã€å°‘ã—ãšã¤ã§ã¯ã‚ã‚‹ãŒã“ã„ã—ã‚‚ä»¥å‰ã¨ã¯å¤‰ã‚ã£ã¦ããŸã‚ˆã†ã§ã‚ã‚‹ã€‚
-    åšéº—éœŠå¤¢ã‚„éœ§é›¨é­”ç†æ²™ãŒåœ°åº•ã«ã‚„ã£ã¦æ¥ã¦ã•ã¨ã‚Šã‚„ãŠç‡ã€éœŠçƒè·¯ç©ºã¨æˆ¦ã„ã‚’ç¹°ã‚Šåºƒã’ãŸã“ã¨ã‚’èãã€ä¸­ã§ã‚‚å…«å’«çƒã‚’å–ã‚Šè¾¼ã‚“ã ç©ºã®é©šç•°çš„ãªèƒ½åŠ›ã‚¢ãƒƒãƒ—ã«èˆˆå‘³ã‚’ç¤ºã™ã€‚è‡ªåˆ†ã®ãƒšãƒƒãƒˆã‚‚ç©ºã®ã‚ˆã†ã«å¼·åŒ–ã—ã¦ã‚‚ã‚‰ãŠã†ã¨æ€ã£ãŸã“ã„ã—ã¯ã€å¦–æ€ªã®å±±ã®å®ˆçŸ¢ç¥ç¤¾ã‚’ç›®æŒ‡ã™ã“ã¨ã«ã™ã‚‹ã€‚ã€åœ°éœŠæ®¿ã€Extraã‚¹ãƒ†ãƒ¼ã‚¸ã§ã¯ã€å¤©ç‹—ãŒè­¦å‚™ã™ã‚‹å¦–æ€ªã®å±±ã‚’èª°ã«ã‚‚æ°—ä»˜ã‹ã‚Œã‚‹ã“ã¨ãªãä¾µå…¥ã—ã€ãã®å…ˆã§åšéº—éœŠå¤¢ã‚„éœ§é›¨é­”ç†æ²™ã¨é­é‡ã—ã¦ã„ã‚‹ã€‚ä½œä¸­ã§ã¯å°„å‘½ä¸¸æ–‡ã‚„ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸ã‹ã‚‰ã¯ã€å…¨ãæ°—é…ã‚’æ„Ÿã˜ãªã„å­˜åœ¨ã ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã€‚
-
-### æ—¢å­˜ã®ç™»å ´äººç‰©
-
-
-ã“ã“ã§ã¯ã€ã€åœ°éœŠæ®¿ã€ãŒåˆå‡ºã§ã¯ãªã„ç™»å ´äººç‰©ã‚’è§£èª¬ã™ã‚‹ã€‚ 
-
-#### [åšéº—éœŠå¤¢](/wiki/%E5%8D%9A%E9%BA%97%E9%9C%8A%E5%A4%A2 "åšéº—éœŠå¤¢")
-
-    åšéº—ç¥ç¤¾ã®å·«å¥³ã€‚æ¸©æ³‰ã‚’æ­¢ã‚ã‚‹æ°—ã¯ãªã„ã€‚
-
-#### [éœ§é›¨é­”ç†æ²™](/wiki/%E9%9C%A7%E9%9B%A8%E9%AD%94%E7%90%86%E6%B2%99 "éœ§é›¨é­”ç†æ²™")
-
-    é­”æ³•ä½¿ã„ã®å°‘å¥³ã€‚é–“æ¬ æ³‰ã«èˆˆå‘³æ´¥ã€…ã€‚
-
-#### å…«é›²ç´«
-
-    å¤ã„å¦–æ€ªã€‚åœ°ä¸Šã®å¦–æ€ªã¨åœ°åº•ã®å¦–æ€ªãŒå¹²æ¸‰ã™ã‚‹ã“ã¨ã«é›£è‰²ã‚’ç¤ºã—ã€äººé–“ã§ã‚ã‚‹éœŠå¤¢ã‚’åœ°åº•ã«é€ã‚‹ã€‚éœŠå¤¢ã®é™°é™½ç‰ã«é€šä¿¡æ©Ÿèƒ½ã‚’ä»˜ã‘ãŸã€‚
-
-#### ä¼Šå¹èƒé¦™
-
-    åœ°åº•ã«ä½ã‚“ã§ã„ãŸé¬¼ã€‚è‡ªåˆ†ã§åœ°åº•ã«è¡Œã£ã¦ã‚‚å•é¡Œã¯ãªã„ã®ã ãŒã€ç´«ã®ä½œæˆ¦ãŒé¢ç™½ãã†ã ã£ãŸã®ã§éœŠå¤¢ã®ã‚µãƒãƒ¼ãƒˆã«å›ã‚‹ã€‚
-
-#### å°„å‘½ä¸¸æ–‡
-
-    å±±ã«ä½ã‚€é´‰å¤©ç‹—ã§ã€æ–°èè¨˜è€…ã€‚å±±ã®ç¥ã€…ã¨æ²³ç«¥ã®ä¸ç©ãªå‹•ãã‚’èª¿æŸ»ã—ã¦ã„ãŸã¨ã“ã‚åœ°åº•ãŒæ€ªã—ã„ã“ã¨ã‚’ã¤ã‹ã‚“ã ãŸã‚ã€éœŠå¤¢ã‚’åˆ©ç”¨ã—ã¦èª¿æŸ»ã•ã›ã‚‹ã€‚
-
-#### [ã‚¢ãƒªã‚¹ãƒ»ãƒãƒ¼ã‚¬ãƒˆãƒ­ã‚¤ãƒ‰](/wiki/%E3%82%A2%E3%83%AA%E3%82%B9%E3%83%BB%E3%83%9E%E3%83%BC%E3%82%AC%E3%83%88%E3%83%AD%E3%82%A4%E3%83%89 "ã‚¢ãƒªã‚¹ãƒ»ãƒãƒ¼ã‚¬ãƒˆãƒ­ã‚¤ãƒ‰")
-
-    äººå½¢ã‚’æ“ã‚‹å¦–æ€ªã€‚ç´«ã«ä½œã£ã¦ã‚‚ã‚‰ã£ãŸé éš”æ“ä½œã§ãã‚‹äººå½¢ã§ã€é­”ç†æ²™ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚
-
-#### [ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸](/wiki/%E3%83%91%E3%83%81%E3%83%A5%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8E%E3%83%BC%E3%83%AC%E3%83%83%E3%82%B8 "ãƒ‘ãƒãƒ¥ãƒªãƒ¼ãƒ»ãƒãƒ¼ãƒ¬ãƒƒã‚¸")
-
-    [ç´…é­”é¤¨](/wiki/%E5%B9%BB%E6%83%B3%E9%83%B7#ç´…é­”é¤¨ "å¹»æƒ³éƒ·")ã®é­”æ³•ä½¿ã„ã€‚é–“æ¬ æ³‰ã‹ã‚‰æ¹§ã„ãŸéœŠã®æ­£ä½“ãŒæœ‰å®³ãªã€Œæ€¨éœŠã€ã§ã‚ã‚‹ã“ã¨ã«æ°—ä»˜ãã€ç´«ã«ç›¸è«‡ã™ã‚‹ã€‚
-
-#### æ²³åŸã«ã¨ã‚Š
-
-    å±±ã«ä½ã‚€æ²³ç«¥ã€‚å±±ã®ç¥ã€…ãŒåœ°åº•ã«æ ¸èåˆç‚‰ã‚’ä½œã£ãŸã¨ã„ã†æƒ…å ±ã«èˆˆå‘³ã‚’ç¤ºã™ã€‚éœŠå¤¢ãŒå¦–æ€ªã«ä¿ƒã•ã‚Œã¦åœ°åº•ã«æ½œã‚‹ã¨ã„ã†è©±ã‚’èã„ãŸãŸã‚ã€å…ˆã‚’è¶Šã•ã‚Œã‚‹ã“ã¨ã‚’ãŠãã‚Œã¦é­”ç†æ²™ã‚’ã‘ã—ã‹ã‘ã¦åœ°åº•ã«é€ã‚‹ã€‚
-
-#### [æ±é¢¨è°·æ—©è‹—](/wiki/%E6%9D%B1%E9%A2%A8%E8%B0%B7%E6%97%A9%E8%8B%97 "æ±é¢¨è°·æ—©è‹—")
-
-    å®ˆçŸ¢ç¥ç¤¾ã®é¢¨ç¥ã€‚ç¥ç¤¾ã¸ã‚„ã£ã¦ããŸéœŠå¤¢ãŸã¡ã«ã€æŒ¨æ‹¶ã¨ç§°ã—ã¦å‹è² ã‚’æŒ‘ã‚“ã§ãã‚‹ã€‚
-
-#### å…«å‚ç¥å¥ˆå­ã€æ´©çŸ¢è«è¨ªå­
-
-    å®ˆçŸ¢ç¥ç¤¾ã®ç¥ã€…ã€‚éœŠçƒè·¯ç©ºã«æ ¸èåˆã®åŠ›ã‚’ä¸ãˆã€ã€åœ°éœŠæ®¿ã€ã§ã®ä¸€é€£ã®é¨’å‹•ã®åŸå› ã‚’ä½œã£ãŸå¼µæœ¬äººã€‚
-
-## ã‚¹ãƒ†ãƒ¼ã‚¸
-
-
-ã‚¹ãƒ†ãƒ¼ã‚¸ | ã‚¹ãƒ†ãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ« | å ´æ‰€ | ä¸­ãƒœã‚¹ | ãƒœã‚¹   
----|---|---|---|---  
-Stage 1  | å¿˜æ©ã®åœ°ã‹ã‚‰å¹ãé¢¨ | å¹»æƒ³é¢¨ç©´ | ã‚­ã‚¹ãƒ¡ | é»’è°·ãƒ¤ãƒãƒ¡   
-Stage 2  | åœ°ä¸Šã¨éå»ã‚’çµã¶æ·±é“ | åœ°ç„ã®æ·±é“ | æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£ | æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£   
-Stage 3  | å¿˜ã‚Œã‚‰ã‚ŒãŸé›ªã®æ—§éƒ½ | æ—§åœ°ç„è¡—é“ | æ˜Ÿç†Šå‹‡å„€ | æ˜Ÿç†Šå‹‡å„€   
-Stage 4  | èª°ã‹ã‚‰ã‚‚å¥½ã‹ã‚Œãªã„ææ€–ã®ç›® | åœ°éœŠæ®¿ | ç«ç„”çŒ«ç‡ï¼ˆçŒ«ã®å§¿ï¼‰ | å¤æ˜åœ°ã•ã¨ã‚Š   
-Stage 5  | æ˜”æ™‚ã®æ¥­ç« | ç¼ç†±åœ°ç„è·¡ | ç«ç„”çŒ«ç‡ï¼ˆçŒ«ã®å§¿ï¼‰ | ç«ç„”çŒ«ç‡   
-Stage 6  | è’ã€…ã—ãäºŒã¤ç›®ã®å¤ªé™½ | åœ°åº•éƒ½å¸‚æœ€æ·±éƒ¨ | ç«ç„”çŒ«ç‡ | éœŠçƒè·¯ç©º   
-Extra Stage  | åœ°ç„ã®ãƒ©ãƒ–ãƒªãƒ¼ãƒ“ã‚¸ã‚¿ãƒ¼ | å®ˆçŸ¢ç¥ç¤¾ | æ±é¢¨è°·æ—©è‹— | å¤æ˜åœ°ã“ã„ã—   
-  
-## æ›²ç›®ãƒªã‚¹ãƒˆ
-
-
-  1. åœ°éœŠé”ã®èµ·åºŠ - ã‚¿ã‚¤ãƒˆãƒ«
-  2. æš—é—‡ã®é¢¨ç©´ - 1é¢ã®ãƒ†ãƒ¼ãƒ
-  3. å°ã˜ã‚‰ã‚ŒãŸå¦–æ€ª ã€œ Lost Place - é»’è°·ãƒ¤ãƒãƒ¡ã®ãƒ†ãƒ¼ãƒ
-  4. æ¸¡ã‚‹è€…ã®é€”çµ¶ãˆãŸæ©‹ - 2é¢ã®ãƒ†ãƒ¼ãƒ
-  5. ç·‘çœ¼ã®ã‚¸ã‚§ãƒ©ã‚·ãƒ¼ - æ°´æ©‹ãƒ‘ãƒ«ã‚¹ã‚£ã®ãƒ†ãƒ¼ãƒ
-  6. æ—§åœ°ç„è¡—é“ã‚’è¡Œã - 3é¢ã®ãƒ†ãƒ¼ãƒ
-  7. è¯ã®ã•ã‹ã¥ãå¤§æ±Ÿå±± - æ˜Ÿç†Šå‹‡å„€ã®ãƒ†ãƒ¼ãƒ
-  8. ãƒãƒ¼ãƒˆãƒ•ã‚§ãƒ«ãƒˆãƒ•ã‚¡ãƒ³ã‚·ãƒ¼ - 4é¢ã®ãƒ†ãƒ¼ãƒ
-  9. å°‘å¥³ã•ã¨ã‚Š ã€œ 3rd eye - å¤æ˜åœ°ã•ã¨ã‚Šã®ãƒ†ãƒ¼ãƒ
-  10. å»ƒç„ãƒ©ãƒ©ãƒã‚¤ - 5é¢ã®ãƒ†ãƒ¼ãƒ
-  11. æ­»ä½“æ—…è¡Œ ã€œ Be of good cheer! - ç«ç„”çŒ«ç‡ã®ãƒ†ãƒ¼ãƒ
-  12. æ¥­ç«ãƒãƒ³ãƒˆãƒ« - 6é¢ã®ãƒ†ãƒ¼ãƒ
-  13. éœŠçŸ¥ã®å¤ªé™½ä¿¡ä»° ã€œ Nuclear Fusion - éœŠçƒè·¯ç©ºã®ãƒ†ãƒ¼ãƒ
-  14. ãƒ©ã‚¹ãƒˆãƒªãƒ¢ãƒ¼ãƒˆ - Extraã®ãƒ†ãƒ¼ãƒ
-  15. ãƒãƒ«ãƒˆãƒãƒ³ã®å¦–æ€ªå°‘å¥³ - å¤æ˜åœ°ã“ã„ã—ã®ãƒ†ãƒ¼ãƒ
-  16. åœ°éœŠé”ã®å¸°å®… - ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
-  17. ã‚¨ãƒãƒ«ã‚®ãƒ¼é»æ˜ ã€œ Future Dream... - ã‚¹ã‚¿ãƒƒãƒ•ãƒ­ãƒ¼ãƒ«
\ No newline at end of file
diff --git a/example/input/documents/sample_ga_definition.md b/example/input/documents/sample_ga_definition.md
deleted file mode 100644
index 8e20c5d..0000000
--- a/example/input/documents/sample_ga_definition.md
+++ /dev/null
@@ -1,21 +0,0 @@
-# Genre: å­¦è¡“è«–æ–‡
-å­¦è¡“çš„ã§å³å¯†ãªè¡¨ç¾ã‚’ç”¨ã„ã€å°‚é–€ç”¨èªã‚’æ­£ç¢ºã«ä½¿ç”¨ã—ã€è«–ç†çš„ã§å®¢è¦³çš„ãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚
-
-# Audience: å¤§å­¦ç”Ÿ
-å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®çŸ¥è­˜ã‚’æŒã¤å­¦ç¿’è€…å‘ã‘ã«ã€åŸºç¤æ¦‚å¿µã‹ã‚‰å¿œç”¨ã¾ã§æ®µéšçš„ã«èª¬æ˜ã—ã¾ã™ã€‚
-
----
-
-# Genre: æŠ€è¡“ãƒ–ãƒ­ã‚°
-å®Ÿè·µçš„ã§è¦ªã—ã¿ã‚„ã™ã„è¡¨ç¾ã‚’ç”¨ã„ã€å…·ä½“ä¾‹ã‚„ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜ã—ã¾ã™ã€‚
-
-# Audience: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
-å®Ÿå‹™çµŒé¨“ã®ã‚ã‚‹é–‹ç™ºè€…å‘ã‘ã«ã€å®Ÿè£…ã®è©³ç´°ã‚„æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆã‚’é‡è¦–ã—ãŸå†…å®¹ã‚’æä¾›ã—ã¾ã™ã€‚
-
----
-
-# Genre: æ•™ç§‘æ›¸
-ä½“ç³»çš„ã§ç¶²ç¾…çš„ãªèª¬æ˜ã‚’è¡Œã„ã€å­¦ç¿’ã®é †åºã‚’è€ƒæ…®ã—ãŸæ§‹æˆã§çŸ¥è­˜ã‚’æ•´ç†ã—ã¾ã™ã€‚
-
-# Audience: åˆå¿ƒè€…
-ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„æŠ€è¡“åˆ†é‡ã®åˆå­¦è€…å‘ã‘ã«ã€åŸºæœ¬æ¦‚å¿µã‹ã‚‰ä¸å¯§ã«è§£èª¬ã—ã¾ã™ã€‚
\ No newline at end of file
diff --git a/example/scripts/simple.bat b/example/scripts/simple.bat
index 15240c4..63cdd23 100644
--- a/example/scripts/simple.bat
+++ b/example/scripts/simple.bat
@@ -1 +1,7 @@
 uv run easy-dataset generate .\example\input\documents\sample_document.txt  --ga-file .\example\output\sample_document\ga\ga_definitions.xml  --output-dir .\example\output\sample_document\ --use-thinking --append
+uv run easy-dataset create-ga ./example/input/documents/ --output-dir ./example/output/sample_document_batch --num-ga-pairs 2
+
+uv run easy-dataset create-ga example/input/documents/ --output-dir ./test_output/test_batch2 --max-context-length 3000 --num-ga-pairs 2
+uv run easy-dataset generate ./example/input/documents/ --ga-base-dir ./test_output/test_batch2/ --output-dir ./test_output/test_batch2/ --chunk-size 2000 --use-surrounding-context  --append
+
+uv run easy-dataset generate .\example\input\documents\sample_document.txt  --ga-file .\example\output\sample_document\ga\ga_definitions.xml  --output-dir .\example\output\sample_document\ --use-thinking --append
\ No newline at end of file
diff --git a/example/scripts/test_all_commands.sh b/example/scripts/test_all_commands.sh
new file mode 100755
index 0000000..ffb7f60
--- /dev/null
+++ b/example/scripts/test_all_commands.sh
@@ -0,0 +1,157 @@
+#!/bin/bash
+
+# =============================================================================
+# Easy Dataset CLI - å®Ÿè¡Œãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
+# =============================================================================
+
+set -e
+
+# ã‚«ãƒ©ãƒ¼å®šç¾©
+RED='\033[0;31m'
+GREEN='\033[0;32m'
+YELLOW='\033[1;33m'
+BLUE='\033[0;34m'
+PURPLE='\033[0;35m'
+CYAN='\033[0;36m'
+NC='\033[0m'
+
+# ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
+INPUT_FILE="./example/input/documents/sample_document.txt"
+SHORT_FILE="./example/input/documents/test_short.md"
+OUTPUT_DIR="./test_output"
+
+print_step() {
+    echo -e "\n${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
+    echo -e "${BLUE}$1${NC}"
+    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
+}
+
+run_command() {
+    echo -e "\n${PURPLE}å®Ÿè¡Œä¸­: $1${NC}"
+    eval "$1"
+    if [ $? -eq 0 ]; then
+        echo -e "${GREEN}âœ“ æˆåŠŸ${NC}"
+    else
+        echo -e "${RED}âœ— å¤±æ•—${NC}"
+        echo "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™ã‹? [y/N]"
+        read -r response
+        if [[ ! "$response" =~ ^[Yy]$ ]]; then
+            exit 1
+        fi
+    fi
+}
+
+# æº–å‚™
+print_step "æº–å‚™: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"
+rm -rf "$OUTPUT_DIR"
+mkdir -p "$OUTPUT_DIR"
+echo -e "${GREEN}å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: $OUTPUT_DIR${NC}"
+
+# 1. ãƒ˜ãƒ«ãƒ—ã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª
+print_step "1. ãƒ˜ãƒ«ãƒ—ã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª"
+run_command "uv run easy-dataset --help"
+
+# 2. create-ga ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
+print_step "2. GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ"
+run_command "uv run easy-dataset create-ga '$INPUT_FILE' --output-dir '$OUTPUT_DIR'"
+
+echo -e "\n${CYAN}ç”Ÿæˆã•ã‚ŒãŸGAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª:${NC}"
+if [ -f "$OUTPUT_DIR/ga/ga_definitions.xml" ]; then
+    echo -e "${GREEN}âœ“ GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ${NC}"
+    echo "æœ€åˆã®20è¡Œã‚’è¡¨ç¤º:"
+    head -20 "$OUTPUT_DIR/ga/ga_definitions.xml"
+else
+    echo -e "${RED}âœ— GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“${NC}"
+fi
+
+# 3. generate ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆï¼ˆåŸºæœ¬ï¼‰
+print_step "3. Q&Aç”Ÿæˆï¼ˆåŸºæœ¬ãƒ¢ãƒ¼ãƒ‰ï¼‰"
+run_command "uv run easy-dataset generate '$INPUT_FILE' --ga-file '$OUTPUT_DIR/ga/ga_definitions.xml' --output-dir '$OUTPUT_DIR/qa_basic' --num-qa-pairs 2"
+
+echo -e "\n${CYAN}ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª:${NC}"
+find "$OUTPUT_DIR/qa_basic" -name "*.xml" -type f 2>/dev/null | head -1 | while read -r file; do
+    if [ -n "$file" ]; then
+        echo -e "${GREEN}âœ“ Q&Aãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: $file${NC}"
+        echo "å†…å®¹ã®ä¸€éƒ¨:"
+        head -15 "$file"
+    fi
+done
+
+# 4. generate ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆï¼ˆå‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
+print_step "4. Q&Aç”Ÿæˆï¼ˆå‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰"
+run_command "uv run easy-dataset generate '$INPUT_FILE' --ga-file '$OUTPUT_DIR/ga/ga_definitions.xml' --output-dir '$OUTPUT_DIR/qa_context' --use-surrounding-context --context-before 1 --context-after 1 --num-qa-pairs 2"
+
+# 5. generate ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆï¼ˆAlpacaå‡ºåŠ›ï¼‰
+print_step "5. Q&Aç”Ÿæˆï¼ˆAlpacaå½¢å¼å‡ºåŠ›ï¼‰"
+run_command "uv run easy-dataset generate '$INPUT_FILE' --ga-file '$OUTPUT_DIR/ga/ga_definitions.xml' --output-dir '$OUTPUT_DIR/qa_alpaca' --export-alpaca --num-qa-pairs 2"
+
+echo -e "\n${CYAN}ç”Ÿæˆã•ã‚ŒãŸAlpacaãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª:${NC}"
+find "$OUTPUT_DIR/qa_alpaca" -name "*.json" -type f 2>/dev/null | head -1 | while read -r file; do
+    if [ -n "$file" ]; then
+        echo -e "${GREEN}âœ“ Alpacaãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: $file${NC}"
+        echo "å†…å®¹ã®ä¸€éƒ¨:"
+        head -5 "$file"
+    fi
+done
+
+# 6. convert-to-alpaca ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
+print_step "6. XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã«å¤‰æ›"
+run_command "uv run easy-dataset convert-to-alpaca '$OUTPUT_DIR/qa_basic' --output-file '$OUTPUT_DIR/converted_dataset.json'"
+
+echo -e "\n${CYAN}å¤‰æ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª:${NC}"
+if [ -f "$OUTPUT_DIR/converted_dataset.json" ]; then
+    echo -e "${GREEN}âœ“ å¤‰æ›å®Œäº†: $OUTPUT_DIR/converted_dataset.json${NC}"
+    echo "å†…å®¹ã®ä¸€éƒ¨:"
+    head -5 "$OUTPUT_DIR/converted_dataset.json"
+else
+    echo -e "${RED}âœ— å¤‰æ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“${NC}"
+fi
+
+# 7. aggregate-logs ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
+print_step "7. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®é›†ç´„"
+# ãƒ†ã‚¹ãƒˆç”¨ã«logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
+mkdir -p "$OUTPUT_DIR/logs"
+find "$OUTPUT_DIR/qa_basic" -name "*.xml" -type f -exec cp {} "$OUTPUT_DIR/logs/" \; 2>/dev/null || echo "ã‚³ãƒ”ãƒ¼å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"
+
+run_command "uv run easy-dataset aggregate-logs '$OUTPUT_DIR' --qa-dir '$OUTPUT_DIR/aggregated'"
+
+echo -e "\n${CYAN}é›†ç´„çµæœã‚’ç¢ºèª:${NC}"
+if [ -d "$OUTPUT_DIR/aggregated" ]; then
+    file_count=$(find "$OUTPUT_DIR/aggregated" -name "*.xml" -type f | wc -l)
+    echo -e "${GREEN}âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ãŒé›†ç´„ã•ã‚Œã¾ã—ãŸ: $file_count ä»¶${NC}"
+else
+    echo -e "${RED}âœ— é›†ç´„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“${NC}"
+fi
+
+# 8. çŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ
+if [ -f "$SHORT_FILE" ]; then
+    print_step "8. çŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ"
+    run_command "uv run easy-dataset create-ga '$SHORT_FILE' --output-dir '$OUTPUT_DIR/short'"
+    run_command "uv run easy-dataset generate '$SHORT_FILE' --ga-file '$OUTPUT_DIR/short/ga/ga_definitions.xml' --output-dir '$OUTPUT_DIR/short_qa' --num-qa-pairs 1"
+else
+    print_step "8. çŸ­ã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰"
+    echo -e "${YELLOW}test_short.md ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™${NC}"
+fi
+
+# çµæœã‚µãƒãƒªãƒ¼
+print_step "ãƒ†ã‚¹ãƒˆå®Œäº†ã‚µãƒãƒªãƒ¼"
+echo -e "${GREEN}ã™ã¹ã¦ã®ã‚³ãƒãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼${NC}"
+echo ""
+echo "ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:"
+echo "ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: $OUTPUT_DIR"
+find "$OUTPUT_DIR" -type f -name "*.xml" -o -name "*.json" | head -10 | while read -r file; do
+    echo "  ğŸ“„ $file"
+done
+
+echo ""
+echo -e "${CYAN}æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:${NC}"
+echo "1. ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèª"
+echo "2. å¿…è¦ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´"
+echo "3. å®Ÿéš›ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æœ¬æ ¼é‹ç”¨"
+
+echo ""
+echo -e "${BLUE}å„ã‚³ãƒãƒ³ãƒ‰ã®è©³ç´°ãƒ˜ãƒ«ãƒ—:${NC}"
+echo "uv run easy-dataset create-ga --help"
+echo "uv run easy-dataset generate --help"
+echo "uv run easy-dataset convert-to-alpaca --help"
+echo "uv run easy-dataset aggregate-logs --help"
diff --git a/fix_xml_generation.py b/fix_xml_generation.py
deleted file mode 100644
index ebe1b4b..0000000
--- a/fix_xml_generation.py
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/usr/bin/env python3
-"""Q&Aã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿®æ­£ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
-
-import sys
-import os
-
-def fix_system_messages():
-    """qa_generator.pyã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿®æ­£"""
-    file_path = "c:/Prj/easy-dataset-cli/easy_dataset_cli/qa_generator.py"
-    
-    with open(file_path, 'r', encoding='utf-8') as f:
-        content = f.read()
-    
-    # å¤ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ–°ã—ã„ã‚‚ã®ã«ç½®æ›
-    old_message = '"ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, \\", \'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"'
-    new_message = '"ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å³å¯†ã«å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é€šå¸¸ã®XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, \\", \'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€<Question>ã€<Answer>ã€<think>ã‚¿ã‚°ã¯ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"'
-    
-    # ç½®æ›å®Ÿè¡Œ
-    new_content = content.replace(old_message, new_message)
-    
-    # æ€è€ƒãƒ•ãƒ­ãƒ¼ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚çµ±ä¸€
-    thinking_old = '"ã‚ãªãŸã¯ã€XMLå½¢å¼ã§å‡ºåŠ›ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚<think>ã‚¿ã‚°ã¯ç‰¹åˆ¥ãªã‚¿ã‚°ãªã®ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ãªã„ã§ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®XMLã®ç‰¹æ®Šæ–‡å­—ï¼ˆ&, <, >, \\", \'ï¼‰ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€æ”¹è¡Œã¯å«ã‚ãšã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"'
-    new_content = new_content.replace(thinking_old, new_message)
-    
-    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
-    with open(file_path, 'w', encoding='utf-8') as f:
-        f.write(new_content)
-    
-    print(f"ä¿®æ­£å®Œäº†: {file_path}")
-    print(f"ç½®æ›å›æ•° (é€šå¸¸): {content.count(old_message)}")
-    print(f"ç½®æ›å›æ•° (æ€è€ƒ): {content.count(thinking_old)}")
-
-if __name__ == "__main__":
-    fix_system_messages()
diff --git a/pyproject.toml b/pyproject.toml
index d5d958e..3643071 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,23 +1,41 @@
 [project]
 name = "easy-dataset-cli"
-version = "1.0.0"
+version = "1.1.0"
 description = "A simple CLI tool to generate QA pairs from a text file using LLMs and GA pairs, outputting genre-specific XML files."
 requires-python = ">=3.9"
 dependencies = [
     "typer",               # CLIãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
     "rich",                # ãƒªãƒƒãƒãªã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
-    "litellm",             # LLMé€£æºãƒ©ã‚¤ãƒ–ãƒ©ãƒª
+    "art",                 # ASCII art generation
+    "openai",              # OpenAI APIé€£æºãƒ©ã‚¤ãƒ–ãƒ©ãƒª
     "langchain-text-splitters", # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ç”¨
     "mistune",             # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è§£æç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
     "python-dotenv",       # .env ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ç”¨
     "huggingface-hub",     # Hugging Face Hub API
-    "datasets"             # Hugging Face Datasets
+    "datasets",            # Hugging Face Datasets
+    "tqdm"                 # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
 ]
 
 [project.scripts]
-# "easy-dataset" ã‚³ãƒãƒ³ãƒ‰ã§ "easy_dataset_cli.main:app" ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†è¨­å®š
-easy-dataset = "easy_dataset_cli.main:app"
+# "easy-dataset" ã‚³ãƒãƒ³ãƒ‰ã§ "easy_dataset_cli.main:main" ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†è¨­å®š
+easy-dataset = "easy_dataset_cli.main:main"
 
 [build-system]
 requires = ["setuptools>=61.0"]
 build-backend = "setuptools.build_meta"
+
+# Limit package discovery to our library only to avoid
+# accidental inclusion of folders like `output/` or `test_output/`.
+[tool.setuptools.packages.find]
+where = ["."]
+include = ["easy_dataset_cli*"]
+exclude = [
+  "tests*",
+  "test_*",
+  "docs*",
+  "example*",
+  "examples*",
+  "output*",
+  "test_output*",
+  ".SourceSage*"
+]
```
